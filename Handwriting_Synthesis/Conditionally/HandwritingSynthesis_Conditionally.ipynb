{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwfkEIzbzgLJ",
    "outputId": "0ed15e94-ee8d-4448-cdd6-8f4bc0cd06ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xsW8Lq33lhWaVc4Q_O1zOdy1_siXX1QW\n",
      "To: /ssd_scratch/cvit/sashank.sridhar/Conditionally/data.zip\n",
      "100%|██████████████████████████████████████| 11.6M/11.6M [00:00<00:00, 31.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown \"1xsW8Lq33lhWaVc4Q_O1zOdy1_siXX1QW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzdF9BSEzlxe",
    "outputId": "6147fa52-9462-4a12-dd6f-f47c67bfb0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/sentences.txt      \n",
      "  inflating: data/strokes.npy        \n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0qMjZl9YzoEK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "axTOjpmtzw4-"
   },
   "outputs": [],
   "source": [
    "strokes = np.load('data/strokes.npy', encoding='latin1', allow_pickle=True)\n",
    "with open('data/sentences.txt') as f:\n",
    "    texts = f.readlines()\n",
    "    \n",
    "train_strokes = []\n",
    "train_texts = []\n",
    "validation_strokes = []\n",
    "validation_texts = []\n",
    "\n",
    "# only train data with length at most 800\n",
    "for _ in range(len(strokes)):\n",
    "    if len(strokes[_]) <= 801:\n",
    "        train_strokes.append(strokes[_])\n",
    "        train_texts.append(texts[_])\n",
    "    else:\n",
    "        validation_strokes.append(strokes[_])\n",
    "        validation_texts.append(texts[_])\n",
    "\n",
    "# pad with zeros and build masks\n",
    "train_masks = np.zeros((len(train_strokes),800))\n",
    "for i in range(len(train_strokes)):\n",
    "    train_masks[i][0:len(train_strokes[i])-1] = 1\n",
    "    train_strokes[i] = np.vstack([train_strokes[i], np.zeros((801-len(train_strokes[i]), 3))])\n",
    "    \n",
    "validation_masks = np.zeros((len(validation_strokes),1200))\n",
    "for i in range(len(validation_strokes)):\n",
    "    validation_masks[i][0:len(validation_strokes[i])-1] = 1\n",
    "    validation_strokes[i] = np.vstack([validation_strokes[i], np.zeros((1201-len(validation_strokes[i]), 3))])\n",
    "\n",
    "np.save('data/train_strokes_800', np.stack(train_strokes))\n",
    "np.save('data/train_masks_800', train_masks)\n",
    "np.save('data/validation_strokes_800', np.stack(validation_strokes))\n",
    "np.save('data/validation_masks_800', validation_masks)\n",
    "\n",
    "# convert each text sentence to an array of onehots\n",
    "char_list = ' ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.\"\\'?-'\n",
    "\n",
    "char_to_code = {}\n",
    "code_to_char = {}\n",
    "c = 0\n",
    "for _ in char_list:\n",
    "    char_to_code[_] = c\n",
    "    code_to_char[c] = _\n",
    "    c += 1\n",
    "torch.save(char_to_code, 'char_to_code.pt')\n",
    "\n",
    "max_text_len = np.max(np.array([len(a) for a in validation_texts]))\n",
    "\n",
    "train_onehot_800 = []\n",
    "train_text_masks = []\n",
    "for t in train_texts:\n",
    "    onehots = np.zeros((max_text_len, len(char_to_code)+1))\n",
    "    mask = np.ones(max_text_len)\n",
    "    for _ in range(len(t)):\n",
    "        try:\n",
    "            onehots[_][char_to_code[t[_]]] = 1\n",
    "        except:\n",
    "            onehots[_][-1] = 1\n",
    "    mask[len(t):] = 0\n",
    "    train_onehot_800.append(onehots)\n",
    "    train_text_masks.append(mask)\n",
    "train_onehot_800 = np.stack(train_onehot_800)\n",
    "train_text_masks = np.stack(train_text_masks)\n",
    "train_text_lens = np.array([[len(a)] for a in train_texts])\n",
    "\n",
    "validation_onehot_800 = []\n",
    "validation_text_masks = []\n",
    "for t in validation_texts:\n",
    "    onehots = np.zeros((max_text_len, len(char_to_code)+1))\n",
    "    mask = np.ones(max_text_len)\n",
    "    for _ in range(len(t)):\n",
    "        try:\n",
    "            onehots[_][char_to_code[t[_]]] = 1\n",
    "        except:\n",
    "            onehots[_][-1] = 1\n",
    "    mask[len(t):] = 0\n",
    "    validation_onehot_800.append(onehots)\n",
    "    validation_text_masks.append(mask)\n",
    "validation_onehot_800 = np.stack(validation_onehot_800)\n",
    "validation_text_masks = np.stack(validation_text_masks)\n",
    "validation_text_lens = np.array([[len(a)] for a in validation_texts])\n",
    "\n",
    "np.save('data/train_onehot_800', train_onehot_800)\n",
    "np.save('data/validation_onehot_800', validation_onehot_800)\n",
    "np.save('data/train_text_masks', train_text_masks)\n",
    "np.save('data/validation_text_masks', validation_text_masks)\n",
    "np.save('data/train_text_lens', train_text_lens)\n",
    "np.save('data/validation_text_lens', validation_text_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sgAy3f_z0ANg"
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YJU_NRzp0Xn2"
   },
   "outputs": [],
   "source": [
    "class LSTMRandWriter(nn.Module):\n",
    "    def __init__(self, cell_size, num_clusters):\n",
    "        super(LSTMRandWriter, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = 3, hidden_size = cell_size,\\\n",
    "                                num_layers = 1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size = cell_size+3, \\\n",
    "                                hidden_size = cell_size,\\\n",
    "                                num_layers = 1, batch_first=True)\n",
    "        self.linear1 = nn.Linear(cell_size*2,\\\n",
    "                                    1+ num_clusters*6)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, prev, prev2):\n",
    "        h1, (h1_n, c1_n) = self.lstm(x, prev)\n",
    "        \n",
    "        x2 = torch.cat([h1, x], dim=-1) # skip connection\n",
    "        h2, (h2_n, c2_n) = self.lstm2(x2, prev2)\n",
    "        \n",
    "        h = torch.cat([h1, h2], dim=-1) # skip connection\n",
    "        params = self.linear1(h)\n",
    "\n",
    "        \n",
    "        mog_params = params.narrow(-1, 0, params.size()[-1]-1)\n",
    "        pre_weights, mu_1, mu_2, log_sigma_1, log_sigma_2, pre_rho = mog_params.chunk(6, dim=-1)\n",
    "        weights = F.softmax(pre_weights, dim=-1)\n",
    "        rho = self.tanh(pre_rho)\n",
    "        end = F.sigmoid(params.narrow(-1, params.size()[-1]-1, 1))\n",
    "        \n",
    "        return end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2,\\\n",
    "            rho, (h1_n, c1_n), (h2_n, c2_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wP-R5VJD0bs_"
   },
   "outputs": [],
   "source": [
    "# attention window for handwriting synthesis\n",
    "class Window(nn.Module):\n",
    "    def __init__(self, padded_text_len, cell_size, K):\n",
    "        super(Window, self).__init__()\n",
    "        self.linear = nn.Linear(cell_size, 3*K)\n",
    "        self.padded_text_len = padded_text_len\n",
    "        \n",
    "    def forward(self, x, kappa_old, onehots, text_lens):\n",
    "        params = self.linear(x).exp()\n",
    "        \n",
    "        alpha, beta, pre_kappa = params.chunk(3, dim=-1)\n",
    "        kappa = kappa_old + pre_kappa\n",
    "        \n",
    "        indices = torch.from_numpy(np.array(range(self.padded_text_len + 1))).type(torch.FloatTensor)\n",
    "        if cuda:\n",
    "            indices = indices.cuda()\n",
    "        indices = Variable(indices, requires_grad=False)\n",
    "        gravity = -beta.unsqueeze(2)*(kappa.unsqueeze(2).repeat(1, 1, self.padded_text_len + 1)-indices)**2\n",
    "        phi = (alpha.unsqueeze(2) * gravity.exp()).sum(dim=1)*(self.padded_text_len/text_lens)\n",
    "        \n",
    "        w = (phi.narrow(-1, 0, self.padded_text_len).unsqueeze(2) * onehots).sum(dim=1) \n",
    "        return w, kappa, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9tmBjBLU0fz0"
   },
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, padded_text_len, vocab_len, cell_size, K):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(input_size = 3 + vocab_len, hidden_size = cell_size)\n",
    "        self.window = Window(padded_text_len, cell_size, K)\n",
    "        \n",
    "    def forward(self, x, onehots, text_lens, w_old, kappa_old, prev):\n",
    "        h1s = []\n",
    "        ws = []\n",
    "        phis = []\n",
    "        for _ in range(x.size()[1]):\n",
    "\n",
    "            cell_input = torch.cat([x.narrow(1,_,1).squeeze(1),w_old], dim=-1)\n",
    "            prev = self.lstm(cell_input, prev)\n",
    "            \n",
    "            # attention window parameters\n",
    "            w_old, kappa_old, old_phi = self.window(prev[0], kappa_old, onehots, text_lens)\n",
    "            \n",
    "            # concatenate for single pass through the next layer\n",
    "            h1s.append(prev[0])\n",
    "            ws.append(w_old)\n",
    "        \n",
    "        return torch.stack(ws, dim=0).permute(1,0,2), torch.stack(h1s, dim=0).permute(1,0,2), \\\n",
    "                prev, w_old, kappa_old, old_phi \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IBsVkGjm0iqB"
   },
   "outputs": [],
   "source": [
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, vocab_len, cell_size):\n",
    "        super(LSTM2, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=3 + vocab_len + cell_size, \n",
    "                            hidden_size = cell_size, num_layers = 1, batch_first =True)\n",
    "        \n",
    "    def forward(self, x, ws, h1s, prev2):\n",
    "        lstm_input = torch.cat([x,ws,h1s], -1)\n",
    "        h2s, prev2 = self.lstm(lstm_input, prev2)\n",
    "        return h2s, prev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2sI6pz1d0kw5"
   },
   "outputs": [],
   "source": [
    "# 2-layer lstm with mixture of gaussian parameters as outputs\n",
    "# with skip connections\n",
    "class LSTMSynthesis(nn.Module):\n",
    "    def __init__(self, padded_text_len, vocab_len, cell_size, num_clusters, K):\n",
    "        super(LSTMSynthesis, self).__init__()\n",
    "        self.lstm1 = LSTM1(padded_text_len, vocab_len, cell_size, K)\n",
    "        self.lstm2 = LSTM2(vocab_len, cell_size)\n",
    "        self.linear = nn.Linear(cell_size*2, 1+ num_clusters*6)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, onehots, text_lens, w_old, kappa_old, prev, prev2, bias=0.):\n",
    "        ws, h1s, prev, w_old, kappa_old, old_phi = self.lstm1(x, onehots, text_lens, w_old, kappa_old, prev)\n",
    "        h2s, prev2 = self.lstm2(x, ws, h1s, prev2)\n",
    "        \n",
    "        params = self.linear(torch.cat([h1s,h2s], dim=-1))\n",
    "        mog_params = params.narrow(-1, 0, params.size()[-1]-1)\n",
    "        pre_weights, mu_1, mu_2, log_sigma_1, log_sigma_2, pre_rho = mog_params.chunk(6, dim=-1)\n",
    "        weights = F.softmax(pre_weights*(1+bias), dim=-1)\n",
    "        rho = self.tanh(pre_rho)\n",
    "        end = F.sigmoid(params.narrow(-1, params.size()[-1]-1, 1))\n",
    "        \n",
    "        return end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, rho, w_old, kappa_old, prev, prev2, old_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a_Qy4fCJ0nla"
   },
   "outputs": [],
   "source": [
    "def decay_learning_rate(optimizer, decay_rate):\n",
    "    # learning rate annealing\n",
    "    state_dict = optimizer.state_dict()\n",
    "    lr = state_dict['param_groups'][0]['lr']\n",
    "    lr *= decay_rate\n",
    "    for param_group in state_dict['param_groups']:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.load_state_dict(state_dict)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5u_miIC40wdr"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, validation_loss, optimizer, directory, \\\n",
    "                    filename='best.pt'):\n",
    "    checkpoint=({'epoch': epoch+1,\n",
    "    'model': model.state_dict(),\n",
    "    'validation_loss': validation_loss,\n",
    "    'optimizer' : optimizer.state_dict()\n",
    "    })\n",
    "    try:\n",
    "        torch.save(checkpoint, os.path.join(directory, filename))\n",
    "        \n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "        torch.save(checkpoint, os.path.join(directory, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nIaGI8pl00Iz"
   },
   "outputs": [],
   "source": [
    "def plot_stroke(stroke, save_name=None):\n",
    "    # Plot a single example.\n",
    "    f, ax = plt.subplots()\n",
    "\n",
    "    x = np.cumsum(stroke[:, 1])\n",
    "    y = np.cumsum(stroke[:, 2])\n",
    "\n",
    "    size_x = x.max() - x.min() + 1.\n",
    "    size_y = y.max() - y.min() + 1.\n",
    "\n",
    "    f.set_size_inches(5. * size_x / size_y, 5.)\n",
    "\n",
    "    cuts = np.where(stroke[:, 0] == 1)[0]\n",
    "    start = 0\n",
    "\n",
    "    for cut_value in cuts:\n",
    "        ax.plot(x[start:cut_value], y[start:cut_value],\n",
    "                'k-', linewidth=3)\n",
    "        start = cut_value + 1\n",
    "    ax.axis('equal')\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    if save_name is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        try:\n",
    "            plt.savefig(\n",
    "                save_name,\n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.5)\n",
    "        except Exception:\n",
    "            print (\"Error building image!: \" + save_name)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "b7pEwyaS2cxp"
   },
   "outputs": [],
   "source": [
    "# training objective\n",
    "def log_likelihood(end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, rho, \\\n",
    "                    y, masks):\n",
    "    # targets\n",
    "    y_0 = y.narrow(-1,0,1)\n",
    "    y_1 = y.narrow(-1,1,1)\n",
    "    y_2 = y.narrow(-1,2,1)\n",
    "    \n",
    "    # end of stroke prediction\n",
    "    end_loglik = (y_0*end + (1-y_0)*(1-end)).log().squeeze()\n",
    "    \n",
    "    # new stroke point prediction\n",
    "    const = 1E-20 # to prevent numerical error\n",
    "    pi_term = torch.Tensor([2*np.pi])\n",
    "    if cuda:\n",
    "        pi_term = pi_term.cuda()\n",
    "    pi_term = -Variable(pi_term, requires_grad = False).log()\n",
    "    \n",
    "    z = (y_1 - mu_1)**2/(log_sigma_1.exp()**2)\\\n",
    "        + ((y_2 - mu_2)**2/(log_sigma_2.exp()**2)) \\\n",
    "        - 2*rho*(y_1-mu_1)*(y_2-mu_2)/((log_sigma_1 + log_sigma_2).exp())\n",
    "    mog_lik1 =  pi_term -log_sigma_1 - log_sigma_2 - 0.5*((1-rho**2).log())\n",
    "    mog_lik2 = z/(2*(1-rho**2))\n",
    "    mog_loglik = ((weights.log() + (mog_lik1 - mog_lik2)).exp().sum(dim=-1)+const).log()\n",
    "    \n",
    "    return (end_loglik*masks).sum() + ((mog_loglik)*masks).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZGeHsr402LNk"
   },
   "outputs": [],
   "source": [
    "def synthesis_train(args, train_loader, validation_loader):\n",
    "    # infer padded text len and vocab len\n",
    "    padded_text_len, vocab_len = train_loader.dataset[0][2].size()\n",
    "    \n",
    "    # define model and optimizer\n",
    "    model = LSTMSynthesis(padded_text_len, vocab_len, args['cell_size'], args['num_clusters'], args['K'])\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    optimizer = optim.Adam([{'params':model.parameters()},], lr=args['learning_rate'])\n",
    "    \n",
    "    # initialize null hidden, memory states and cluster centers\n",
    "    h1_init = c1_init = torch.zeros((args['batch_size'], args['cell_size']))\n",
    "    h2_init = c2_init = torch.zeros((1, args['batch_size'], args['cell_size']))\n",
    "    kappa_old = torch.zeros(args['batch_size'], args['K'])\n",
    "    \n",
    "    if cuda:\n",
    "        h1_init, c1_init = h1_init.cuda(), c1_init.cuda()\n",
    "        h2_init, c2_init = h2_init.cuda(), c2_init.cuda()\n",
    "        kappa_old = kappa_old.cuda()\n",
    "        \n",
    "    h1_init, c1_init = Variable(h1_init, requires_grad=False), Variable(c1_init, requires_grad=False)\n",
    "    h2_init, c2_init = Variable(h2_init, requires_grad=False), Variable(c2_init, requires_grad=False)\n",
    "    kappa_old = Variable(kappa_old, requires_grad=False)\n",
    "    \n",
    "    t_loss = []\n",
    "    v_loss = []\n",
    "    best_validation_loss = 1E10\n",
    "    \n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args['num_epochs']):\n",
    "        train_loss =0\n",
    "        for batch_idx, (data, masks, onehots, text_lens) in enumerate(train_loader):\n",
    "            \n",
    "            # gather training batch\n",
    "            step_back = data.narrow(1,0,args['timesteps'])\n",
    "            x = Variable(step_back, requires_grad=False)\n",
    "            onehots = Variable(onehots, requires_grad = False)\n",
    "            masks = Variable(masks, requires_grad=False)\n",
    "            masks = masks.narrow(1,0,args['timesteps'])\n",
    "            text_lens = Variable(text_lens, requires_grad=False)\n",
    "            \n",
    "            # focus window weight on first text char\n",
    "            w_old = onehots.narrow(1,0,1).squeeze()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # feed forward\n",
    "            outputs = model(x,onehots, text_lens, w_old, kappa_old, (h1_init, c1_init), (h2_init, c2_init))\n",
    "            end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, rho, w, kappa, prev, prev2, old_phi = outputs\n",
    "            data = data.narrow(1,1,args['timesteps'])\n",
    "            y = Variable(data, requires_grad=False)\n",
    "            loss = -log_likelihood(end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, rho, y, masks)/torch.sum(masks)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item()))\n",
    "    \n",
    "        print('====> Epoch: {} Average train loss: {:.4f}'.format(\n",
    "            epoch+1, train_loss/(len(train_loader.dataset)//args['batch_size'])))\n",
    "        t_loss.append(train_loss/(len(train_loader.dataset)//args['batch_size']))\n",
    "    \n",
    "        # validation\n",
    "        # prepare validation data\n",
    "        (validation_samples, masks, onehots, text_lens) = list(enumerate(validation_loader))[0][1]\n",
    "        step_back = validation_samples.narrow(1,0,args['timesteps'])\n",
    "        masks = Variable(masks, requires_grad=False)\n",
    "        masks = masks.narrow(1,0,args['timesteps'])\n",
    "        onehots = Variable(onehots, requires_grad=False)\n",
    "        text_lens = Variable(text_lens, requires_grad=False)\n",
    "        \n",
    "        w_old = onehots.narrow(1,0,1).squeeze()\n",
    "        x = Variable(step_back, requires_grad=False)\n",
    "        \n",
    "        validation_samples = validation_samples.narrow(1,1,args['timesteps'])\n",
    "        y = Variable(validation_samples, requires_grad = False)\n",
    "    \n",
    "        outputs = model(x, onehots, text_lens, w_old, kappa_old, (h1_init, c1_init), (h2_init, c2_init))\n",
    "        end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, rho, w, kappa, prev, prev2, old_phi = outputs\n",
    "        loss = -log_likelihood(end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, rho, y, masks)/torch.sum(masks)\n",
    "        validation_loss = loss.item()\n",
    "        print('====> Epoch: {} Average validation loss: {:.4f}'.format(\\\n",
    "            epoch+1, validation_loss))\n",
    "        v_loss.append(validation_loss)\n",
    "    \n",
    "        \n",
    "        # # learning rate annealing\n",
    "        # if (epoch+1)%10 == 0:\n",
    "        #     optimizer = decay_learning_rate(optimizer)\n",
    "        \n",
    "        # checkpoint model and training\n",
    "        filename = args['task'] + '_epoch_{}.pt'.format(epoch+1)\n",
    "        save_checkpoint(epoch, model, validation_loss, optimizer, args['model_dir'], filename)\n",
    "        \n",
    "        print('wall time: {}s'.format(time.time()-start_time))\n",
    "        \n",
    "    f1 = plt.figure(1)\n",
    "    plt.plot(range(1, args['num_epochs']+1), t_loss, color='blue', linestyle='solid')\n",
    "    plt.plot(range(1, args['num_epochs']+1), v_loss, color='red', linestyle='solid')\n",
    "    f1.savefig(args['task'] +\"_loss_curves\", bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FlNXZxSI1fQb"
   },
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['task'] = 'synthesis'\n",
    "args['cell_size'] = 400\n",
    "args['batch_size'] = 50\n",
    "args['timesteps'] = 800\n",
    "args['num_epochs'] = 50\n",
    "args['model_dir'] = 'save'\n",
    "args['learning_rate'] = 8E-4\n",
    "args['decay_rate'] = 0.99\n",
    "args['num_clusters'] = 20\n",
    "args['K'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "C6jWqG-Z019K"
   },
   "outputs": [],
   "source": [
    "train_data = [np.load('data/train_strokes_800.npy'), np.load('data/train_masks_800.npy'), np.load('data/train_onehot_800.npy'),\n",
    "                np.load('data/train_text_lens.npy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Jy2PjgIs1EcS"
   },
   "outputs": [],
   "source": [
    "for _ in range(len(train_data)):\n",
    "        train_data[_] =torch.from_numpy(train_data[_]).type(torch.FloatTensor)\n",
    "        if cuda:\n",
    "            train_data[_] = train_data[_].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xfmk2qOs1Fwx"
   },
   "outputs": [],
   "source": [
    "train_data = [(train_data[0][i], train_data[1][i], \n",
    "                train_data[2][i], train_data[3][i]) for i in range(len(train_data[0]))] \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=args['batch_size'], shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4133tvRD1_Kf"
   },
   "outputs": [],
   "source": [
    "validation_data = [np.load('data/validation_strokes_800.npy'), np.load('data/validation_masks_800.npy'), \n",
    "                    np.load('data/validation_onehot_800.npy'), np.load('data/validation_text_lens.npy')]\n",
    "for _ in range(len(validation_data)):\n",
    "    validation_data[_] = torch.from_numpy(validation_data[_]).type(torch.FloatTensor)\n",
    "    if cuda:\n",
    "        validation_data[_] = validation_data[_].cuda()\n",
    "validation_data = [(validation_data[0][i], validation_data[1][i], validation_data[2][i], validation_data[3][i]) \n",
    "                for i in range(len(validation_data[0]))] \n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_data, batch_size=args['batch_size'], shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEBNaAp92E_J",
    "outputId": "2221f59c-27a8-4b73-831c-95e301d44507"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/sashank.sridhar/miniconda3/envs/TripletLoss/lib/python3.9/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4977 (0%)]\tLoss: 4.556543\n",
      "Train Epoch: 1 [500/4977 (10%)]\tLoss: 2.914186\n",
      "Train Epoch: 1 [1000/4977 (20%)]\tLoss: 2.812930\n",
      "Train Epoch: 1 [1500/4977 (30%)]\tLoss: 2.826246\n",
      "Train Epoch: 1 [2000/4977 (40%)]\tLoss: 2.587486\n",
      "Train Epoch: 1 [2500/4977 (51%)]\tLoss: 2.200056\n",
      "Train Epoch: 1 [3000/4977 (61%)]\tLoss: 1.787325\n",
      "Train Epoch: 1 [3500/4977 (71%)]\tLoss: 1.378647\n",
      "Train Epoch: 1 [4000/4977 (81%)]\tLoss: 1.030092\n",
      "Train Epoch: 1 [4500/4977 (91%)]\tLoss: 0.876814\n",
      "====> Epoch: 1 Average train loss: 2.1414\n",
      "====> Epoch: 1 Average validation loss: 0.4021\n",
      "wall time: 114.05437898635864s\n",
      "Train Epoch: 2 [0/4977 (0%)]\tLoss: 0.809524\n",
      "Train Epoch: 2 [500/4977 (10%)]\tLoss: 0.897870\n",
      "Train Epoch: 2 [1000/4977 (20%)]\tLoss: 0.746667\n",
      "Train Epoch: 2 [1500/4977 (30%)]\tLoss: 0.736921\n",
      "Train Epoch: 2 [2000/4977 (40%)]\tLoss: 0.668248\n",
      "Train Epoch: 2 [2500/4977 (51%)]\tLoss: 0.636920\n",
      "Train Epoch: 2 [3000/4977 (61%)]\tLoss: 0.509773\n",
      "Train Epoch: 2 [3500/4977 (71%)]\tLoss: 0.490124\n",
      "Train Epoch: 2 [4000/4977 (81%)]\tLoss: 0.622755\n",
      "Train Epoch: 2 [4500/4977 (91%)]\tLoss: 0.508281\n",
      "====> Epoch: 2 Average train loss: 0.6544\n",
      "====> Epoch: 2 Average validation loss: 0.0189\n",
      "wall time: 228.44349789619446s\n",
      "Train Epoch: 3 [0/4977 (0%)]\tLoss: 0.500804\n",
      "Train Epoch: 3 [500/4977 (10%)]\tLoss: 0.484492\n",
      "Train Epoch: 3 [1000/4977 (20%)]\tLoss: 0.514873\n",
      "Train Epoch: 3 [1500/4977 (30%)]\tLoss: 0.442972\n",
      "Train Epoch: 3 [2000/4977 (40%)]\tLoss: 0.322398\n",
      "Train Epoch: 3 [2500/4977 (51%)]\tLoss: 0.458332\n",
      "Train Epoch: 3 [3000/4977 (61%)]\tLoss: 0.387205\n",
      "Train Epoch: 3 [3500/4977 (71%)]\tLoss: 0.387682\n",
      "Train Epoch: 3 [4000/4977 (81%)]\tLoss: 0.385355\n",
      "Train Epoch: 3 [4500/4977 (91%)]\tLoss: 0.386082\n",
      "====> Epoch: 3 Average train loss: 0.4460\n",
      "====> Epoch: 3 Average validation loss: -0.0788\n",
      "wall time: 346.4060688018799s\n",
      "Train Epoch: 4 [0/4977 (0%)]\tLoss: 0.362941\n",
      "Train Epoch: 4 [500/4977 (10%)]\tLoss: 0.312233\n",
      "Train Epoch: 4 [1000/4977 (20%)]\tLoss: 0.322413\n",
      "Train Epoch: 4 [1500/4977 (30%)]\tLoss: 0.360920\n",
      "Train Epoch: 4 [2000/4977 (40%)]\tLoss: 0.405066\n",
      "Train Epoch: 4 [2500/4977 (51%)]\tLoss: 0.336632\n",
      "Train Epoch: 4 [3000/4977 (61%)]\tLoss: 0.304685\n",
      "Train Epoch: 4 [3500/4977 (71%)]\tLoss: 0.348774\n",
      "Train Epoch: 4 [4000/4977 (81%)]\tLoss: 0.261617\n",
      "Train Epoch: 4 [4500/4977 (91%)]\tLoss: 0.366183\n",
      "====> Epoch: 4 Average train loss: 0.3558\n",
      "====> Epoch: 4 Average validation loss: -0.1609\n",
      "wall time: 458.8578214645386s\n",
      "Train Epoch: 5 [0/4977 (0%)]\tLoss: 0.298448\n",
      "Train Epoch: 5 [500/4977 (10%)]\tLoss: 0.265659\n",
      "Train Epoch: 5 [1000/4977 (20%)]\tLoss: 0.405105\n",
      "Train Epoch: 5 [1500/4977 (30%)]\tLoss: 0.451088\n",
      "Train Epoch: 5 [2000/4977 (40%)]\tLoss: 0.205732\n",
      "Train Epoch: 5 [2500/4977 (51%)]\tLoss: 0.126078\n",
      "Train Epoch: 5 [3000/4977 (61%)]\tLoss: 0.291333\n",
      "Train Epoch: 5 [3500/4977 (71%)]\tLoss: 0.305945\n",
      "Train Epoch: 5 [4000/4977 (81%)]\tLoss: 0.304399\n",
      "Train Epoch: 5 [4500/4977 (91%)]\tLoss: 0.233388\n",
      "====> Epoch: 5 Average train loss: 0.2930\n",
      "====> Epoch: 5 Average validation loss: -0.2027\n",
      "wall time: 573.5003852844238s\n",
      "Train Epoch: 6 [0/4977 (0%)]\tLoss: 0.226145\n",
      "Train Epoch: 6 [500/4977 (10%)]\tLoss: 0.328103\n",
      "Train Epoch: 6 [1000/4977 (20%)]\tLoss: 0.373477\n",
      "Train Epoch: 6 [1500/4977 (30%)]\tLoss: 0.227380\n",
      "Train Epoch: 6 [2000/4977 (40%)]\tLoss: 0.238467\n",
      "Train Epoch: 6 [2500/4977 (51%)]\tLoss: 0.286165\n",
      "Train Epoch: 6 [3000/4977 (61%)]\tLoss: 0.209645\n",
      "Train Epoch: 6 [3500/4977 (71%)]\tLoss: 0.193561\n",
      "Train Epoch: 6 [4000/4977 (81%)]\tLoss: 0.198288\n",
      "Train Epoch: 6 [4500/4977 (91%)]\tLoss: 0.226781\n",
      "====> Epoch: 6 Average train loss: 0.2462\n",
      "====> Epoch: 6 Average validation loss: -0.2625\n",
      "wall time: 688.2121062278748s\n",
      "Train Epoch: 7 [0/4977 (0%)]\tLoss: 0.138307\n",
      "Train Epoch: 7 [500/4977 (10%)]\tLoss: 0.237330\n",
      "Train Epoch: 7 [1000/4977 (20%)]\tLoss: 0.216721\n",
      "Train Epoch: 7 [1500/4977 (30%)]\tLoss: 0.258995\n",
      "Train Epoch: 7 [2000/4977 (40%)]\tLoss: 0.127169\n",
      "Train Epoch: 7 [2500/4977 (51%)]\tLoss: 0.215769\n",
      "Train Epoch: 7 [3000/4977 (61%)]\tLoss: 0.223481\n",
      "Train Epoch: 7 [3500/4977 (71%)]\tLoss: 0.331601\n",
      "Train Epoch: 7 [4000/4977 (81%)]\tLoss: 0.221229\n",
      "Train Epoch: 7 [4500/4977 (91%)]\tLoss: 0.233543\n",
      "====> Epoch: 7 Average train loss: 0.2124\n",
      "====> Epoch: 7 Average validation loss: -0.2922\n",
      "wall time: 804.9774644374847s\n",
      "Train Epoch: 8 [0/4977 (0%)]\tLoss: 0.111087\n",
      "Train Epoch: 8 [500/4977 (10%)]\tLoss: 0.181756\n",
      "Train Epoch: 8 [1000/4977 (20%)]\tLoss: 0.115604\n",
      "Train Epoch: 8 [1500/4977 (30%)]\tLoss: 0.263139\n",
      "Train Epoch: 8 [2000/4977 (40%)]\tLoss: 0.137860\n",
      "Train Epoch: 8 [2500/4977 (51%)]\tLoss: 0.192943\n",
      "Train Epoch: 8 [3000/4977 (61%)]\tLoss: 0.196118\n",
      "Train Epoch: 8 [3500/4977 (71%)]\tLoss: 0.110009\n",
      "Train Epoch: 8 [4000/4977 (81%)]\tLoss: 0.245972\n",
      "Train Epoch: 8 [4500/4977 (91%)]\tLoss: 0.152143\n",
      "====> Epoch: 8 Average train loss: 0.1813\n",
      "====> Epoch: 8 Average validation loss: -0.3003\n",
      "wall time: 923.907874584198s\n",
      "Train Epoch: 9 [0/4977 (0%)]\tLoss: 0.149298\n",
      "Train Epoch: 9 [500/4977 (10%)]\tLoss: 0.198819\n",
      "Train Epoch: 9 [1000/4977 (20%)]\tLoss: 0.240181\n",
      "Train Epoch: 9 [1500/4977 (30%)]\tLoss: 0.163970\n",
      "Train Epoch: 9 [2000/4977 (40%)]\tLoss: 0.163922\n",
      "Train Epoch: 9 [2500/4977 (51%)]\tLoss: 0.086641\n",
      "Train Epoch: 9 [3000/4977 (61%)]\tLoss: 0.148178\n",
      "Train Epoch: 9 [3500/4977 (71%)]\tLoss: 0.183617\n",
      "Train Epoch: 9 [4000/4977 (81%)]\tLoss: 0.082629\n",
      "Train Epoch: 9 [4500/4977 (91%)]\tLoss: 0.222063\n",
      "====> Epoch: 9 Average train loss: 0.1535\n",
      "====> Epoch: 9 Average validation loss: -0.3329\n",
      "wall time: 1042.932216644287s\n",
      "Train Epoch: 10 [0/4977 (0%)]\tLoss: 0.059593\n",
      "Train Epoch: 10 [500/4977 (10%)]\tLoss: 0.155918\n",
      "Train Epoch: 10 [1000/4977 (20%)]\tLoss: 0.221742\n",
      "Train Epoch: 10 [1500/4977 (30%)]\tLoss: 0.144214\n",
      "Train Epoch: 10 [2000/4977 (40%)]\tLoss: 0.214194\n",
      "Train Epoch: 10 [2500/4977 (51%)]\tLoss: 0.157131\n",
      "Train Epoch: 10 [3000/4977 (61%)]\tLoss: 0.195256\n",
      "Train Epoch: 10 [3500/4977 (71%)]\tLoss: 0.091498\n",
      "Train Epoch: 10 [4000/4977 (81%)]\tLoss: 0.144387\n",
      "Train Epoch: 10 [4500/4977 (91%)]\tLoss: 0.121293\n",
      "====> Epoch: 10 Average train loss: 0.1288\n",
      "====> Epoch: 10 Average validation loss: -0.3666\n",
      "wall time: 1162.3572957515717s\n",
      "Train Epoch: 11 [0/4977 (0%)]\tLoss: 0.148981\n",
      "Train Epoch: 11 [500/4977 (10%)]\tLoss: 0.122360\n",
      "Train Epoch: 11 [1000/4977 (20%)]\tLoss: 0.056112\n",
      "Train Epoch: 11 [1500/4977 (30%)]\tLoss: 0.105860\n",
      "Train Epoch: 11 [2000/4977 (40%)]\tLoss: 0.164453\n",
      "Train Epoch: 11 [2500/4977 (51%)]\tLoss: 0.117191\n",
      "Train Epoch: 11 [3000/4977 (61%)]\tLoss: 0.119739\n",
      "Train Epoch: 11 [3500/4977 (71%)]\tLoss: 0.154667\n",
      "Train Epoch: 11 [4000/4977 (81%)]\tLoss: 0.093969\n",
      "Train Epoch: 11 [4500/4977 (91%)]\tLoss: 0.087338\n",
      "====> Epoch: 11 Average train loss: 0.1072\n",
      "====> Epoch: 11 Average validation loss: -0.3910\n",
      "wall time: 1267.2255520820618s\n",
      "Train Epoch: 12 [0/4977 (0%)]\tLoss: 0.090627\n",
      "Train Epoch: 12 [500/4977 (10%)]\tLoss: 0.121964\n",
      "Train Epoch: 12 [1000/4977 (20%)]\tLoss: 0.021526\n",
      "Train Epoch: 12 [1500/4977 (30%)]\tLoss: 0.074559\n",
      "Train Epoch: 12 [2000/4977 (40%)]\tLoss: 0.131307\n",
      "Train Epoch: 12 [2500/4977 (51%)]\tLoss: 0.218876\n",
      "Train Epoch: 12 [3000/4977 (61%)]\tLoss: 0.081952\n",
      "Train Epoch: 12 [3500/4977 (71%)]\tLoss: -0.000037\n",
      "Train Epoch: 12 [4000/4977 (81%)]\tLoss: -0.015047\n",
      "Train Epoch: 12 [4500/4977 (91%)]\tLoss: 0.052719\n",
      "====> Epoch: 12 Average train loss: 0.0840\n",
      "====> Epoch: 12 Average validation loss: -0.3728\n",
      "wall time: 1380.1086750030518s\n",
      "Train Epoch: 13 [0/4977 (0%)]\tLoss: 0.001603\n",
      "Train Epoch: 13 [500/4977 (10%)]\tLoss: 0.055411\n",
      "Train Epoch: 13 [1000/4977 (20%)]\tLoss: 0.058196\n",
      "Train Epoch: 13 [1500/4977 (30%)]\tLoss: 0.089753\n",
      "Train Epoch: 13 [2000/4977 (40%)]\tLoss: 0.073875\n",
      "Train Epoch: 13 [2500/4977 (51%)]\tLoss: 0.055501\n",
      "Train Epoch: 13 [3000/4977 (61%)]\tLoss: 0.056971\n",
      "Train Epoch: 13 [3500/4977 (71%)]\tLoss: 0.144872\n",
      "Train Epoch: 13 [4000/4977 (81%)]\tLoss: 0.101432\n",
      "Train Epoch: 13 [4500/4977 (91%)]\tLoss: -0.012195\n",
      "====> Epoch: 13 Average train loss: 0.0704\n",
      "====> Epoch: 13 Average validation loss: -0.4121\n",
      "wall time: 1494.9724380970001s\n",
      "Train Epoch: 14 [0/4977 (0%)]\tLoss: -0.009842\n",
      "Train Epoch: 14 [500/4977 (10%)]\tLoss: -0.009861\n",
      "Train Epoch: 14 [1000/4977 (20%)]\tLoss: -0.056169\n",
      "Train Epoch: 14 [1500/4977 (30%)]\tLoss: 0.031481\n",
      "Train Epoch: 14 [2000/4977 (40%)]\tLoss: 0.057729\n",
      "Train Epoch: 14 [2500/4977 (51%)]\tLoss: 0.040821\n",
      "Train Epoch: 14 [3000/4977 (61%)]\tLoss: 0.118824\n",
      "Train Epoch: 14 [3500/4977 (71%)]\tLoss: 0.048245\n",
      "Train Epoch: 14 [4000/4977 (81%)]\tLoss: 0.103938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [4500/4977 (91%)]\tLoss: 0.138780\n",
      "====> Epoch: 14 Average train loss: 0.0548\n",
      "====> Epoch: 14 Average validation loss: -0.4047\n",
      "wall time: 1609.4874033927917s\n",
      "Train Epoch: 15 [0/4977 (0%)]\tLoss: 0.020593\n",
      "Train Epoch: 15 [500/4977 (10%)]\tLoss: 0.018112\n",
      "Train Epoch: 15 [1000/4977 (20%)]\tLoss: 0.045228\n",
      "Train Epoch: 15 [1500/4977 (30%)]\tLoss: -0.029493\n",
      "Train Epoch: 15 [2000/4977 (40%)]\tLoss: -0.055458\n",
      "Train Epoch: 15 [2500/4977 (51%)]\tLoss: 0.084180\n",
      "Train Epoch: 15 [3000/4977 (61%)]\tLoss: 0.017243\n",
      "Train Epoch: 15 [3500/4977 (71%)]\tLoss: 0.028643\n",
      "Train Epoch: 15 [4000/4977 (81%)]\tLoss: -0.039126\n",
      "Train Epoch: 15 [4500/4977 (91%)]\tLoss: 0.046945\n",
      "====> Epoch: 15 Average train loss: 0.0340\n",
      "====> Epoch: 15 Average validation loss: -0.4482\n",
      "wall time: 1724.2178814411163s\n",
      "Train Epoch: 16 [0/4977 (0%)]\tLoss: 0.008547\n",
      "Train Epoch: 16 [500/4977 (10%)]\tLoss: 0.108094\n",
      "Train Epoch: 16 [1000/4977 (20%)]\tLoss: 0.011423\n",
      "Train Epoch: 16 [1500/4977 (30%)]\tLoss: -0.035508\n",
      "Train Epoch: 16 [2000/4977 (40%)]\tLoss: -0.004968\n",
      "Train Epoch: 16 [2500/4977 (51%)]\tLoss: 0.016764\n",
      "Train Epoch: 16 [3000/4977 (61%)]\tLoss: -0.012583\n",
      "Train Epoch: 16 [3500/4977 (71%)]\tLoss: 0.036334\n",
      "Train Epoch: 16 [4000/4977 (81%)]\tLoss: -0.030296\n",
      "Train Epoch: 16 [4500/4977 (91%)]\tLoss: 0.127975\n",
      "====> Epoch: 16 Average train loss: 0.0364\n",
      "====> Epoch: 16 Average validation loss: -0.3155\n",
      "wall time: 1839.0284469127655s\n",
      "Train Epoch: 17 [0/4977 (0%)]\tLoss: 0.176814\n",
      "Train Epoch: 17 [500/4977 (10%)]\tLoss: 0.097333\n",
      "Train Epoch: 17 [1000/4977 (20%)]\tLoss: 0.153214\n",
      "Train Epoch: 17 [1500/4977 (30%)]\tLoss: 0.087410\n",
      "Train Epoch: 17 [2000/4977 (40%)]\tLoss: 0.045434\n",
      "Train Epoch: 17 [2500/4977 (51%)]\tLoss: 0.054632\n",
      "Train Epoch: 17 [3000/4977 (61%)]\tLoss: 0.116017\n",
      "Train Epoch: 17 [3500/4977 (71%)]\tLoss: 0.093007\n",
      "Train Epoch: 17 [4000/4977 (81%)]\tLoss: 0.121266\n",
      "Train Epoch: 17 [4500/4977 (91%)]\tLoss: -0.018809\n",
      "====> Epoch: 17 Average train loss: 0.0859\n",
      "====> Epoch: 17 Average validation loss: -0.4282\n",
      "wall time: 1953.5319347381592s\n",
      "Train Epoch: 18 [0/4977 (0%)]\tLoss: 0.112547\n",
      "Train Epoch: 18 [500/4977 (10%)]\tLoss: 0.023195\n",
      "Train Epoch: 18 [1000/4977 (20%)]\tLoss: -0.031967\n",
      "Train Epoch: 18 [1500/4977 (30%)]\tLoss: 0.066675\n",
      "Train Epoch: 18 [2000/4977 (40%)]\tLoss: 0.010946\n",
      "Train Epoch: 18 [2500/4977 (51%)]\tLoss: 0.108409\n",
      "Train Epoch: 18 [3000/4977 (61%)]\tLoss: -0.070074\n",
      "Train Epoch: 18 [3500/4977 (71%)]\tLoss: 0.009911\n",
      "Train Epoch: 18 [4000/4977 (81%)]\tLoss: 0.019196\n",
      "Train Epoch: 18 [4500/4977 (91%)]\tLoss: -0.018578\n",
      "====> Epoch: 18 Average train loss: 0.0376\n",
      "====> Epoch: 18 Average validation loss: -0.4480\n",
      "wall time: 2068.315532207489s\n",
      "Train Epoch: 19 [0/4977 (0%)]\tLoss: 0.028399\n",
      "Train Epoch: 19 [500/4977 (10%)]\tLoss: 0.019963\n",
      "Train Epoch: 19 [1000/4977 (20%)]\tLoss: 0.047632\n",
      "Train Epoch: 19 [1500/4977 (30%)]\tLoss: 0.014643\n",
      "Train Epoch: 19 [2000/4977 (40%)]\tLoss: 0.044605\n",
      "Train Epoch: 19 [2500/4977 (51%)]\tLoss: -0.001483\n",
      "Train Epoch: 19 [3000/4977 (61%)]\tLoss: -0.048692\n",
      "Train Epoch: 19 [3500/4977 (71%)]\tLoss: 0.004922\n",
      "Train Epoch: 19 [4000/4977 (81%)]\tLoss: 0.044372\n",
      "Train Epoch: 19 [4500/4977 (91%)]\tLoss: -0.044256\n",
      "====> Epoch: 19 Average train loss: 0.0079\n",
      "====> Epoch: 19 Average validation loss: -0.4626\n",
      "wall time: 2183.0642099380493s\n",
      "Train Epoch: 20 [0/4977 (0%)]\tLoss: 0.025995\n",
      "Train Epoch: 20 [500/4977 (10%)]\tLoss: 0.015236\n",
      "Train Epoch: 20 [1000/4977 (20%)]\tLoss: -0.025054\n",
      "Train Epoch: 20 [1500/4977 (30%)]\tLoss: 0.024916\n",
      "Train Epoch: 20 [2000/4977 (40%)]\tLoss: -0.003551\n",
      "Train Epoch: 20 [2500/4977 (51%)]\tLoss: -0.019939\n",
      "Train Epoch: 20 [3000/4977 (61%)]\tLoss: 0.005622\n",
      "Train Epoch: 20 [3500/4977 (71%)]\tLoss: -0.077669\n",
      "Train Epoch: 20 [4000/4977 (81%)]\tLoss: 0.081636\n",
      "Train Epoch: 20 [4500/4977 (91%)]\tLoss: -0.021304\n",
      "====> Epoch: 20 Average train loss: -0.0263\n",
      "====> Epoch: 20 Average validation loss: -0.5063\n",
      "wall time: 2290.816257238388s\n",
      "Train Epoch: 21 [0/4977 (0%)]\tLoss: -0.097099\n",
      "Train Epoch: 21 [500/4977 (10%)]\tLoss: -0.003181\n",
      "Train Epoch: 21 [1000/4977 (20%)]\tLoss: -0.030901\n",
      "Train Epoch: 21 [1500/4977 (30%)]\tLoss: -0.031674\n",
      "Train Epoch: 21 [2000/4977 (40%)]\tLoss: -0.021135\n",
      "Train Epoch: 21 [2500/4977 (51%)]\tLoss: 0.011424\n",
      "Train Epoch: 21 [3000/4977 (61%)]\tLoss: -0.098619\n",
      "Train Epoch: 21 [3500/4977 (71%)]\tLoss: 0.000299\n",
      "Train Epoch: 21 [4000/4977 (81%)]\tLoss: -0.072167\n",
      "Train Epoch: 21 [4500/4977 (91%)]\tLoss: 0.073102\n",
      "====> Epoch: 21 Average train loss: -0.0578\n",
      "====> Epoch: 21 Average validation loss: -0.5131\n",
      "wall time: 2405.521641254425s\n",
      "Train Epoch: 22 [0/4977 (0%)]\tLoss: -0.043882\n",
      "Train Epoch: 22 [500/4977 (10%)]\tLoss: -0.132373\n",
      "Train Epoch: 22 [1000/4977 (20%)]\tLoss: -0.044875\n",
      "Train Epoch: 22 [1500/4977 (30%)]\tLoss: -0.125333\n",
      "Train Epoch: 22 [2000/4977 (40%)]\tLoss: -0.152110\n",
      "Train Epoch: 22 [2500/4977 (51%)]\tLoss: -0.140448\n",
      "Train Epoch: 22 [3000/4977 (61%)]\tLoss: -0.168814\n",
      "Train Epoch: 22 [3500/4977 (71%)]\tLoss: -0.131717\n",
      "Train Epoch: 22 [4000/4977 (81%)]\tLoss: -0.159009\n",
      "Train Epoch: 22 [4500/4977 (91%)]\tLoss: -0.068908\n",
      "====> Epoch: 22 Average train loss: -0.0798\n",
      "====> Epoch: 22 Average validation loss: -0.5607\n",
      "wall time: 2520.281669855118s\n",
      "Train Epoch: 23 [0/4977 (0%)]\tLoss: -0.046443\n",
      "Train Epoch: 23 [500/4977 (10%)]\tLoss: -0.088159\n",
      "Train Epoch: 23 [1000/4977 (20%)]\tLoss: -0.030942\n",
      "Train Epoch: 23 [1500/4977 (30%)]\tLoss: -0.189526\n",
      "Train Epoch: 23 [2000/4977 (40%)]\tLoss: -0.072183\n",
      "Train Epoch: 23 [2500/4977 (51%)]\tLoss: -0.013606\n",
      "Train Epoch: 23 [3000/4977 (61%)]\tLoss: -0.026737\n",
      "Train Epoch: 23 [3500/4977 (71%)]\tLoss: -0.076337\n",
      "Train Epoch: 23 [4000/4977 (81%)]\tLoss: -0.073937\n",
      "Train Epoch: 23 [4500/4977 (91%)]\tLoss: -0.057935\n",
      "====> Epoch: 23 Average train loss: -0.0604\n",
      "====> Epoch: 23 Average validation loss: -0.5545\n",
      "wall time: 2634.6627638339996s\n",
      "Train Epoch: 24 [0/4977 (0%)]\tLoss: -0.111922\n",
      "Train Epoch: 24 [500/4977 (10%)]\tLoss: -0.149136\n",
      "Train Epoch: 24 [1000/4977 (20%)]\tLoss: -0.075122\n",
      "Train Epoch: 24 [1500/4977 (30%)]\tLoss: -0.152248\n",
      "Train Epoch: 24 [2000/4977 (40%)]\tLoss: -0.177277\n",
      "Train Epoch: 24 [2500/4977 (51%)]\tLoss: -0.089272\n",
      "Train Epoch: 24 [3000/4977 (61%)]\tLoss: -0.014639\n",
      "Train Epoch: 24 [3500/4977 (71%)]\tLoss: -0.030011\n",
      "Train Epoch: 24 [4000/4977 (81%)]\tLoss: 0.061512\n",
      "Train Epoch: 24 [4500/4977 (91%)]\tLoss: 0.069601\n",
      "====> Epoch: 24 Average train loss: 0.1184\n",
      "====> Epoch: 24 Average validation loss: 2.3740\n",
      "wall time: 2749.5089616775513s\n",
      "Train Epoch: 25 [0/4977 (0%)]\tLoss: 2.814685\n",
      "Train Epoch: 25 [500/4977 (10%)]\tLoss: 2.682661\n",
      "Train Epoch: 25 [1000/4977 (20%)]\tLoss: 2.362931\n",
      "Train Epoch: 25 [1500/4977 (30%)]\tLoss: 2.211465\n",
      "Train Epoch: 25 [2000/4977 (40%)]\tLoss: 1.537995\n",
      "Train Epoch: 25 [2500/4977 (51%)]\tLoss: 1.265839\n",
      "Train Epoch: 25 [3000/4977 (61%)]\tLoss: 0.718171\n",
      "Train Epoch: 25 [3500/4977 (71%)]\tLoss: 0.707270\n",
      "Train Epoch: 25 [4000/4977 (81%)]\tLoss: 0.635307\n",
      "Train Epoch: 25 [4500/4977 (91%)]\tLoss: 0.601047\n",
      "====> Epoch: 25 Average train loss: 1.4452\n",
      "====> Epoch: 25 Average validation loss: 0.2955\n",
      "wall time: 2864.4263796806335s\n",
      "Train Epoch: 26 [0/4977 (0%)]\tLoss: 0.766210\n",
      "Train Epoch: 26 [500/4977 (10%)]\tLoss: 0.583422\n",
      "Train Epoch: 26 [1000/4977 (20%)]\tLoss: 0.494029\n",
      "Train Epoch: 26 [1500/4977 (30%)]\tLoss: 0.493349\n",
      "Train Epoch: 26 [2000/4977 (40%)]\tLoss: 0.496548\n",
      "Train Epoch: 26 [2500/4977 (51%)]\tLoss: 0.583428\n",
      "Train Epoch: 26 [3000/4977 (61%)]\tLoss: 0.519100\n",
      "Train Epoch: 26 [3500/4977 (71%)]\tLoss: 0.537620\n",
      "Train Epoch: 26 [4000/4977 (81%)]\tLoss: 0.428415\n",
      "Train Epoch: 26 [4500/4977 (91%)]\tLoss: 0.527793\n",
      "====> Epoch: 26 Average train loss: 0.5326\n",
      "====> Epoch: 26 Average validation loss: 0.0101\n",
      "wall time: 2981.3047659397125s\n",
      "Train Epoch: 27 [0/4977 (0%)]\tLoss: 0.511609\n",
      "Train Epoch: 27 [500/4977 (10%)]\tLoss: 0.520792\n",
      "Train Epoch: 27 [1000/4977 (20%)]\tLoss: 0.321501\n",
      "Train Epoch: 27 [1500/4977 (30%)]\tLoss: 0.528564\n",
      "Train Epoch: 27 [2000/4977 (40%)]\tLoss: 0.380746\n",
      "Train Epoch: 27 [2500/4977 (51%)]\tLoss: 0.388118\n",
      "Train Epoch: 27 [3000/4977 (61%)]\tLoss: 0.387419\n",
      "Train Epoch: 27 [3500/4977 (71%)]\tLoss: 0.387940\n",
      "Train Epoch: 27 [4000/4977 (81%)]\tLoss: 0.414005\n",
      "Train Epoch: 27 [4500/4977 (91%)]\tLoss: 0.447578\n",
      "====> Epoch: 27 Average train loss: 0.4328\n",
      "====> Epoch: 27 Average validation loss: -0.0364\n",
      "wall time: 3100.4642074108124s\n",
      "Train Epoch: 28 [0/4977 (0%)]\tLoss: 0.448141\n",
      "Train Epoch: 28 [500/4977 (10%)]\tLoss: 0.365087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28 [1000/4977 (20%)]\tLoss: 0.451446\n",
      "Train Epoch: 28 [1500/4977 (30%)]\tLoss: 0.402816\n",
      "Train Epoch: 28 [2000/4977 (40%)]\tLoss: 0.392351\n",
      "Train Epoch: 28 [2500/4977 (51%)]\tLoss: 0.322281\n",
      "Train Epoch: 28 [3000/4977 (61%)]\tLoss: 0.371765\n",
      "Train Epoch: 28 [3500/4977 (71%)]\tLoss: 0.391027\n",
      "Train Epoch: 28 [4000/4977 (81%)]\tLoss: 0.314722\n",
      "Train Epoch: 28 [4500/4977 (91%)]\tLoss: 0.395815\n",
      "====> Epoch: 28 Average train loss: 0.3764\n",
      "====> Epoch: 28 Average validation loss: -0.1455\n",
      "wall time: 3219.487847328186s\n",
      "Train Epoch: 29 [0/4977 (0%)]\tLoss: 0.292911\n",
      "Train Epoch: 29 [500/4977 (10%)]\tLoss: 0.396078\n",
      "Train Epoch: 29 [1000/4977 (20%)]\tLoss: 0.364565\n",
      "Train Epoch: 29 [1500/4977 (30%)]\tLoss: 0.325419\n",
      "Train Epoch: 29 [2000/4977 (40%)]\tLoss: 0.307071\n",
      "Train Epoch: 29 [2500/4977 (51%)]\tLoss: 0.278637\n",
      "Train Epoch: 29 [3000/4977 (61%)]\tLoss: 0.279326\n",
      "Train Epoch: 29 [3500/4977 (71%)]\tLoss: 0.321351\n",
      "Train Epoch: 29 [4000/4977 (81%)]\tLoss: 0.410274\n",
      "Train Epoch: 29 [4500/4977 (91%)]\tLoss: 0.367783\n",
      "====> Epoch: 29 Average train loss: 0.3325\n",
      "====> Epoch: 29 Average validation loss: -0.1795\n",
      "wall time: 3338.5430591106415s\n",
      "Train Epoch: 30 [0/4977 (0%)]\tLoss: 0.390898\n",
      "Train Epoch: 30 [500/4977 (10%)]\tLoss: 0.368738\n",
      "Train Epoch: 30 [1000/4977 (20%)]\tLoss: 0.298876\n",
      "Train Epoch: 30 [1500/4977 (30%)]\tLoss: 0.269897\n",
      "Train Epoch: 30 [2000/4977 (40%)]\tLoss: 0.258205\n",
      "Train Epoch: 30 [2500/4977 (51%)]\tLoss: 0.328150\n",
      "Train Epoch: 30 [3000/4977 (61%)]\tLoss: 0.342794\n",
      "Train Epoch: 30 [3500/4977 (71%)]\tLoss: 0.268313\n",
      "Train Epoch: 30 [4000/4977 (81%)]\tLoss: 0.340939\n",
      "Train Epoch: 30 [4500/4977 (91%)]\tLoss: 0.230024\n",
      "====> Epoch: 30 Average train loss: 0.3063\n",
      "====> Epoch: 30 Average validation loss: -0.2090\n",
      "wall time: 3457.7806210517883s\n",
      "Train Epoch: 31 [0/4977 (0%)]\tLoss: 0.330425\n",
      "Train Epoch: 31 [500/4977 (10%)]\tLoss: 0.351473\n",
      "Train Epoch: 31 [1000/4977 (20%)]\tLoss: 0.316522\n",
      "Train Epoch: 31 [1500/4977 (30%)]\tLoss: 0.300452\n",
      "Train Epoch: 31 [2000/4977 (40%)]\tLoss: 0.266043\n",
      "Train Epoch: 31 [2500/4977 (51%)]\tLoss: 0.260460\n",
      "Train Epoch: 31 [3000/4977 (61%)]\tLoss: 0.160262\n",
      "Train Epoch: 31 [3500/4977 (71%)]\tLoss: 0.329635\n",
      "Train Epoch: 31 [4000/4977 (81%)]\tLoss: 0.403726\n",
      "Train Epoch: 31 [4500/4977 (91%)]\tLoss: 0.296481\n",
      "====> Epoch: 31 Average train loss: 0.2780\n",
      "====> Epoch: 31 Average validation loss: -0.2056\n",
      "wall time: 3576.979803800583s\n",
      "Train Epoch: 32 [0/4977 (0%)]\tLoss: 0.304624\n",
      "Train Epoch: 32 [500/4977 (10%)]\tLoss: 0.318317\n",
      "Train Epoch: 32 [1000/4977 (20%)]\tLoss: 0.228907\n",
      "Train Epoch: 32 [1500/4977 (30%)]\tLoss: 0.269841\n",
      "Train Epoch: 32 [2000/4977 (40%)]\tLoss: 0.302899\n",
      "Train Epoch: 32 [2500/4977 (51%)]\tLoss: 0.279644\n",
      "Train Epoch: 32 [3000/4977 (61%)]\tLoss: 0.248187\n",
      "Train Epoch: 32 [3500/4977 (71%)]\tLoss: 0.189584\n",
      "Train Epoch: 32 [4000/4977 (81%)]\tLoss: 0.262382\n",
      "Train Epoch: 32 [4500/4977 (91%)]\tLoss: 0.227953\n",
      "====> Epoch: 32 Average train loss: 0.2497\n",
      "====> Epoch: 32 Average validation loss: -0.2297\n",
      "wall time: 3695.8286838531494s\n",
      "Train Epoch: 33 [0/4977 (0%)]\tLoss: 0.113626\n",
      "Train Epoch: 33 [500/4977 (10%)]\tLoss: 0.219905\n",
      "Train Epoch: 33 [1000/4977 (20%)]\tLoss: 0.263863\n",
      "Train Epoch: 33 [1500/4977 (30%)]\tLoss: 0.204724\n",
      "Train Epoch: 33 [2000/4977 (40%)]\tLoss: 0.319837\n",
      "Train Epoch: 33 [2500/4977 (51%)]\tLoss: 0.268029\n",
      "Train Epoch: 33 [3000/4977 (61%)]\tLoss: 0.182931\n",
      "Train Epoch: 33 [3500/4977 (71%)]\tLoss: 0.251942\n",
      "Train Epoch: 33 [4000/4977 (81%)]\tLoss: 0.248080\n",
      "Train Epoch: 33 [4500/4977 (91%)]\tLoss: 0.229260\n",
      "====> Epoch: 33 Average train loss: 0.2368\n",
      "====> Epoch: 33 Average validation loss: -0.2571\n",
      "wall time: 3815.0167031288147s\n",
      "Train Epoch: 34 [0/4977 (0%)]\tLoss: 0.180839\n",
      "Train Epoch: 34 [500/4977 (10%)]\tLoss: 0.053955\n",
      "Train Epoch: 34 [1000/4977 (20%)]\tLoss: 0.193538\n",
      "Train Epoch: 34 [1500/4977 (30%)]\tLoss: 0.214548\n",
      "Train Epoch: 34 [2000/4977 (40%)]\tLoss: 0.292108\n",
      "Train Epoch: 34 [2500/4977 (51%)]\tLoss: 0.293381\n",
      "Train Epoch: 34 [3000/4977 (61%)]\tLoss: 0.156347\n",
      "Train Epoch: 34 [3500/4977 (71%)]\tLoss: 0.202113\n",
      "Train Epoch: 34 [4000/4977 (81%)]\tLoss: 0.310239\n",
      "Train Epoch: 34 [4500/4977 (91%)]\tLoss: 0.310650\n",
      "====> Epoch: 34 Average train loss: 0.2157\n",
      "====> Epoch: 34 Average validation loss: -0.2659\n",
      "wall time: 3934.1850786209106s\n",
      "Train Epoch: 35 [0/4977 (0%)]\tLoss: 0.220088\n",
      "Train Epoch: 35 [500/4977 (10%)]\tLoss: 0.257120\n",
      "Train Epoch: 35 [1000/4977 (20%)]\tLoss: 0.277921\n",
      "Train Epoch: 35 [1500/4977 (30%)]\tLoss: 0.152492\n",
      "Train Epoch: 35 [2000/4977 (40%)]\tLoss: 0.253495\n",
      "Train Epoch: 35 [2500/4977 (51%)]\tLoss: 0.200060\n",
      "Train Epoch: 35 [3000/4977 (61%)]\tLoss: 0.192016\n",
      "Train Epoch: 35 [3500/4977 (71%)]\tLoss: 0.177788\n",
      "Train Epoch: 35 [4000/4977 (81%)]\tLoss: 0.215826\n",
      "Train Epoch: 35 [4500/4977 (91%)]\tLoss: 0.215909\n",
      "====> Epoch: 35 Average train loss: 0.2062\n",
      "====> Epoch: 35 Average validation loss: -0.2929\n",
      "wall time: 4053.118749141693s\n",
      "Train Epoch: 36 [0/4977 (0%)]\tLoss: 0.157239\n",
      "Train Epoch: 36 [500/4977 (10%)]\tLoss: 0.168609\n",
      "Train Epoch: 36 [1000/4977 (20%)]\tLoss: 0.153843\n",
      "Train Epoch: 36 [1500/4977 (30%)]\tLoss: 0.101936\n",
      "Train Epoch: 36 [2000/4977 (40%)]\tLoss: 0.231103\n",
      "Train Epoch: 36 [2500/4977 (51%)]\tLoss: 0.107583\n",
      "Train Epoch: 36 [3000/4977 (61%)]\tLoss: 0.160321\n",
      "Train Epoch: 36 [3500/4977 (71%)]\tLoss: 0.086271\n",
      "Train Epoch: 36 [4000/4977 (81%)]\tLoss: 0.189153\n",
      "Train Epoch: 36 [4500/4977 (91%)]\tLoss: 0.291747\n",
      "====> Epoch: 36 Average train loss: 0.1821\n",
      "====> Epoch: 36 Average validation loss: -0.2959\n",
      "wall time: 4172.410207033157s\n",
      "Train Epoch: 37 [0/4977 (0%)]\tLoss: 0.203428\n",
      "Train Epoch: 37 [500/4977 (10%)]\tLoss: 0.060797\n",
      "Train Epoch: 37 [1000/4977 (20%)]\tLoss: 0.081771\n",
      "Train Epoch: 37 [1500/4977 (30%)]\tLoss: 0.145089\n",
      "Train Epoch: 37 [2000/4977 (40%)]\tLoss: 0.197963\n",
      "Train Epoch: 37 [2500/4977 (51%)]\tLoss: 0.182080\n",
      "Train Epoch: 37 [3000/4977 (61%)]\tLoss: 0.099673\n",
      "Train Epoch: 37 [3500/4977 (71%)]\tLoss: 0.207043\n",
      "Train Epoch: 37 [4000/4977 (81%)]\tLoss: 0.170020\n",
      "Train Epoch: 37 [4500/4977 (91%)]\tLoss: 0.196525\n",
      "====> Epoch: 37 Average train loss: 0.1740\n",
      "====> Epoch: 37 Average validation loss: -0.3059\n",
      "wall time: 4291.567979335785s\n",
      "Train Epoch: 38 [0/4977 (0%)]\tLoss: 0.179462\n",
      "Train Epoch: 38 [500/4977 (10%)]\tLoss: 0.170915\n",
      "Train Epoch: 38 [1000/4977 (20%)]\tLoss: 0.157014\n",
      "Train Epoch: 38 [1500/4977 (30%)]\tLoss: 0.037129\n",
      "Train Epoch: 38 [2000/4977 (40%)]\tLoss: 0.300454\n",
      "Train Epoch: 38 [2500/4977 (51%)]\tLoss: 0.192749\n",
      "Train Epoch: 38 [3000/4977 (61%)]\tLoss: 0.149911\n",
      "Train Epoch: 38 [3500/4977 (71%)]\tLoss: 0.134311\n",
      "Train Epoch: 38 [4000/4977 (81%)]\tLoss: 0.325976\n",
      "Train Epoch: 38 [4500/4977 (91%)]\tLoss: 0.063090\n",
      "====> Epoch: 38 Average train loss: 0.1600\n",
      "====> Epoch: 38 Average validation loss: -0.3208\n",
      "wall time: 4406.244713544846s\n",
      "Train Epoch: 39 [0/4977 (0%)]\tLoss: 0.199317\n",
      "Train Epoch: 39 [500/4977 (10%)]\tLoss: 0.254624\n",
      "Train Epoch: 39 [1000/4977 (20%)]\tLoss: 0.137725\n",
      "Train Epoch: 39 [1500/4977 (30%)]\tLoss: 0.097343\n",
      "Train Epoch: 39 [2000/4977 (40%)]\tLoss: 0.132727\n",
      "Train Epoch: 39 [2500/4977 (51%)]\tLoss: 0.192226\n",
      "Train Epoch: 39 [3000/4977 (61%)]\tLoss: 0.174341\n",
      "Train Epoch: 39 [3500/4977 (71%)]\tLoss: 0.202296\n",
      "Train Epoch: 39 [4000/4977 (81%)]\tLoss: 0.175733\n",
      "Train Epoch: 39 [4500/4977 (91%)]\tLoss: 0.136676\n",
      "====> Epoch: 39 Average train loss: 0.1471\n",
      "====> Epoch: 39 Average validation loss: -0.3309\n",
      "wall time: 4520.935904979706s\n",
      "Train Epoch: 40 [0/4977 (0%)]\tLoss: 0.124608\n",
      "Train Epoch: 40 [500/4977 (10%)]\tLoss: 0.027503\n",
      "Train Epoch: 40 [1000/4977 (20%)]\tLoss: 0.150487\n",
      "Train Epoch: 40 [1500/4977 (30%)]\tLoss: 0.114775\n",
      "Train Epoch: 40 [2000/4977 (40%)]\tLoss: 0.063481\n",
      "Train Epoch: 40 [2500/4977 (51%)]\tLoss: 0.065626\n",
      "Train Epoch: 40 [3000/4977 (61%)]\tLoss: 0.126996\n",
      "Train Epoch: 40 [3500/4977 (71%)]\tLoss: 0.071026\n",
      "Train Epoch: 40 [4000/4977 (81%)]\tLoss: 0.194572\n",
      "Train Epoch: 40 [4500/4977 (91%)]\tLoss: 0.031315\n",
      "====> Epoch: 40 Average train loss: 0.1346\n",
      "====> Epoch: 40 Average validation loss: -0.3201\n",
      "wall time: 4635.844252586365s\n",
      "Train Epoch: 41 [0/4977 (0%)]\tLoss: 0.124806\n",
      "Train Epoch: 41 [500/4977 (10%)]\tLoss: 0.161561\n",
      "Train Epoch: 41 [1000/4977 (20%)]\tLoss: 0.097231\n",
      "Train Epoch: 41 [1500/4977 (30%)]\tLoss: 0.117207\n",
      "Train Epoch: 41 [2000/4977 (40%)]\tLoss: 0.163728\n",
      "Train Epoch: 41 [2500/4977 (51%)]\tLoss: 0.212640\n",
      "Train Epoch: 41 [3000/4977 (61%)]\tLoss: 0.097916\n",
      "Train Epoch: 41 [3500/4977 (71%)]\tLoss: 0.097071\n",
      "Train Epoch: 41 [4000/4977 (81%)]\tLoss: 0.185145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 41 [4500/4977 (91%)]\tLoss: 0.137804\n",
      "====> Epoch: 41 Average train loss: 0.1251\n",
      "====> Epoch: 41 Average validation loss: -0.3558\n",
      "wall time: 4740.067611217499s\n",
      "Train Epoch: 42 [0/4977 (0%)]\tLoss: 0.034845\n",
      "Train Epoch: 42 [500/4977 (10%)]\tLoss: 0.113422\n",
      "Train Epoch: 42 [1000/4977 (20%)]\tLoss: 0.162848\n",
      "Train Epoch: 42 [1500/4977 (30%)]\tLoss: 0.120060\n",
      "Train Epoch: 42 [2000/4977 (40%)]\tLoss: 0.104378\n",
      "Train Epoch: 42 [2500/4977 (51%)]\tLoss: 0.021528\n",
      "Train Epoch: 42 [3000/4977 (61%)]\tLoss: 0.100538\n",
      "Train Epoch: 42 [3500/4977 (71%)]\tLoss: 0.141493\n",
      "Train Epoch: 42 [4000/4977 (81%)]\tLoss: 0.215145\n",
      "Train Epoch: 42 [4500/4977 (91%)]\tLoss: 0.146952\n",
      "====> Epoch: 42 Average train loss: 0.1082\n",
      "====> Epoch: 42 Average validation loss: -0.3363\n",
      "wall time: 4843.881251811981s\n",
      "Train Epoch: 43 [0/4977 (0%)]\tLoss: 0.162088\n",
      "Train Epoch: 43 [500/4977 (10%)]\tLoss: 0.032233\n",
      "Train Epoch: 43 [1000/4977 (20%)]\tLoss: 0.094111\n",
      "Train Epoch: 43 [1500/4977 (30%)]\tLoss: 0.056549\n",
      "Train Epoch: 43 [2000/4977 (40%)]\tLoss: 0.158100\n",
      "Train Epoch: 43 [2500/4977 (51%)]\tLoss: 0.202686\n",
      "Train Epoch: 43 [3000/4977 (61%)]\tLoss: 0.106246\n",
      "Train Epoch: 43 [3500/4977 (71%)]\tLoss: 0.161440\n",
      "Train Epoch: 43 [4000/4977 (81%)]\tLoss: 0.113919\n",
      "Train Epoch: 43 [4500/4977 (91%)]\tLoss: 0.117914\n",
      "====> Epoch: 43 Average train loss: 0.0929\n",
      "====> Epoch: 43 Average validation loss: -0.3555\n",
      "wall time: 4947.905767440796s\n",
      "Train Epoch: 44 [0/4977 (0%)]\tLoss: 0.092426\n",
      "Train Epoch: 44 [500/4977 (10%)]\tLoss: 0.152040\n",
      "Train Epoch: 44 [1000/4977 (20%)]\tLoss: 0.100620\n",
      "Train Epoch: 44 [1500/4977 (30%)]\tLoss: 0.125971\n",
      "Train Epoch: 44 [2000/4977 (40%)]\tLoss: 0.017244\n",
      "Train Epoch: 44 [2500/4977 (51%)]\tLoss: 0.117469\n",
      "Train Epoch: 44 [3000/4977 (61%)]\tLoss: 0.040679\n",
      "Train Epoch: 44 [3500/4977 (71%)]\tLoss: 0.075286\n",
      "Train Epoch: 44 [4000/4977 (81%)]\tLoss: 0.091160\n",
      "Train Epoch: 44 [4500/4977 (91%)]\tLoss: -0.025503\n",
      "====> Epoch: 44 Average train loss: 0.0860\n",
      "====> Epoch: 44 Average validation loss: -0.3670\n",
      "wall time: 5052.983860731125s\n",
      "Train Epoch: 45 [0/4977 (0%)]\tLoss: 0.105344\n",
      "Train Epoch: 45 [500/4977 (10%)]\tLoss: 0.079185\n",
      "Train Epoch: 45 [1000/4977 (20%)]\tLoss: 0.105621\n",
      "Train Epoch: 45 [1500/4977 (30%)]\tLoss: 0.067598\n",
      "Train Epoch: 45 [2000/4977 (40%)]\tLoss: 0.107237\n",
      "Train Epoch: 45 [2500/4977 (51%)]\tLoss: 0.141031\n",
      "Train Epoch: 45 [3000/4977 (61%)]\tLoss: 0.070053\n",
      "Train Epoch: 45 [3500/4977 (71%)]\tLoss: 0.065226\n",
      "Train Epoch: 45 [4000/4977 (81%)]\tLoss: 0.014187\n",
      "Train Epoch: 45 [4500/4977 (91%)]\tLoss: 0.112403\n",
      "====> Epoch: 45 Average train loss: 0.0764\n",
      "====> Epoch: 45 Average validation loss: -0.2797\n",
      "wall time: 5156.956275224686s\n",
      "Train Epoch: 46 [0/4977 (0%)]\tLoss: 0.070879\n",
      "Train Epoch: 46 [500/4977 (10%)]\tLoss: 0.120378\n",
      "Train Epoch: 46 [1000/4977 (20%)]\tLoss: 0.099973\n",
      "Train Epoch: 46 [1500/4977 (30%)]\tLoss: 0.017081\n",
      "Train Epoch: 46 [2000/4977 (40%)]\tLoss: 0.003679\n",
      "Train Epoch: 46 [2500/4977 (51%)]\tLoss: 0.044707\n",
      "Train Epoch: 46 [3000/4977 (61%)]\tLoss: 0.057992\n",
      "Train Epoch: 46 [3500/4977 (71%)]\tLoss: 0.046906\n",
      "Train Epoch: 46 [4000/4977 (81%)]\tLoss: 0.114928\n",
      "Train Epoch: 46 [4500/4977 (91%)]\tLoss: 0.042827\n",
      "====> Epoch: 46 Average train loss: 0.0651\n",
      "====> Epoch: 46 Average validation loss: -0.3946\n",
      "wall time: 5260.8094830513s\n",
      "Train Epoch: 47 [0/4977 (0%)]\tLoss: 0.076033\n",
      "Train Epoch: 47 [500/4977 (10%)]\tLoss: 0.039866\n",
      "Train Epoch: 47 [1000/4977 (20%)]\tLoss: -0.002358\n",
      "Train Epoch: 47 [1500/4977 (30%)]\tLoss: 0.100311\n",
      "Train Epoch: 47 [2000/4977 (40%)]\tLoss: 0.017307\n",
      "Train Epoch: 47 [2500/4977 (51%)]\tLoss: 0.094915\n",
      "Train Epoch: 47 [3000/4977 (61%)]\tLoss: 0.082826\n",
      "Train Epoch: 47 [3500/4977 (71%)]\tLoss: 0.061127\n",
      "Train Epoch: 47 [4000/4977 (81%)]\tLoss: 0.033144\n",
      "Train Epoch: 47 [4500/4977 (91%)]\tLoss: 0.063583\n",
      "====> Epoch: 47 Average train loss: 0.0481\n",
      "====> Epoch: 47 Average validation loss: -0.4039\n",
      "wall time: 5364.3154356479645s\n",
      "Train Epoch: 48 [0/4977 (0%)]\tLoss: 0.085810\n",
      "Train Epoch: 48 [500/4977 (10%)]\tLoss: 0.017696\n",
      "Train Epoch: 48 [1000/4977 (20%)]\tLoss: 0.051433\n",
      "Train Epoch: 48 [1500/4977 (30%)]\tLoss: 0.087207\n",
      "Train Epoch: 48 [2000/4977 (40%)]\tLoss: 0.086598\n",
      "Train Epoch: 48 [2500/4977 (51%)]\tLoss: 0.034313\n",
      "Train Epoch: 48 [3000/4977 (61%)]\tLoss: 0.024764\n",
      "Train Epoch: 48 [3500/4977 (71%)]\tLoss: -0.000012\n",
      "Train Epoch: 48 [4000/4977 (81%)]\tLoss: -0.073362\n",
      "Train Epoch: 48 [4500/4977 (91%)]\tLoss: -0.050324\n",
      "====> Epoch: 48 Average train loss: 0.0404\n",
      "====> Epoch: 48 Average validation loss: -0.4066\n",
      "wall time: 5468.298925638199s\n",
      "Train Epoch: 49 [0/4977 (0%)]\tLoss: 0.026984\n",
      "Train Epoch: 49 [500/4977 (10%)]\tLoss: 0.036511\n",
      "Train Epoch: 49 [1000/4977 (20%)]\tLoss: 0.134946\n",
      "Train Epoch: 49 [1500/4977 (30%)]\tLoss: 0.063317\n",
      "Train Epoch: 49 [2000/4977 (40%)]\tLoss: 0.053882\n",
      "Train Epoch: 49 [2500/4977 (51%)]\tLoss: 0.060856\n",
      "Train Epoch: 49 [3000/4977 (61%)]\tLoss: -0.009179\n",
      "Train Epoch: 49 [3500/4977 (71%)]\tLoss: -0.056463\n",
      "Train Epoch: 49 [4000/4977 (81%)]\tLoss: 0.008247\n",
      "Train Epoch: 49 [4500/4977 (91%)]\tLoss: -0.040849\n",
      "====> Epoch: 49 Average train loss: 0.0287\n",
      "====> Epoch: 49 Average validation loss: -0.4300\n",
      "wall time: 5572.266665458679s\n",
      "Train Epoch: 50 [0/4977 (0%)]\tLoss: 0.105916\n",
      "Train Epoch: 50 [500/4977 (10%)]\tLoss: 0.044063\n",
      "Train Epoch: 50 [1000/4977 (20%)]\tLoss: 0.018009\n",
      "Train Epoch: 50 [1500/4977 (30%)]\tLoss: -0.006552\n",
      "Train Epoch: 50 [2000/4977 (40%)]\tLoss: 0.021665\n",
      "Train Epoch: 50 [2500/4977 (51%)]\tLoss: 0.047010\n",
      "Train Epoch: 50 [3000/4977 (61%)]\tLoss: 0.046588\n",
      "Train Epoch: 50 [3500/4977 (71%)]\tLoss: 0.062620\n",
      "Train Epoch: 50 [4000/4977 (81%)]\tLoss: 0.017737\n",
      "Train Epoch: 50 [4500/4977 (91%)]\tLoss: 0.011930\n",
      "====> Epoch: 50 Average train loss: 0.0145\n",
      "====> Epoch: 50 Average validation loss: -0.4238\n",
      "wall time: 5676.033495187759s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args['task'] == 'synthesis':\n",
    "        synthesis_train(args, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_plot(phis):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    phis= phis/(np.sum(phis, axis = 0, keepdims=True))\n",
    "    plt.xlabel('handwriting generation')\n",
    "    plt.ylabel('text scanning')\n",
    "    plt.imshow(phis, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conditionally(text, cell_size=400, num_clusters=20, K=20, random_state=700, \\\n",
    "                            bias=1., bias2=1., state_dict_file='save/synthesis_epoch_50.pt'):\n",
    "    \n",
    "    char_to_code = torch.load('char_to_code.pt')\n",
    "    np.random.seed(random_state)\n",
    "    text = text + ' '\n",
    "    \n",
    "    model = LSTMSynthesis(len(text), len(char_to_code)+1, cell_size, num_clusters, K)\n",
    "    model.load_state_dict(torch.load(state_dict_file)['model'])\n",
    "    \n",
    "    onehots = np.zeros((len(text), len(char_to_code)+1))\n",
    "    for _ in range(len(text)):\n",
    "        try:\n",
    "            onehots[_][char_to_code[text[_]]] = 1\n",
    "        except:\n",
    "            onehots[_][-1] = 1\n",
    "    \n",
    "    zero_tensor = torch.zeros((1,1,3))\n",
    "    h1_init, c1_init = torch.zeros((1,cell_size)), torch.zeros((1,cell_size))\n",
    "    h2_init, c2_init = torch.zeros((1,1,cell_size)), torch.zeros((1,1,cell_size))\n",
    "    kappa_old = torch.zeros(1, K)\n",
    "    onehots = torch.from_numpy(onehots).type(torch.FloatTensor)\n",
    "    text_len = torch.from_numpy(np.array([[len(text)]])).type(torch.FloatTensor)\n",
    "    \n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "        zero_tensor = zero_tensor.cuda()\n",
    "        h1_init, c1_init = h1_init.cuda(), c1_init.cuda()\n",
    "        h2_init, c2_init = h2_init.cuda(), c2_init.cuda()\n",
    "        kappa_old = kappa_old.cuda()\n",
    "        onehots = onehots.cuda()\n",
    "        text_len = text_len.cuda()\n",
    "        \n",
    "    x = Variable(zero_tensor)\n",
    "    h1_init, c1_init = Variable(h1_init), Variable(c1_init)\n",
    "    h2_init, c2_init = Variable(h2_init), Variable(c2_init)\n",
    "    prev = (h1_init, c1_init)\n",
    "    prev2 = (h2_init, c2_init)\n",
    "    kappa_old = Variable(kappa_old)\n",
    "    onehots = Variable(onehots, requires_grad = False)\n",
    "    w_old = onehots.narrow(0,0,1)  # attention on the first input text char\n",
    "    text_len = Variable(text_len)\n",
    "    \n",
    "    record = [np.zeros(3)]\n",
    "    phis = []\n",
    "    stop = False\n",
    "    count = 0\n",
    "    while not stop:    \n",
    "        outputs = model(x, onehots, text_len, w_old, kappa_old, prev, prev2, bias)\n",
    "        end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, rho, w_old, kappa_old, prev, prev2, old_phi = outputs\n",
    "        \n",
    "        #bernoulli sample\n",
    "        prob_end = end.data[0][0][0]\n",
    "        sample_end = np.random.binomial(1,prob_end.item())\n",
    "\n",
    "        #mog sample\n",
    "        sample_index = np.random.choice(range(20),p = weights.data[0][0].cpu().numpy())\n",
    "        mu = np.array([mu_1.data[0][0][sample_index].item(), mu_2.data[0][0][sample_index].item()])\n",
    "        log_sigma_1 = log_sigma_1 - bias2\n",
    "        log_sigma_2 = log_sigma_2 - bias2\n",
    "        v1 = (log_sigma_1).exp().data[0][0][sample_index]**2\n",
    "        v2 = (log_sigma_2).exp().data[0][0][sample_index]**2\n",
    "        c = rho.data[0][0][sample_index]*log_sigma_1.exp().data[0][0][sample_index]\\\n",
    "            *log_sigma_2.exp().data[0][0][sample_index]\n",
    "        cov = np.array([[v1.item(),c.item()],[c.item(),v2.item()]])\n",
    "        sample_point = np.random.multivariate_normal(mu, cov)\n",
    "        \n",
    "        out = np.insert(sample_point,0,sample_end)\n",
    "        record.append(out)\n",
    "        x = torch.from_numpy(out).type(torch.FloatTensor)\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        x = Variable(x, requires_grad=False)\n",
    "        x = x.view(1,1,3)\n",
    "        \n",
    "        # attention\n",
    "        old_phi = old_phi.squeeze(0)\n",
    "        phis.append(old_phi)\n",
    "        old_phi = old_phi.data.cpu().numpy()\n",
    "        \n",
    "        # hack to prevent early exit (attention is unstable at the beginning)\n",
    "        if count >=20 and np.max(old_phi) == old_phi[-1]:\n",
    "            stop = True\n",
    "        count += 1\n",
    "        \n",
    "    phis = torch.stack(phis).data.cpu().numpy().T\n",
    "         \n",
    "    plot_stroke(np.array(record))\n",
    "    attention_plot(phis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/sashank.sridhar/miniconda3/envs/TripletLoss/lib/python3.9/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1676.87x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_conditionally('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HandwritingSynthesis_Conditionally.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
