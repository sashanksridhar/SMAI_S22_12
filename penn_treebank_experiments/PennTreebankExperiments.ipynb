{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PennTreebankExperiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!echo \"=== Acquiring datasets ===\"\n",
        "!echo \"---\"\n",
        "\n",
        "!mkdir -p data\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgHmdlPhL8oF",
        "outputId": "aa6336f4-446a-4e23-e69b-76ef7f67cfee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Acquiring datasets ===\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz_r7xiOI_6s",
        "outputId": "edc3838e-cbce-4dfa-efec-7f8a1cbc1a01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "L0wfV1YeMKok"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"- Downloading Penn Treebank (PTB)\"\n",
        "!wget --quiet --continue http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "!tar -xzf simple-examples.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK7K7MACI4nY",
        "outputId": "73c09e7d-b960-4b3c-d7f3-d71398222e19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Downloading Penn Treebank (PTB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p penn\n",
        "%cd penn\n",
        "!mv ../simple-examples/data/ptb.train.txt train.txt\n",
        "!mv ../simple-examples/data/ptb.test.txt test.txt\n",
        "!mv ../simple-examples/data/ptb.valid.txt valid.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdDffaaGJAer",
        "outputId": "1eb554c1-348a-48a5-f8c7-f51aa7f1e6b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data/penn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"- Downloading Penn Treebank (Character)\"\n",
        "!mkdir -p ../pennchar\n",
        "%cd ../pennchar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCTp_5tbNPhS",
        "outputId": "ae20e2a6-7912-42de-bd4d-4a52e24453d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Downloading Penn Treebank (Character)\n",
            "/content/data/pennchar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ../simple-examples/data/ptb.char.train.txt train.txt\n",
        "!mv ../simple-examples/data/ptb.char.test.txt test.txt\n",
        "!mv ../simple-examples/data/ptb.char.valid.txt valid.txt"
      ],
      "metadata": {
        "id": "74fCeou3JAqF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ../simple-examples/"
      ],
      "metadata": {
        "id": "5-sFaSl5NbMX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Character - Without Noise**"
      ],
      "metadata": {
        "id": "6rnPPAw67682"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp) # Token2Embeddings\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout) #(seq_len, batch_size, emb_size)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        self.init_weights()\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.05\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input size(bptt, bsz)\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        # emb size(bptt, bsz, embsize)\n",
        "        # hid size(layers, bsz, nhid)\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # output size(bptt, bsz, nhid)\n",
        "        output = self.drop(output)\n",
        "        # decoder: nhid -> ntoken\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        # LSTM h and c\n",
        "        weight = next(self.parameters()).data\n",
        "        return weight.new_zeros(self.nlayers, bsz, self.nhid), weight.new_zeros(self.nlayers, bsz, self.nhid)"
      ],
      "metadata": {
        "id": "m_kekY5EAKbx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = Counter()\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        token_id = self.word2idx[word]\n",
        "        self.counter[token_id] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids"
      ],
      "metadata": {
        "id": "3_xiIWpEAOGD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import math"
      ],
      "metadata": {
        "id": "RhzjMOhhAQxf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9y8nlquAgmg",
        "outputId": "033baaac-e2c5-4702-c648-d2c3a9e2622d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOf8u9RhAlcT",
        "outputId": "65c30ab5-2de1-42c4-d8bb-c17da3adf157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = '/content/data/pennchar'\n",
        "batch_size = 256\n",
        "emsize = 256\n",
        "nlayers = 1\n",
        "nhid = 1000\n",
        "lr = 0.0001\n",
        "dropout = 0.5\n",
        "checkpoint = ''\n",
        "clip = 1\n",
        "bptt = 35\n",
        "epochs = 10\n",
        "save = '/content/output/model_test_character_none.pt'\n",
        "\n",
        "torch.manual_seed(1111)\n",
        "\n",
        "# Load data\n",
        "corpus = Corpus(data)\n"
      ],
      "metadata": {
        "id": "pzlWhsBYAYMl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data"
      ],
      "metadata": {
        "id": "6FvrRumyA1L7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_batch_size = 256\n",
        "train_data = batchify(corpus.train, batch_size) # size(total_len//bsz, bsz)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ],
      "metadata": {
        "id": "bGLbW8TzAbRw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNXQXMK9BA4B",
        "outputId": "c5a072d6-129f-4399-de1b-1600076cec1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byMAy3UfBCKQ",
        "outputId": "fc894476-d76b-4a5d-cfed-ca493ba4152a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  9,  ...,  3, 20,  0],\n",
              "        [ 1,  2, 10,  ...,  7,  7, 24],\n",
              "        [ 2,  3,  7,  ..., 17, 13,  0],\n",
              "        ...,\n",
              "        [ 8,  4,  7,  ..., 21, 28, 26],\n",
              "        [ 7, 10, 15,  ...,  3,  3, 10],\n",
              "        [ 4,  9,  2,  ..., 16, 12,  5]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data.to(device)\n",
        "test_data.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1NZXjd9BLM7",
        "outputId": "7f4a296a-0a65-4716-dbe6-6b41e93d14e2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5,  3,  3,  ...,  7,  3, 14],\n",
              "        [ 7, 29, 24,  ..., 15, 18,  3],\n",
              "        [ 3,  3,  0,  ...,  5,  2, 33],\n",
              "        ...,\n",
              "        [ 3,  3,  3,  ...,  7,  2, 17],\n",
              "        [ 7,  0,  8,  ..., 18, 21,  3],\n",
              "        [ 2, 16, 20,  ...,  1,  0, 13]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "interval = 50 # interval to report\n",
        "ntokens = len(corpus.dictionary) # 10000\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout)\n",
        "\n",
        "# Load checkpoint\n",
        "if checkpoint != '':\n",
        "    model = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "\n",
        "print(model)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-dm-QF-BQlt",
        "outputId": "30eae925-e0ed-484c-8518-3fc9e17156ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(50, 256)\n",
            "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
            "  (decoder): Linear(in_features=1000, out_features=50, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Sd1VoOBYb3",
        "outputId": "72beda04-ea32-420c-9b4e-25f3b0bfd020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (drop): Dropout(p=0.5, inplace=False)\n",
              "  (encoder): Embedding(50, 256)\n",
              "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
              "  (decoder): Linear(in_features=1000, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    # detach\n",
        "    return tuple(v.clone().detach() for v in h)\n"
      ],
      "metadata": {
        "id": "a4NH_KQaBbW-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_batch(source, i):\n",
        "    # source: size(total_len//bsz, bsz)\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    #data = torch.tensor(source[i:i+seq_len]) # size(bptt, bsz)\n",
        "    data = source[i:i+seq_len].clone().detach()\n",
        "    target = source[i+1:i+1+seq_len].clone().detach().view(-1)\n",
        "    #target = torch.tensor(source[i+1:i+1+seq_len].view(-1)) # size(bptt * bsz)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "YolhdEasBgCt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        ntokens = len(corpus.dictionary)\n",
        "        hidden = model.init_hidden(eval_batch_size) #hidden size(nlayers, bsz, hdsize)\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):# iterate over every timestep\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data.to(device), hidden)\n",
        "            # model input and output\n",
        "            # inputdata size(bptt, bsz), and size(bptt, bsz, embsize) after embedding\n",
        "            # output size(bptt*bsz, ntoken)\n",
        "            total_loss += len(data) * criterion(output.to(device), targets.to(device)).data\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        return total_loss / len(data_source)\n"
      ],
      "metadata": {
        "id": "RyWNpGhkBgyt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # choose a optimizer\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    # train_data size(batchcnt, bsz)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        # print(hidden.to(device))\n",
        "        output, hidden = model(data.to(device), hidden)\n",
        "        loss = criterion(output.to(device), targets.to(device))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_value_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                elapsed * 1000 / interval, cur_loss, math.exp(cur_loss), cur_loss / math.log(2)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n"
      ],
      "metadata": {
        "id": "37Y1ELGCBiaF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of tokens:\")\n",
        "print(\"Train: \", len(corpus.train))\n",
        "print(\"Valid: \", len(corpus.valid))\n",
        "print(\"Test:  \", len(corpus.test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIOn7vmfBtif",
        "outputId": "a549bd30-8a60-4e2c-bb64-9151c4d41b28"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens:\n",
            "Train:  5017483\n",
            "Valid:  393043\n",
            "Test:   442424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = lr\n",
        "best_val_loss = None\n",
        "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "opts = 'SGD'\n",
        "# if opt == 'Adam':\n",
        "#     opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
        "#     lr = 0.001\n",
        "# if args.opt == 'Momentum':\n",
        "#     opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.8)\n",
        "# if args.opt == 'RMSprop':\n",
        "#     opt = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
        "#     lr = 0.001\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | bpc {:8.3f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if opts == 'SGD' or opts == 'Momentum':\n",
        "                lr /= 4.0\n",
        "                for group in opt.param_groups:\n",
        "                    group['lr'] = lr\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCKEoCZjBk9q",
        "outputId": "07266aa7-1694-4a72-c758-1f9fbaebaa7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  559 batches | lr 0.0001 | ms/batch 179.50 | loss  3.99 | ppl    53.92 | bpc    5.753\n",
            "| epoch   1 |   100/  559 batches | lr 0.0001 | ms/batch 175.88 | loss  3.89 | ppl    49.09 | bpc    5.617\n",
            "| epoch   1 |   150/  559 batches | lr 0.0001 | ms/batch 175.70 | loss  3.87 | ppl    47.90 | bpc    5.582\n",
            "| epoch   1 |   200/  559 batches | lr 0.0001 | ms/batch 175.62 | loss  3.84 | ppl    46.49 | bpc    5.539\n",
            "| epoch   1 |   250/  559 batches | lr 0.0001 | ms/batch 175.46 | loss  3.81 | ppl    44.95 | bpc    5.490\n",
            "| epoch   1 |   300/  559 batches | lr 0.0001 | ms/batch 175.06 | loss  3.77 | ppl    43.41 | bpc    5.440\n",
            "| epoch   1 |   350/  559 batches | lr 0.0001 | ms/batch 174.96 | loss  3.73 | ppl    41.87 | bpc    5.388\n",
            "| epoch   1 |   400/  559 batches | lr 0.0001 | ms/batch 174.64 | loss  3.70 | ppl    40.36 | bpc    5.335\n",
            "| epoch   1 |   450/  559 batches | lr 0.0001 | ms/batch 174.95 | loss  3.66 | ppl    38.87 | bpc    5.281\n",
            "| epoch   1 |   500/  559 batches | lr 0.0001 | ms/batch 174.03 | loss  3.62 | ppl    37.40 | bpc    5.225\n",
            "| epoch   1 |   550/  559 batches | lr 0.0001 | ms/batch 174.58 | loss  3.58 | ppl    35.95 | bpc    5.168\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 100.52s | valid loss  3.55 | valid ppl    34.86 | bpc    5.123\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    50/  559 batches | lr 0.0001 | ms/batch 177.98 | loss  3.61 | ppl    36.79 | bpc    5.201\n",
            "| epoch   2 |   100/  559 batches | lr 0.0001 | ms/batch 174.51 | loss  3.49 | ppl    32.87 | bpc    5.039\n",
            "| epoch   2 |   150/  559 batches | lr 0.0001 | ms/batch 174.44 | loss  3.45 | ppl    31.54 | bpc    4.979\n",
            "| epoch   2 |   200/  559 batches | lr 0.0001 | ms/batch 174.92 | loss  3.41 | ppl    30.26 | bpc    4.919\n",
            "| epoch   2 |   250/  559 batches | lr 0.0001 | ms/batch 174.30 | loss  3.37 | ppl    28.96 | bpc    4.856\n",
            "| epoch   2 |   300/  559 batches | lr 0.0001 | ms/batch 174.86 | loss  3.33 | ppl    27.85 | bpc    4.799\n",
            "| epoch   2 |   350/  559 batches | lr 0.0001 | ms/batch 174.53 | loss  3.29 | ppl    26.85 | bpc    4.747\n",
            "| epoch   2 |   400/  559 batches | lr 0.0001 | ms/batch 174.82 | loss  3.26 | ppl    26.01 | bpc    4.701\n",
            "| epoch   2 |   450/  559 batches | lr 0.0001 | ms/batch 174.55 | loss  3.23 | ppl    25.27 | bpc    4.659\n",
            "| epoch   2 |   500/  559 batches | lr 0.0001 | ms/batch 174.85 | loss  3.20 | ppl    24.58 | bpc    4.619\n",
            "| epoch   2 |   550/  559 batches | lr 0.0001 | ms/batch 174.57 | loss  3.18 | ppl    23.93 | bpc    4.581\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 100.26s | valid loss  3.15 | valid ppl    23.45 | bpc    4.551\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  559 batches | lr 0.0001 | ms/batch 178.02 | loss  3.21 | ppl    24.85 | bpc    4.635\n",
            "| epoch   3 |   100/  559 batches | lr 0.0001 | ms/batch 174.62 | loss  3.13 | ppl    22.86 | bpc    4.515\n",
            "| epoch   3 |   150/  559 batches | lr 0.0001 | ms/batch 174.59 | loss  3.12 | ppl    22.56 | bpc    4.496\n",
            "| epoch   3 |   200/  559 batches | lr 0.0001 | ms/batch 174.49 | loss  3.11 | ppl    22.38 | bpc    4.484\n",
            "| epoch   3 |   250/  559 batches | lr 0.0001 | ms/batch 174.68 | loss  3.09 | ppl    22.03 | bpc    4.462\n",
            "| epoch   3 |   300/  559 batches | lr 0.0001 | ms/batch 174.65 | loss  3.09 | ppl    21.89 | bpc    4.452\n",
            "| epoch   3 |   350/  559 batches | lr 0.0001 | ms/batch 174.90 | loss  3.08 | ppl    21.71 | bpc    4.440\n",
            "| epoch   3 |   400/  559 batches | lr 0.0001 | ms/batch 174.55 | loss  3.07 | ppl    21.59 | bpc    4.432\n",
            "| epoch   3 |   450/  559 batches | lr 0.0001 | ms/batch 174.85 | loss  3.07 | ppl    21.44 | bpc    4.422\n",
            "| epoch   3 |   500/  559 batches | lr 0.0001 | ms/batch 174.34 | loss  3.06 | ppl    21.34 | bpc    4.415\n",
            "| epoch   3 |   550/  559 batches | lr 0.0001 | ms/batch 174.17 | loss  3.05 | ppl    21.21 | bpc    4.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 100.18s | valid loss  3.05 | valid ppl    21.08 | bpc    4.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  559 batches | lr 0.0001 | ms/batch 177.84 | loss  3.11 | ppl    22.52 | bpc    4.493\n",
            "| epoch   4 |   100/  559 batches | lr 0.0001 | ms/batch 174.76 | loss  3.05 | ppl    21.07 | bpc    4.397\n",
            "| epoch   4 |   150/  559 batches | lr 0.0001 | ms/batch 174.30 | loss  3.05 | ppl    21.03 | bpc    4.394\n",
            "| epoch   4 |   200/  559 batches | lr 0.0001 | ms/batch 174.12 | loss  3.05 | ppl    21.05 | bpc    4.396\n",
            "| epoch   4 |   250/  559 batches | lr 0.0001 | ms/batch 174.58 | loss  3.04 | ppl    20.90 | bpc    4.385\n",
            "| epoch   4 |   300/  559 batches | lr 0.0001 | ms/batch 173.77 | loss  3.04 | ppl    20.90 | bpc    4.385\n",
            "| epoch   4 |   350/  559 batches | lr 0.0001 | ms/batch 175.24 | loss  3.04 | ppl    20.84 | bpc    4.382\n",
            "| epoch   4 |   400/  559 batches | lr 0.0001 | ms/batch 174.72 | loss  3.04 | ppl    20.82 | bpc    4.380\n",
            "| epoch   4 |   450/  559 batches | lr 0.0001 | ms/batch 174.96 | loss  3.03 | ppl    20.76 | bpc    4.376\n",
            "| epoch   4 |   500/  559 batches | lr 0.0001 | ms/batch 174.81 | loss  3.03 | ppl    20.75 | bpc    4.375\n",
            "| epoch   4 |   550/  559 batches | lr 0.0001 | ms/batch 175.39 | loss  3.03 | ppl    20.67 | bpc    4.369\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 100.20s | valid loss  3.02 | valid ppl    20.54 | bpc    4.361\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  559 batches | lr 0.0001 | ms/batch 177.61 | loss  3.09 | ppl    22.02 | bpc    4.461\n",
            "| epoch   5 |   100/  559 batches | lr 0.0001 | ms/batch 174.36 | loss  3.03 | ppl    20.64 | bpc    4.367\n",
            "| epoch   5 |   150/  559 batches | lr 0.0001 | ms/batch 174.63 | loss  3.03 | ppl    20.64 | bpc    4.368\n",
            "| epoch   5 |   200/  559 batches | lr 0.0001 | ms/batch 174.04 | loss  3.03 | ppl    20.71 | bpc    4.372\n",
            "| epoch   5 |   250/  559 batches | lr 0.0001 | ms/batch 174.92 | loss  3.03 | ppl    20.60 | bpc    4.365\n",
            "| epoch   5 |   300/  559 batches | lr 0.0001 | ms/batch 174.93 | loss  3.03 | ppl    20.61 | bpc    4.365\n",
            "| epoch   5 |   350/  559 batches | lr 0.0001 | ms/batch 174.97 | loss  3.02 | ppl    20.59 | bpc    4.364\n",
            "| epoch   5 |   400/  559 batches | lr 0.0001 | ms/batch 173.88 | loss  3.02 | ppl    20.59 | bpc    4.364\n",
            "| epoch   5 |   450/  559 batches | lr 0.0001 | ms/batch 173.62 | loss  3.02 | ppl    20.56 | bpc    4.362\n",
            "| epoch   5 |   500/  559 batches | lr 0.0001 | ms/batch 173.70 | loss  3.02 | ppl    20.56 | bpc    4.362\n",
            "| epoch   5 |   550/  559 batches | lr 0.0001 | ms/batch 173.96 | loss  3.02 | ppl    20.51 | bpc    4.358\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 100.02s | valid loss  3.01 | valid ppl    20.36 | bpc    4.348\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  559 batches | lr 0.0001 | ms/batch 177.26 | loss  3.08 | ppl    21.85 | bpc    4.450\n",
            "| epoch   6 |   100/  559 batches | lr 0.0001 | ms/batch 173.92 | loss  3.02 | ppl    20.50 | bpc    4.358\n",
            "| epoch   6 |   150/  559 batches | lr 0.0001 | ms/batch 173.89 | loss  3.02 | ppl    20.51 | bpc    4.358\n",
            "| epoch   6 |   200/  559 batches | lr 0.0001 | ms/batch 173.80 | loss  3.02 | ppl    20.59 | bpc    4.364\n",
            "| epoch   6 |   250/  559 batches | lr 0.0001 | ms/batch 174.10 | loss  3.02 | ppl    20.49 | bpc    4.357\n",
            "| epoch   6 |   300/  559 batches | lr 0.0001 | ms/batch 173.86 | loss  3.02 | ppl    20.50 | bpc    4.358\n",
            "| epoch   6 |   350/  559 batches | lr 0.0001 | ms/batch 173.78 | loss  3.02 | ppl    20.49 | bpc    4.357\n",
            "| epoch   6 |   400/  559 batches | lr 0.0001 | ms/batch 174.44 | loss  3.02 | ppl    20.50 | bpc    4.357\n",
            "| epoch   6 |   450/  559 batches | lr 0.0001 | ms/batch 174.11 | loss  3.02 | ppl    20.48 | bpc    4.356\n",
            "| epoch   6 |   500/  559 batches | lr 0.0001 | ms/batch 174.20 | loss  3.02 | ppl    20.49 | bpc    4.357\n",
            "| epoch   6 |   550/  559 batches | lr 0.0001 | ms/batch 174.52 | loss  3.02 | ppl    20.44 | bpc    4.353\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 99.87s | valid loss  3.01 | valid ppl    20.28 | bpc    4.342\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  559 batches | lr 0.0001 | ms/batch 177.70 | loss  3.08 | ppl    21.78 | bpc    4.445\n",
            "| epoch   7 |   100/  559 batches | lr 0.0001 | ms/batch 174.71 | loss  3.02 | ppl    20.44 | bpc    4.353\n",
            "| epoch   7 |   150/  559 batches | lr 0.0001 | ms/batch 175.09 | loss  3.02 | ppl    20.45 | bpc    4.354\n",
            "| epoch   7 |   200/  559 batches | lr 0.0001 | ms/batch 174.21 | loss  3.02 | ppl    20.52 | bpc    4.359\n",
            "| epoch   7 |   250/  559 batches | lr 0.0001 | ms/batch 174.42 | loss  3.02 | ppl    20.44 | bpc    4.353\n",
            "| epoch   7 |   300/  559 batches | lr 0.0001 | ms/batch 174.47 | loss  3.02 | ppl    20.45 | bpc    4.354\n",
            "| epoch   7 |   350/  559 batches | lr 0.0001 | ms/batch 173.77 | loss  3.02 | ppl    20.44 | bpc    4.353\n",
            "| epoch   7 |   400/  559 batches | lr 0.0001 | ms/batch 174.20 | loss  3.02 | ppl    20.45 | bpc    4.354\n",
            "| epoch   7 |   450/  559 batches | lr 0.0001 | ms/batch 173.45 | loss  3.02 | ppl    20.43 | bpc    4.353\n",
            "| epoch   7 |   500/  559 batches | lr 0.0001 | ms/batch 175.15 | loss  3.02 | ppl    20.43 | bpc    4.353\n",
            "| epoch   7 |   550/  559 batches | lr 0.0001 | ms/batch 173.90 | loss  3.02 | ppl    20.39 | bpc    4.350\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 100.10s | valid loss  3.01 | valid ppl    20.24 | bpc    4.339\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  559 batches | lr 0.0001 | ms/batch 177.33 | loss  3.08 | ppl    21.72 | bpc    4.441\n",
            "| epoch   8 |   100/  559 batches | lr 0.0001 | ms/batch 174.10 | loss  3.02 | ppl    20.40 | bpc    4.350\n",
            "| epoch   8 |   150/  559 batches | lr 0.0001 | ms/batch 173.87 | loss  3.02 | ppl    20.42 | bpc    4.352\n",
            "| epoch   8 |   200/  559 batches | lr 0.0001 | ms/batch 173.89 | loss  3.02 | ppl    20.49 | bpc    4.357\n",
            "| epoch   8 |   250/  559 batches | lr 0.0001 | ms/batch 173.98 | loss  3.02 | ppl    20.39 | bpc    4.350\n",
            "| epoch   8 |   300/  559 batches | lr 0.0001 | ms/batch 174.46 | loss  3.02 | ppl    20.40 | bpc    4.351\n",
            "| epoch   8 |   350/  559 batches | lr 0.0001 | ms/batch 174.87 | loss  3.02 | ppl    20.41 | bpc    4.351\n",
            "| epoch   8 |   400/  559 batches | lr 0.0001 | ms/batch 175.24 | loss  3.02 | ppl    20.41 | bpc    4.351\n",
            "| epoch   8 |   450/  559 batches | lr 0.0001 | ms/batch 175.31 | loss  3.01 | ppl    20.39 | bpc    4.350\n",
            "| epoch   8 |   500/  559 batches | lr 0.0001 | ms/batch 174.52 | loss  3.02 | ppl    20.39 | bpc    4.350\n",
            "| epoch   8 |   550/  559 batches | lr 0.0001 | ms/batch 175.07 | loss  3.01 | ppl    20.35 | bpc    4.347\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 100.12s | valid loss  3.01 | valid ppl    20.21 | bpc    4.337\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  559 batches | lr 0.0001 | ms/batch 176.95 | loss  3.08 | ppl    21.69 | bpc    4.439\n",
            "| epoch   9 |   100/  559 batches | lr 0.0001 | ms/batch 174.44 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   150/  559 batches | lr 0.0001 | ms/batch 174.30 | loss  3.01 | ppl    20.38 | bpc    4.349\n",
            "| epoch   9 |   200/  559 batches | lr 0.0001 | ms/batch 173.97 | loss  3.02 | ppl    20.45 | bpc    4.354\n",
            "| epoch   9 |   250/  559 batches | lr 0.0001 | ms/batch 174.01 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   300/  559 batches | lr 0.0001 | ms/batch 173.85 | loss  3.01 | ppl    20.37 | bpc    4.348\n",
            "| epoch   9 |   350/  559 batches | lr 0.0001 | ms/batch 174.16 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   400/  559 batches | lr 0.0001 | ms/batch 174.45 | loss  3.01 | ppl    20.38 | bpc    4.349\n",
            "| epoch   9 |   450/  559 batches | lr 0.0001 | ms/batch 174.83 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   500/  559 batches | lr 0.0001 | ms/batch 174.67 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   550/  559 batches | lr 0.0001 | ms/batch 174.81 | loss  3.01 | ppl    20.32 | bpc    4.345\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 100.00s | valid loss  3.00 | valid ppl    20.18 | bpc    4.335\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  559 batches | lr 0.0001 | ms/batch 177.22 | loss  3.07 | ppl    21.65 | bpc    4.436\n",
            "| epoch  10 |   100/  559 batches | lr 0.0001 | ms/batch 173.61 | loss  3.01 | ppl    20.33 | bpc    4.345\n",
            "| epoch  10 |   150/  559 batches | lr 0.0001 | ms/batch 174.10 | loss  3.01 | ppl    20.34 | bpc    4.346\n",
            "| epoch  10 |   200/  559 batches | lr 0.0001 | ms/batch 174.18 | loss  3.02 | ppl    20.42 | bpc    4.352\n",
            "| epoch  10 |   250/  559 batches | lr 0.0001 | ms/batch 174.44 | loss  3.01 | ppl    20.33 | bpc    4.345\n",
            "| epoch  10 |   300/  559 batches | lr 0.0001 | ms/batch 174.68 | loss  3.01 | ppl    20.34 | bpc    4.346\n",
            "| epoch  10 |   350/  559 batches | lr 0.0001 | ms/batch 174.36 | loss  3.01 | ppl    20.35 | bpc    4.347\n",
            "| epoch  10 |   400/  559 batches | lr 0.0001 | ms/batch 173.96 | loss  3.01 | ppl    20.34 | bpc    4.347\n",
            "| epoch  10 |   450/  559 batches | lr 0.0001 | ms/batch 174.00 | loss  3.01 | ppl    20.33 | bpc    4.346\n",
            "| epoch  10 |   500/  559 batches | lr 0.0001 | ms/batch 174.38 | loss  3.01 | ppl    20.33 | bpc    4.345\n",
            "| epoch  10 |   550/  559 batches | lr 0.0001 | ms/batch 173.86 | loss  3.01 | ppl    20.30 | bpc    4.343\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 99.92s | valid loss  3.00 | valid ppl    20.16 | bpc    4.333\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  3.01 | test ppl    20.29 | bpc    4.343\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bptt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4NSlqBvSoVv",
        "outputId": "fd48f5a0-30fd-486b-c63d-67e24ecf3cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "interval = 50 # interval to report\n",
        "ntokens = len(corpus.dictionary) # 10000\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout)\n",
        "save = '/content/output/model_test_character_noise.pt'\n",
        "checkpoint = \"/content/output/model_test_character_none.pt\"\n",
        "\n",
        "# Load checkpoint\n",
        "if checkpoint != '':\n",
        "    model = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "\n",
        "print(model)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gPNHLTnVSLT",
        "outputId": "9897f2e7-ffd0-487e-e875-0a938a273e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(50, 256)\n",
            "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
            "  (decoder): Linear(in_features=1000, out_features=50, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.rnn.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nml5HMPKQ0G0",
        "outputId": "f053b3ff-e95e-45c0-f727-d545c317aa37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f5749274150>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import vector_to_parameters, parameters_to_vector\n",
        "lr = lr\n",
        "best_val_loss = None\n",
        "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "opts = 'SGD'\n",
        "epochs = 25\n",
        "# if opt == 'Adam':\n",
        "#     opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
        "#     lr = 0.001\n",
        "# if args.opt == 'Momentum':\n",
        "#     opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.8)\n",
        "# if args.opt == 'RMSprop':\n",
        "#     opt = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
        "#     lr = 0.001\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        model.to(device)\n",
        "\n",
        "        param_vector = parameters_to_vector(model.rnn.parameters())\n",
        "        param_vector.to(device)\n",
        "        n_params = len(param_vector)\n",
        "        noise = torch.distributions.Normal(loc=torch.tensor(0.), scale=torch.tensor(0.075)).sample_n(n_params)\n",
        "        param_vector.add_(noise.to(device))\n",
        "        \n",
        "        vector_to_parameters(param_vector, model.rnn.parameters())\n",
        "        model.to(device)\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | bpc {:8.3f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if opts == 'SGD' or opts == 'Momentum':\n",
        "                lr /= 4.0\n",
        "                for group in opt.param_groups:\n",
        "                    group['lr'] = lr\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy-oBaJnV_4E",
        "outputId": "574778e1-e6a0-4f93-fde8-a30d1df1ae40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:161: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n",
            "  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  559 batches | lr 0.0000 | ms/batch 180.71 | loss  3.15 | ppl    23.41 | bpc    4.549\n",
            "| epoch   1 |   100/  559 batches | lr 0.0000 | ms/batch 176.41 | loss  3.09 | ppl    22.00 | bpc    4.460\n",
            "| epoch   1 |   150/  559 batches | lr 0.0000 | ms/batch 175.86 | loss  3.09 | ppl    22.03 | bpc    4.462\n",
            "| epoch   1 |   200/  559 batches | lr 0.0000 | ms/batch 175.55 | loss  3.09 | ppl    22.07 | bpc    4.464\n",
            "| epoch   1 |   250/  559 batches | lr 0.0000 | ms/batch 175.15 | loss  3.09 | ppl    22.04 | bpc    4.462\n",
            "| epoch   1 |   300/  559 batches | lr 0.0000 | ms/batch 175.00 | loss  3.09 | ppl    22.01 | bpc    4.460\n",
            "| epoch   1 |   350/  559 batches | lr 0.0000 | ms/batch 174.57 | loss  3.09 | ppl    22.01 | bpc    4.460\n",
            "| epoch   1 |   400/  559 batches | lr 0.0000 | ms/batch 175.26 | loss  3.09 | ppl    22.03 | bpc    4.461\n",
            "| epoch   1 |   450/  559 batches | lr 0.0000 | ms/batch 174.58 | loss  3.09 | ppl    22.02 | bpc    4.460\n",
            "| epoch   1 |   500/  559 batches | lr 0.0000 | ms/batch 174.22 | loss  3.09 | ppl    22.03 | bpc    4.462\n",
            "| epoch   1 |   550/  559 batches | lr 0.0000 | ms/batch 174.38 | loss  3.09 | ppl    21.97 | bpc    4.458\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 100.62s | valid loss  3.07 | valid ppl    21.45 | bpc    4.423\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    50/  559 batches | lr 0.0000 | ms/batch 177.73 | loss  3.37 | ppl    28.93 | bpc    4.855\n",
            "| epoch   2 |   100/  559 batches | lr 0.0000 | ms/batch 174.63 | loss  3.30 | ppl    27.07 | bpc    4.759\n",
            "| epoch   2 |   150/  559 batches | lr 0.0000 | ms/batch 175.47 | loss  3.30 | ppl    27.13 | bpc    4.762\n",
            "| epoch   2 |   200/  559 batches | lr 0.0000 | ms/batch 174.84 | loss  3.30 | ppl    27.16 | bpc    4.763\n",
            "| epoch   2 |   250/  559 batches | lr 0.0000 | ms/batch 174.86 | loss  3.30 | ppl    27.05 | bpc    4.758\n",
            "| epoch   2 |   300/  559 batches | lr 0.0000 | ms/batch 174.56 | loss  3.30 | ppl    27.06 | bpc    4.758\n",
            "| epoch   2 |   350/  559 batches | lr 0.0000 | ms/batch 174.77 | loss  3.30 | ppl    27.07 | bpc    4.759\n",
            "| epoch   2 |   400/  559 batches | lr 0.0000 | ms/batch 174.93 | loss  3.30 | ppl    27.10 | bpc    4.760\n",
            "| epoch   2 |   450/  559 batches | lr 0.0000 | ms/batch 174.90 | loss  3.30 | ppl    27.12 | bpc    4.761\n",
            "| epoch   2 |   500/  559 batches | lr 0.0000 | ms/batch 174.22 | loss  3.30 | ppl    27.06 | bpc    4.758\n",
            "| epoch   2 |   550/  559 batches | lr 0.0000 | ms/batch 174.83 | loss  3.30 | ppl    27.06 | bpc    4.758\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 100.39s | valid loss  3.26 | valid ppl    26.03 | bpc    4.702\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  559 batches | lr 0.0000 | ms/batch 177.88 | loss  3.48 | ppl    32.34 | bpc    5.015\n",
            "| epoch   3 |   100/  559 batches | lr 0.0000 | ms/batch 174.43 | loss  3.41 | ppl    30.22 | bpc    4.917\n",
            "| epoch   3 |   150/  559 batches | lr 0.0000 | ms/batch 174.90 | loss  3.41 | ppl    30.24 | bpc    4.918\n",
            "| epoch   3 |   200/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.41 | ppl    30.30 | bpc    4.921\n",
            "| epoch   3 |   250/  559 batches | lr 0.0000 | ms/batch 175.21 | loss  3.41 | ppl    30.24 | bpc    4.918\n",
            "| epoch   3 |   300/  559 batches | lr 0.0000 | ms/batch 174.79 | loss  3.41 | ppl    30.25 | bpc    4.919\n",
            "| epoch   3 |   350/  559 batches | lr 0.0000 | ms/batch 174.86 | loss  3.41 | ppl    30.27 | bpc    4.920\n",
            "| epoch   3 |   400/  559 batches | lr 0.0000 | ms/batch 174.46 | loss  3.41 | ppl    30.29 | bpc    4.921\n",
            "| epoch   3 |   450/  559 batches | lr 0.0000 | ms/batch 174.51 | loss  3.41 | ppl    30.32 | bpc    4.922\n",
            "| epoch   3 |   500/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.41 | ppl    30.28 | bpc    4.920\n",
            "| epoch   3 |   550/  559 batches | lr 0.0000 | ms/batch 174.67 | loss  3.41 | ppl    30.27 | bpc    4.920\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 100.34s | valid loss  3.36 | valid ppl    28.73 | bpc    4.845\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  559 batches | lr 0.0000 | ms/batch 178.12 | loss  3.56 | ppl    35.06 | bpc    5.132\n",
            "| epoch   4 |   100/  559 batches | lr 0.0000 | ms/batch 174.98 | loss  3.49 | ppl    32.66 | bpc    5.029\n",
            "| epoch   4 |   150/  559 batches | lr 0.0000 | ms/batch 174.92 | loss  3.49 | ppl    32.66 | bpc    5.030\n",
            "| epoch   4 |   200/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.49 | ppl    32.70 | bpc    5.031\n",
            "| epoch   4 |   250/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.49 | ppl    32.75 | bpc    5.033\n",
            "| epoch   4 |   300/  559 batches | lr 0.0000 | ms/batch 174.88 | loss  3.49 | ppl    32.76 | bpc    5.034\n",
            "| epoch   4 |   350/  559 batches | lr 0.0000 | ms/batch 174.70 | loss  3.49 | ppl    32.72 | bpc    5.032\n",
            "| epoch   4 |   400/  559 batches | lr 0.0000 | ms/batch 175.17 | loss  3.49 | ppl    32.75 | bpc    5.033\n",
            "| epoch   4 |   450/  559 batches | lr 0.0000 | ms/batch 174.61 | loss  3.49 | ppl    32.79 | bpc    5.035\n",
            "| epoch   4 |   500/  559 batches | lr 0.0000 | ms/batch 174.44 | loss  3.49 | ppl    32.72 | bpc    5.032\n",
            "| epoch   4 |   550/  559 batches | lr 0.0000 | ms/batch 174.56 | loss  3.49 | ppl    32.69 | bpc    5.031\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 100.38s | valid loss  3.43 | valid ppl    30.97 | bpc    4.953\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  559 batches | lr 0.0000 | ms/batch 177.92 | loss  3.57 | ppl    35.64 | bpc    5.155\n",
            "| epoch   5 |   100/  559 batches | lr 0.0000 | ms/batch 175.24 | loss  3.51 | ppl    33.31 | bpc    5.058\n",
            "| epoch   5 |   150/  559 batches | lr 0.0000 | ms/batch 174.42 | loss  3.51 | ppl    33.31 | bpc    5.058\n",
            "| epoch   5 |   200/  559 batches | lr 0.0000 | ms/batch 174.92 | loss  3.51 | ppl    33.32 | bpc    5.059\n",
            "| epoch   5 |   250/  559 batches | lr 0.0000 | ms/batch 174.85 | loss  3.51 | ppl    33.31 | bpc    5.058\n",
            "| epoch   5 |   300/  559 batches | lr 0.0000 | ms/batch 174.67 | loss  3.50 | ppl    33.18 | bpc    5.052\n",
            "| epoch   5 |   350/  559 batches | lr 0.0000 | ms/batch 174.47 | loss  3.50 | ppl    33.25 | bpc    5.055\n",
            "| epoch   5 |   400/  559 batches | lr 0.0000 | ms/batch 174.52 | loss  3.51 | ppl    33.32 | bpc    5.058\n",
            "| epoch   5 |   450/  559 batches | lr 0.0000 | ms/batch 174.36 | loss  3.50 | ppl    33.27 | bpc    5.056\n",
            "| epoch   5 |   500/  559 batches | lr 0.0000 | ms/batch 174.06 | loss  3.50 | ppl    33.23 | bpc    5.055\n",
            "| epoch   5 |   550/  559 batches | lr 0.0000 | ms/batch 174.41 | loss  3.50 | ppl    33.27 | bpc    5.056\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 100.23s | valid loss  3.45 | valid ppl    31.36 | bpc    4.971\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  559 batches | lr 0.0000 | ms/batch 177.64 | loss  3.64 | ppl    38.17 | bpc    5.254\n",
            "| epoch   6 |   100/  559 batches | lr 0.0000 | ms/batch 174.81 | loss  3.57 | ppl    35.51 | bpc    5.150\n",
            "| epoch   6 |   150/  559 batches | lr 0.0000 | ms/batch 174.54 | loss  3.57 | ppl    35.62 | bpc    5.155\n",
            "| epoch   6 |   200/  559 batches | lr 0.0000 | ms/batch 173.95 | loss  3.57 | ppl    35.57 | bpc    5.152\n",
            "| epoch   6 |   250/  559 batches | lr 0.0000 | ms/batch 174.26 | loss  3.57 | ppl    35.53 | bpc    5.151\n",
            "| epoch   6 |   300/  559 batches | lr 0.0000 | ms/batch 174.61 | loss  3.57 | ppl    35.48 | bpc    5.149\n",
            "| epoch   6 |   350/  559 batches | lr 0.0000 | ms/batch 174.56 | loss  3.57 | ppl    35.57 | bpc    5.153\n",
            "| epoch   6 |   400/  559 batches | lr 0.0000 | ms/batch 174.36 | loss  3.57 | ppl    35.48 | bpc    5.149\n",
            "| epoch   6 |   450/  559 batches | lr 0.0000 | ms/batch 174.34 | loss  3.57 | ppl    35.51 | bpc    5.150\n",
            "| epoch   6 |   500/  559 batches | lr 0.0000 | ms/batch 174.83 | loss  3.57 | ppl    35.55 | bpc    5.152\n",
            "| epoch   6 |   550/  559 batches | lr 0.0000 | ms/batch 174.97 | loss  3.57 | ppl    35.56 | bpc    5.152\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 100.25s | valid loss  3.51 | valid ppl    33.35 | bpc    5.060\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  559 batches | lr 0.0000 | ms/batch 178.48 | loss  3.65 | ppl    38.31 | bpc    5.260\n",
            "| epoch   7 |   100/  559 batches | lr 0.0000 | ms/batch 175.28 | loss  3.57 | ppl    35.68 | bpc    5.157\n",
            "| epoch   7 |   150/  559 batches | lr 0.0000 | ms/batch 175.13 | loss  3.58 | ppl    35.73 | bpc    5.159\n",
            "| epoch   7 |   200/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.58 | ppl    35.76 | bpc    5.160\n",
            "| epoch   7 |   250/  559 batches | lr 0.0000 | ms/batch 175.01 | loss  3.58 | ppl    35.77 | bpc    5.161\n",
            "| epoch   7 |   300/  559 batches | lr 0.0000 | ms/batch 175.28 | loss  3.58 | ppl    35.72 | bpc    5.159\n",
            "| epoch   7 |   350/  559 batches | lr 0.0000 | ms/batch 175.40 | loss  3.58 | ppl    35.74 | bpc    5.159\n",
            "| epoch   7 |   400/  559 batches | lr 0.0000 | ms/batch 174.66 | loss  3.58 | ppl    35.74 | bpc    5.159\n",
            "| epoch   7 |   450/  559 batches | lr 0.0000 | ms/batch 175.34 | loss  3.58 | ppl    35.72 | bpc    5.159\n",
            "| epoch   7 |   500/  559 batches | lr 0.0000 | ms/batch 175.25 | loss  3.58 | ppl    35.72 | bpc    5.159\n",
            "| epoch   7 |   550/  559 batches | lr 0.0000 | ms/batch 175.03 | loss  3.58 | ppl    35.74 | bpc    5.160\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 100.59s | valid loss  3.51 | valid ppl    33.48 | bpc    5.065\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  559 batches | lr 0.0000 | ms/batch 177.65 | loss  3.66 | ppl    38.77 | bpc    5.277\n",
            "| epoch   8 |   100/  559 batches | lr 0.0000 | ms/batch 174.62 | loss  3.59 | ppl    36.19 | bpc    5.177\n",
            "| epoch   8 |   150/  559 batches | lr 0.0000 | ms/batch 174.34 | loss  3.59 | ppl    36.24 | bpc    5.180\n",
            "| epoch   8 |   200/  559 batches | lr 0.0000 | ms/batch 174.20 | loss  3.59 | ppl    36.26 | bpc    5.181\n",
            "| epoch   8 |   250/  559 batches | lr 0.0000 | ms/batch 175.10 | loss  3.59 | ppl    36.16 | bpc    5.176\n",
            "| epoch   8 |   300/  559 batches | lr 0.0000 | ms/batch 174.68 | loss  3.59 | ppl    36.23 | bpc    5.179\n",
            "| epoch   8 |   350/  559 batches | lr 0.0000 | ms/batch 174.26 | loss  3.59 | ppl    36.18 | bpc    5.177\n",
            "| epoch   8 |   400/  559 batches | lr 0.0000 | ms/batch 174.30 | loss  3.59 | ppl    36.19 | bpc    5.178\n",
            "| epoch   8 |   450/  559 batches | lr 0.0000 | ms/batch 174.75 | loss  3.59 | ppl    36.19 | bpc    5.177\n",
            "| epoch   8 |   500/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.59 | ppl    36.25 | bpc    5.180\n",
            "| epoch   8 |   550/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.59 | ppl    36.16 | bpc    5.176\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 100.23s | valid loss  3.52 | valid ppl    33.88 | bpc    5.082\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  559 batches | lr 0.0000 | ms/batch 177.82 | loss  3.67 | ppl    39.44 | bpc    5.301\n",
            "| epoch   9 |   100/  559 batches | lr 0.0000 | ms/batch 174.57 | loss  3.60 | ppl    36.63 | bpc    5.195\n",
            "| epoch   9 |   150/  559 batches | lr 0.0000 | ms/batch 174.91 | loss  3.60 | ppl    36.72 | bpc    5.199\n",
            "| epoch   9 |   200/  559 batches | lr 0.0000 | ms/batch 174.89 | loss  3.60 | ppl    36.76 | bpc    5.200\n",
            "| epoch   9 |   250/  559 batches | lr 0.0000 | ms/batch 175.02 | loss  3.60 | ppl    36.74 | bpc    5.199\n",
            "| epoch   9 |   300/  559 batches | lr 0.0000 | ms/batch 175.06 | loss  3.60 | ppl    36.72 | bpc    5.198\n",
            "| epoch   9 |   350/  559 batches | lr 0.0000 | ms/batch 174.39 | loss  3.61 | ppl    36.78 | bpc    5.201\n",
            "| epoch   9 |   400/  559 batches | lr 0.0000 | ms/batch 174.92 | loss  3.60 | ppl    36.72 | bpc    5.199\n",
            "| epoch   9 |   450/  559 batches | lr 0.0000 | ms/batch 174.79 | loss  3.60 | ppl    36.71 | bpc    5.198\n",
            "| epoch   9 |   500/  559 batches | lr 0.0000 | ms/batch 175.78 | loss  3.60 | ppl    36.72 | bpc    5.198\n",
            "| epoch   9 |   550/  559 batches | lr 0.0000 | ms/batch 175.25 | loss  3.60 | ppl    36.68 | bpc    5.197\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 100.47s | valid loss  3.54 | valid ppl    34.38 | bpc    5.103\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  559 batches | lr 0.0000 | ms/batch 177.62 | loss  3.69 | ppl    40.12 | bpc    5.326\n",
            "| epoch  10 |   100/  559 batches | lr 0.0000 | ms/batch 174.51 | loss  3.62 | ppl    37.28 | bpc    5.220\n",
            "| epoch  10 |   150/  559 batches | lr 0.0000 | ms/batch 174.64 | loss  3.62 | ppl    37.40 | bpc    5.225\n",
            "| epoch  10 |   200/  559 batches | lr 0.0000 | ms/batch 174.85 | loss  3.62 | ppl    37.39 | bpc    5.225\n",
            "| epoch  10 |   250/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.62 | ppl    37.31 | bpc    5.221\n",
            "| epoch  10 |   300/  559 batches | lr 0.0000 | ms/batch 174.71 | loss  3.62 | ppl    37.33 | bpc    5.222\n",
            "| epoch  10 |   350/  559 batches | lr 0.0000 | ms/batch 174.44 | loss  3.62 | ppl    37.33 | bpc    5.222\n",
            "| epoch  10 |   400/  559 batches | lr 0.0000 | ms/batch 174.80 | loss  3.62 | ppl    37.29 | bpc    5.221\n",
            "| epoch  10 |   450/  559 batches | lr 0.0000 | ms/batch 174.42 | loss  3.62 | ppl    37.38 | bpc    5.224\n",
            "| epoch  10 |   500/  559 batches | lr 0.0000 | ms/batch 174.38 | loss  3.62 | ppl    37.35 | bpc    5.223\n",
            "| epoch  10 |   550/  559 batches | lr 0.0000 | ms/batch 174.44 | loss  3.62 | ppl    37.37 | bpc    5.224\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 100.21s | valid loss  3.55 | valid ppl    34.93 | bpc    5.127\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |    50/  559 batches | lr 0.0000 | ms/batch 177.88 | loss  3.69 | ppl    40.16 | bpc    5.328\n",
            "| epoch  11 |   100/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.62 | ppl    37.28 | bpc    5.220\n",
            "| epoch  11 |   150/  559 batches | lr 0.0000 | ms/batch 174.40 | loss  3.62 | ppl    37.32 | bpc    5.222\n",
            "| epoch  11 |   200/  559 batches | lr 0.0000 | ms/batch 174.39 | loss  3.62 | ppl    37.39 | bpc    5.225\n",
            "| epoch  11 |   250/  559 batches | lr 0.0000 | ms/batch 174.47 | loss  3.62 | ppl    37.37 | bpc    5.224\n",
            "| epoch  11 |   300/  559 batches | lr 0.0000 | ms/batch 174.13 | loss  3.62 | ppl    37.34 | bpc    5.222\n",
            "| epoch  11 |   350/  559 batches | lr 0.0000 | ms/batch 173.95 | loss  3.62 | ppl    37.35 | bpc    5.223\n",
            "| epoch  11 |   400/  559 batches | lr 0.0000 | ms/batch 174.37 | loss  3.62 | ppl    37.37 | bpc    5.224\n",
            "| epoch  11 |   450/  559 batches | lr 0.0000 | ms/batch 174.26 | loss  3.62 | ppl    37.35 | bpc    5.223\n",
            "| epoch  11 |   500/  559 batches | lr 0.0000 | ms/batch 175.08 | loss  3.62 | ppl    37.39 | bpc    5.225\n",
            "| epoch  11 |   550/  559 batches | lr 0.0000 | ms/batch 175.17 | loss  3.62 | ppl    37.28 | bpc    5.220\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 100.20s | valid loss  3.55 | valid ppl    34.88 | bpc    5.124\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    50/  559 batches | lr 0.0000 | ms/batch 178.37 | loss  3.69 | ppl    39.87 | bpc    5.317\n",
            "| epoch  12 |   100/  559 batches | lr 0.0000 | ms/batch 174.66 | loss  3.61 | ppl    37.08 | bpc    5.213\n",
            "| epoch  12 |   150/  559 batches | lr 0.0000 | ms/batch 175.25 | loss  3.61 | ppl    37.11 | bpc    5.214\n",
            "| epoch  12 |   200/  559 batches | lr 0.0000 | ms/batch 175.11 | loss  3.61 | ppl    37.14 | bpc    5.215\n",
            "| epoch  12 |   250/  559 batches | lr 0.0000 | ms/batch 175.44 | loss  3.61 | ppl    37.06 | bpc    5.212\n",
            "| epoch  12 |   300/  559 batches | lr 0.0000 | ms/batch 175.25 | loss  3.61 | ppl    37.03 | bpc    5.211\n",
            "| epoch  12 |   350/  559 batches | lr 0.0000 | ms/batch 175.32 | loss  3.61 | ppl    37.15 | bpc    5.215\n",
            "| epoch  12 |   400/  559 batches | lr 0.0000 | ms/batch 175.80 | loss  3.62 | ppl    37.17 | bpc    5.216\n",
            "| epoch  12 |   450/  559 batches | lr 0.0000 | ms/batch 174.70 | loss  3.61 | ppl    37.11 | bpc    5.214\n",
            "| epoch  12 |   500/  559 batches | lr 0.0000 | ms/batch 174.55 | loss  3.61 | ppl    37.11 | bpc    5.214\n",
            "| epoch  12 |   550/  559 batches | lr 0.0000 | ms/batch 174.62 | loss  3.61 | ppl    37.06 | bpc    5.212\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 100.50s | valid loss  3.55 | valid ppl    34.65 | bpc    5.115\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    50/  559 batches | lr 0.0000 | ms/batch 177.94 | loss  3.69 | ppl    40.08 | bpc    5.325\n",
            "| epoch  13 |   100/  559 batches | lr 0.0000 | ms/batch 174.33 | loss  3.62 | ppl    37.29 | bpc    5.221\n",
            "| epoch  13 |   150/  559 batches | lr 0.0000 | ms/batch 174.70 | loss  3.62 | ppl    37.37 | bpc    5.224\n",
            "| epoch  13 |   200/  559 batches | lr 0.0000 | ms/batch 174.81 | loss  3.62 | ppl    37.40 | bpc    5.225\n",
            "| epoch  13 |   250/  559 batches | lr 0.0000 | ms/batch 174.67 | loss  3.62 | ppl    37.34 | bpc    5.223\n",
            "| epoch  13 |   300/  559 batches | lr 0.0000 | ms/batch 174.38 | loss  3.62 | ppl    37.31 | bpc    5.221\n",
            "| epoch  13 |   350/  559 batches | lr 0.0000 | ms/batch 174.90 | loss  3.62 | ppl    37.34 | bpc    5.222\n",
            "| epoch  13 |   400/  559 batches | lr 0.0000 | ms/batch 174.87 | loss  3.62 | ppl    37.37 | bpc    5.224\n",
            "| epoch  13 |   450/  559 batches | lr 0.0000 | ms/batch 174.64 | loss  3.62 | ppl    37.29 | bpc    5.221\n",
            "| epoch  13 |   500/  559 batches | lr 0.0000 | ms/batch 174.43 | loss  3.62 | ppl    37.25 | bpc    5.219\n",
            "| epoch  13 |   550/  559 batches | lr 0.0000 | ms/batch 174.86 | loss  3.62 | ppl    37.32 | bpc    5.222\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 100.31s | valid loss  3.55 | valid ppl    34.89 | bpc    5.125\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |    50/  559 batches | lr 0.0000 | ms/batch 178.39 | loss  3.70 | ppl    40.52 | bpc    5.340\n",
            "| epoch  14 |   100/  559 batches | lr 0.0000 | ms/batch 174.96 | loss  3.63 | ppl    37.81 | bpc    5.241\n",
            "| epoch  14 |   150/  559 batches | lr 0.0000 | ms/batch 175.23 | loss  3.63 | ppl    37.75 | bpc    5.238\n",
            "| epoch  14 |   200/  559 batches | lr 0.0000 | ms/batch 174.77 | loss  3.63 | ppl    37.76 | bpc    5.239\n",
            "| epoch  14 |   250/  559 batches | lr 0.0000 | ms/batch 175.07 | loss  3.63 | ppl    37.69 | bpc    5.236\n",
            "| epoch  14 |   300/  559 batches | lr 0.0000 | ms/batch 175.23 | loss  3.63 | ppl    37.75 | bpc    5.238\n",
            "| epoch  14 |   350/  559 batches | lr 0.0000 | ms/batch 174.75 | loss  3.63 | ppl    37.73 | bpc    5.238\n",
            "| epoch  14 |   400/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.63 | ppl    37.68 | bpc    5.236\n",
            "| epoch  14 |   450/  559 batches | lr 0.0000 | ms/batch 174.35 | loss  3.63 | ppl    37.78 | bpc    5.240\n",
            "| epoch  14 |   500/  559 batches | lr 0.0000 | ms/batch 174.73 | loss  3.63 | ppl    37.69 | bpc    5.236\n",
            "| epoch  14 |   550/  559 batches | lr 0.0000 | ms/batch 174.14 | loss  3.63 | ppl    37.78 | bpc    5.240\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 100.34s | valid loss  3.56 | valid ppl    35.25 | bpc    5.140\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |    50/  559 batches | lr 0.0000 | ms/batch 177.98 | loss  3.71 | ppl    41.05 | bpc    5.359\n",
            "| epoch  15 |   100/  559 batches | lr 0.0000 | ms/batch 174.34 | loss  3.64 | ppl    38.20 | bpc    5.255\n",
            "| epoch  15 |   150/  559 batches | lr 0.0000 | ms/batch 174.01 | loss  3.64 | ppl    38.19 | bpc    5.255\n",
            "| epoch  15 |   200/  559 batches | lr 0.0000 | ms/batch 173.91 | loss  3.64 | ppl    38.27 | bpc    5.258\n",
            "| epoch  15 |   250/  559 batches | lr 0.0000 | ms/batch 174.41 | loss  3.64 | ppl    38.24 | bpc    5.257\n",
            "| epoch  15 |   300/  559 batches | lr 0.0000 | ms/batch 174.30 | loss  3.64 | ppl    38.25 | bpc    5.257\n",
            "| epoch  15 |   350/  559 batches | lr 0.0000 | ms/batch 174.38 | loss  3.64 | ppl    38.25 | bpc    5.257\n",
            "| epoch  15 |   400/  559 batches | lr 0.0000 | ms/batch 174.16 | loss  3.64 | ppl    38.22 | bpc    5.256\n",
            "| epoch  15 |   450/  559 batches | lr 0.0000 | ms/batch 173.99 | loss  3.64 | ppl    38.24 | bpc    5.257\n",
            "| epoch  15 |   500/  559 batches | lr 0.0000 | ms/batch 174.14 | loss  3.64 | ppl    38.19 | bpc    5.255\n",
            "| epoch  15 |   550/  559 batches | lr 0.0000 | ms/batch 175.34 | loss  3.64 | ppl    38.20 | bpc    5.255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 100.19s | valid loss  3.57 | valid ppl    35.62 | bpc    5.155\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |    50/  559 batches | lr 0.0000 | ms/batch 178.55 | loss  3.73 | ppl    41.62 | bpc    5.379\n",
            "| epoch  16 |   100/  559 batches | lr 0.0000 | ms/batch 175.53 | loss  3.66 | ppl    38.72 | bpc    5.275\n",
            "| epoch  16 |   150/  559 batches | lr 0.0000 | ms/batch 175.53 | loss  3.66 | ppl    38.77 | bpc    5.277\n",
            "| epoch  16 |   200/  559 batches | lr 0.0000 | ms/batch 175.29 | loss  3.66 | ppl    38.83 | bpc    5.279\n",
            "| epoch  16 |   250/  559 batches | lr 0.0000 | ms/batch 175.47 | loss  3.66 | ppl    38.76 | bpc    5.276\n",
            "| epoch  16 |   300/  559 batches | lr 0.0000 | ms/batch 175.61 | loss  3.66 | ppl    38.79 | bpc    5.277\n",
            "| epoch  16 |   350/  559 batches | lr 0.0000 | ms/batch 175.45 | loss  3.66 | ppl    38.76 | bpc    5.277\n",
            "| epoch  16 |   400/  559 batches | lr 0.0000 | ms/batch 174.58 | loss  3.66 | ppl    38.78 | bpc    5.277\n",
            "| epoch  16 |   450/  559 batches | lr 0.0000 | ms/batch 175.25 | loss  3.65 | ppl    38.67 | bpc    5.273\n",
            "| epoch  16 |   500/  559 batches | lr 0.0000 | ms/batch 174.98 | loss  3.66 | ppl    38.71 | bpc    5.275\n",
            "| epoch  16 |   550/  559 batches | lr 0.0000 | ms/batch 175.29 | loss  3.65 | ppl    38.66 | bpc    5.273\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 100.63s | valid loss  3.59 | valid ppl    36.14 | bpc    5.175\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |    50/  559 batches | lr 0.0000 | ms/batch 178.45 | loss  3.73 | ppl    41.70 | bpc    5.382\n",
            "| epoch  17 |   100/  559 batches | lr 0.0000 | ms/batch 175.37 | loss  3.66 | ppl    38.78 | bpc    5.277\n",
            "| epoch  17 |   150/  559 batches | lr 0.0000 | ms/batch 175.69 | loss  3.66 | ppl    38.73 | bpc    5.275\n",
            "| epoch  17 |   200/  559 batches | lr 0.0000 | ms/batch 175.19 | loss  3.66 | ppl    38.80 | bpc    5.278\n",
            "| epoch  17 |   250/  559 batches | lr 0.0000 | ms/batch 175.08 | loss  3.66 | ppl    38.74 | bpc    5.276\n",
            "| epoch  17 |   300/  559 batches | lr 0.0000 | ms/batch 175.20 | loss  3.66 | ppl    38.74 | bpc    5.276\n",
            "| epoch  17 |   350/  559 batches | lr 0.0000 | ms/batch 175.26 | loss  3.66 | ppl    38.79 | bpc    5.278\n",
            "| epoch  17 |   400/  559 batches | lr 0.0000 | ms/batch 175.65 | loss  3.66 | ppl    38.74 | bpc    5.276\n",
            "| epoch  17 |   450/  559 batches | lr 0.0000 | ms/batch 175.14 | loss  3.66 | ppl    38.80 | bpc    5.278\n",
            "| epoch  17 |   500/  559 batches | lr 0.0000 | ms/batch 175.40 | loss  3.66 | ppl    38.82 | bpc    5.279\n",
            "| epoch  17 |   550/  559 batches | lr 0.0000 | ms/batch 175.77 | loss  3.66 | ppl    38.80 | bpc    5.278\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 100.76s | valid loss  3.59 | valid ppl    36.12 | bpc    5.175\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |    50/  559 batches | lr 0.0000 | ms/batch 178.41 | loss  3.74 | ppl    42.05 | bpc    5.394\n",
            "| epoch  18 |   100/  559 batches | lr 0.0000 | ms/batch 174.65 | loss  3.67 | ppl    39.08 | bpc    5.289\n",
            "| epoch  18 |   150/  559 batches | lr 0.0000 | ms/batch 174.77 | loss  3.67 | ppl    39.06 | bpc    5.288\n",
            "| epoch  18 |   200/  559 batches | lr 0.0000 | ms/batch 175.53 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "| epoch  18 |   250/  559 batches | lr 0.0000 | ms/batch 175.43 | loss  3.66 | ppl    39.03 | bpc    5.287\n",
            "| epoch  18 |   300/  559 batches | lr 0.0000 | ms/batch 175.23 | loss  3.67 | ppl    39.14 | bpc    5.290\n",
            "| epoch  18 |   350/  559 batches | lr 0.0000 | ms/batch 175.38 | loss  3.67 | ppl    39.06 | bpc    5.288\n",
            "| epoch  18 |   400/  559 batches | lr 0.0000 | ms/batch 175.12 | loss  3.66 | ppl    39.03 | bpc    5.286\n",
            "| epoch  18 |   450/  559 batches | lr 0.0000 | ms/batch 174.91 | loss  3.66 | ppl    39.00 | bpc    5.285\n",
            "| epoch  18 |   500/  559 batches | lr 0.0000 | ms/batch 175.08 | loss  3.67 | ppl    39.11 | bpc    5.289\n",
            "| epoch  18 |   550/  559 batches | lr 0.0000 | ms/batch 175.56 | loss  3.66 | ppl    39.05 | bpc    5.287\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 100.50s | valid loss  3.60 | valid ppl    36.43 | bpc    5.187\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |    50/  559 batches | lr 0.0000 | ms/batch 177.58 | loss  3.73 | ppl    41.81 | bpc    5.386\n",
            "| epoch  19 |   100/  559 batches | lr 0.0000 | ms/batch 174.97 | loss  3.66 | ppl    38.93 | bpc    5.283\n",
            "| epoch  19 |   150/  559 batches | lr 0.0000 | ms/batch 175.30 | loss  3.66 | ppl    38.88 | bpc    5.281\n",
            "| epoch  19 |   200/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.66 | ppl    38.83 | bpc    5.279\n",
            "| epoch  19 |   250/  559 batches | lr 0.0000 | ms/batch 174.06 | loss  3.66 | ppl    38.83 | bpc    5.279\n",
            "| epoch  19 |   300/  559 batches | lr 0.0000 | ms/batch 175.20 | loss  3.66 | ppl    38.89 | bpc    5.281\n",
            "| epoch  19 |   350/  559 batches | lr 0.0000 | ms/batch 174.54 | loss  3.66 | ppl    38.89 | bpc    5.281\n",
            "| epoch  19 |   400/  559 batches | lr 0.0000 | ms/batch 174.73 | loss  3.66 | ppl    38.88 | bpc    5.281\n",
            "| epoch  19 |   450/  559 batches | lr 0.0000 | ms/batch 174.57 | loss  3.66 | ppl    38.89 | bpc    5.281\n",
            "| epoch  19 |   500/  559 batches | lr 0.0000 | ms/batch 174.18 | loss  3.66 | ppl    38.88 | bpc    5.281\n",
            "| epoch  19 |   550/  559 batches | lr 0.0000 | ms/batch 174.58 | loss  3.66 | ppl    38.87 | bpc    5.281\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 100.31s | valid loss  3.59 | valid ppl    36.20 | bpc    5.178\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |    50/  559 batches | lr 0.0000 | ms/batch 178.39 | loss  3.73 | ppl    41.78 | bpc    5.385\n",
            "| epoch  20 |   100/  559 batches | lr 0.0000 | ms/batch 175.04 | loss  3.66 | ppl    38.82 | bpc    5.279\n",
            "| epoch  20 |   150/  559 batches | lr 0.0000 | ms/batch 174.71 | loss  3.66 | ppl    38.83 | bpc    5.279\n",
            "| epoch  20 |   200/  559 batches | lr 0.0000 | ms/batch 174.04 | loss  3.66 | ppl    38.84 | bpc    5.279\n",
            "| epoch  20 |   250/  559 batches | lr 0.0000 | ms/batch 174.46 | loss  3.66 | ppl    38.80 | bpc    5.278\n",
            "| epoch  20 |   300/  559 batches | lr 0.0000 | ms/batch 174.25 | loss  3.66 | ppl    38.91 | bpc    5.282\n",
            "| epoch  20 |   350/  559 batches | lr 0.0000 | ms/batch 174.21 | loss  3.66 | ppl    38.81 | bpc    5.278\n",
            "| epoch  20 |   400/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.66 | ppl    38.79 | bpc    5.277\n",
            "| epoch  20 |   450/  559 batches | lr 0.0000 | ms/batch 174.66 | loss  3.66 | ppl    38.83 | bpc    5.279\n",
            "| epoch  20 |   500/  559 batches | lr 0.0000 | ms/batch 173.95 | loss  3.66 | ppl    38.85 | bpc    5.280\n",
            "| epoch  20 |   550/  559 batches | lr 0.0000 | ms/batch 174.43 | loss  3.66 | ppl    38.79 | bpc    5.278\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 100.20s | valid loss  3.59 | valid ppl    36.14 | bpc    5.175\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |    50/  559 batches | lr 0.0000 | ms/batch 177.71 | loss  3.74 | ppl    42.13 | bpc    5.397\n",
            "| epoch  21 |   100/  559 batches | lr 0.0000 | ms/batch 174.12 | loss  3.67 | ppl    39.10 | bpc    5.289\n",
            "| epoch  21 |   150/  559 batches | lr 0.0000 | ms/batch 174.27 | loss  3.67 | ppl    39.13 | bpc    5.290\n",
            "| epoch  21 |   200/  559 batches | lr 0.0000 | ms/batch 174.68 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "| epoch  21 |   250/  559 batches | lr 0.0000 | ms/batch 175.13 | loss  3.67 | ppl    39.14 | bpc    5.290\n",
            "| epoch  21 |   300/  559 batches | lr 0.0000 | ms/batch 173.82 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "| epoch  21 |   350/  559 batches | lr 0.0000 | ms/batch 174.14 | loss  3.67 | ppl    39.15 | bpc    5.291\n",
            "| epoch  21 |   400/  559 batches | lr 0.0000 | ms/batch 173.33 | loss  3.66 | ppl    39.02 | bpc    5.286\n",
            "| epoch  21 |   450/  559 batches | lr 0.0000 | ms/batch 173.92 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "| epoch  21 |   500/  559 batches | lr 0.0000 | ms/batch 172.97 | loss  3.66 | ppl    39.03 | bpc    5.286\n",
            "| epoch  21 |   550/  559 batches | lr 0.0000 | ms/batch 173.00 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 99.94s | valid loss  3.59 | valid ppl    36.38 | bpc    5.185\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |    50/  559 batches | lr 0.0000 | ms/batch 176.62 | loss  3.74 | ppl    42.31 | bpc    5.403\n",
            "| epoch  22 |   100/  559 batches | lr 0.0000 | ms/batch 173.65 | loss  3.67 | ppl    39.33 | bpc    5.298\n",
            "| epoch  22 |   150/  559 batches | lr 0.0000 | ms/batch 173.42 | loss  3.67 | ppl    39.29 | bpc    5.296\n",
            "| epoch  22 |   200/  559 batches | lr 0.0000 | ms/batch 173.25 | loss  3.67 | ppl    39.35 | bpc    5.298\n",
            "| epoch  22 |   250/  559 batches | lr 0.0000 | ms/batch 173.89 | loss  3.67 | ppl    39.35 | bpc    5.298\n",
            "| epoch  22 |   300/  559 batches | lr 0.0000 | ms/batch 173.82 | loss  3.67 | ppl    39.26 | bpc    5.295\n",
            "| epoch  22 |   350/  559 batches | lr 0.0000 | ms/batch 173.59 | loss  3.67 | ppl    39.31 | bpc    5.297\n",
            "| epoch  22 |   400/  559 batches | lr 0.0000 | ms/batch 173.35 | loss  3.67 | ppl    39.31 | bpc    5.297\n",
            "| epoch  22 |   450/  559 batches | lr 0.0000 | ms/batch 172.97 | loss  3.67 | ppl    39.31 | bpc    5.297\n",
            "| epoch  22 |   500/  559 batches | lr 0.0000 | ms/batch 173.43 | loss  3.67 | ppl    39.29 | bpc    5.296\n",
            "| epoch  22 |   550/  559 batches | lr 0.0000 | ms/batch 173.65 | loss  3.67 | ppl    39.36 | bpc    5.299\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 99.63s | valid loss  3.60 | valid ppl    36.65 | bpc    5.196\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |    50/  559 batches | lr 0.0000 | ms/batch 176.73 | loss  3.74 | ppl    42.03 | bpc    5.393\n",
            "| epoch  23 |   100/  559 batches | lr 0.0000 | ms/batch 173.57 | loss  3.66 | ppl    39.00 | bpc    5.286\n",
            "| epoch  23 |   150/  559 batches | lr 0.0000 | ms/batch 173.18 | loss  3.66 | ppl    39.04 | bpc    5.287\n",
            "| epoch  23 |   200/  559 batches | lr 0.0000 | ms/batch 173.78 | loss  3.67 | ppl    39.06 | bpc    5.288\n",
            "| epoch  23 |   250/  559 batches | lr 0.0000 | ms/batch 172.84 | loss  3.66 | ppl    39.05 | bpc    5.287\n",
            "| epoch  23 |   300/  559 batches | lr 0.0000 | ms/batch 173.53 | loss  3.67 | ppl    39.07 | bpc    5.288\n",
            "| epoch  23 |   350/  559 batches | lr 0.0000 | ms/batch 173.74 | loss  3.67 | ppl    39.06 | bpc    5.288\n",
            "| epoch  23 |   400/  559 batches | lr 0.0000 | ms/batch 172.90 | loss  3.66 | ppl    39.01 | bpc    5.286\n",
            "| epoch  23 |   450/  559 batches | lr 0.0000 | ms/batch 173.23 | loss  3.66 | ppl    39.01 | bpc    5.286\n",
            "| epoch  23 |   500/  559 batches | lr 0.0000 | ms/batch 173.71 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "| epoch  23 |   550/  559 batches | lr 0.0000 | ms/batch 173.14 | loss  3.66 | ppl    39.00 | bpc    5.285\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 99.58s | valid loss  3.59 | valid ppl    36.36 | bpc    5.184\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |    50/  559 batches | lr 0.0000 | ms/batch 176.42 | loss  3.74 | ppl    42.27 | bpc    5.402\n",
            "| epoch  24 |   100/  559 batches | lr 0.0000 | ms/batch 173.14 | loss  3.67 | ppl    39.25 | bpc    5.295\n",
            "| epoch  24 |   150/  559 batches | lr 0.0000 | ms/batch 173.34 | loss  3.67 | ppl    39.27 | bpc    5.296\n",
            "| epoch  24 |   200/  559 batches | lr 0.0000 | ms/batch 172.81 | loss  3.67 | ppl    39.25 | bpc    5.295\n",
            "| epoch  24 |   250/  559 batches | lr 0.0000 | ms/batch 173.49 | loss  3.67 | ppl    39.23 | bpc    5.294\n",
            "| epoch  24 |   300/  559 batches | lr 0.0000 | ms/batch 173.25 | loss  3.67 | ppl    39.23 | bpc    5.294\n",
            "| epoch  24 |   350/  559 batches | lr 0.0000 | ms/batch 173.86 | loss  3.67 | ppl    39.21 | bpc    5.293\n",
            "| epoch  24 |   400/  559 batches | lr 0.0000 | ms/batch 174.10 | loss  3.67 | ppl    39.22 | bpc    5.293\n",
            "| epoch  24 |   450/  559 batches | lr 0.0000 | ms/batch 174.48 | loss  3.67 | ppl    39.25 | bpc    5.295\n",
            "| epoch  24 |   500/  559 batches | lr 0.0000 | ms/batch 174.13 | loss  3.67 | ppl    39.25 | bpc    5.295\n",
            "| epoch  24 |   550/  559 batches | lr 0.0000 | ms/batch 174.28 | loss  3.67 | ppl    39.22 | bpc    5.293\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 99.74s | valid loss  3.60 | valid ppl    36.53 | bpc    5.191\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |    50/  559 batches | lr 0.0000 | ms/batch 177.11 | loss  3.75 | ppl    42.55 | bpc    5.411\n",
            "| epoch  25 |   100/  559 batches | lr 0.0000 | ms/batch 173.78 | loss  3.68 | ppl    39.52 | bpc    5.305\n",
            "| epoch  25 |   150/  559 batches | lr 0.0000 | ms/batch 173.92 | loss  3.68 | ppl    39.51 | bpc    5.304\n",
            "| epoch  25 |   200/  559 batches | lr 0.0000 | ms/batch 174.14 | loss  3.68 | ppl    39.57 | bpc    5.306\n",
            "| epoch  25 |   250/  559 batches | lr 0.0000 | ms/batch 173.98 | loss  3.68 | ppl    39.54 | bpc    5.305\n",
            "| epoch  25 |   300/  559 batches | lr 0.0000 | ms/batch 174.05 | loss  3.67 | ppl    39.43 | bpc    5.301\n",
            "| epoch  25 |   350/  559 batches | lr 0.0000 | ms/batch 174.11 | loss  3.68 | ppl    39.51 | bpc    5.304\n",
            "| epoch  25 |   400/  559 batches | lr 0.0000 | ms/batch 174.05 | loss  3.68 | ppl    39.49 | bpc    5.304\n",
            "| epoch  25 |   450/  559 batches | lr 0.0000 | ms/batch 173.95 | loss  3.68 | ppl    39.47 | bpc    5.303\n",
            "| epoch  25 |   500/  559 batches | lr 0.0000 | ms/batch 173.55 | loss  3.68 | ppl    39.52 | bpc    5.304\n",
            "| epoch  25 |   550/  559 batches | lr 0.0000 | ms/batch 173.94 | loss  3.67 | ppl    39.43 | bpc    5.301\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 99.90s | valid loss  3.60 | valid ppl    36.74 | bpc    5.199\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  3.07 | test ppl    21.50 | bpc    4.426\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-NfcUZZDpPii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build the model\n",
        "interval = 50 # interval to report\n",
        "ntokens = len(corpus.dictionary) # 10000\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout)\n",
        "save = '/content/output/model_test_character_adaptive.pt'\n",
        "checkpoint = \"/content/output/model_test_character_noise.pt\"\n",
        "\n",
        "# Load checkpoint\n",
        "if checkpoint != '':\n",
        "    model = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "\n",
        "print(model)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-k5ABGNob_x",
        "outputId": "0a465905-2366-465b-8099-5987d4e7b1cb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(50, 256)\n",
            "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
            "  (decoder): Linear(in_features=1000, out_features=50, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # choose a optimizer\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    # train_data size(batchcnt, bsz)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        # print(hidden.to(device))\n",
        "        output, hidden = model(data.to(device), hidden)\n",
        "        loss = criterion(output.to(device), targets.to(device))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_value_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        l2_lambda = 0.01\n",
        "        l2_reg = torch.tensor(0.).to(device)\n",
        "        for param in model.rnn.parameters():\n",
        "            l2_reg += torch.norm(param.to(device))\n",
        "\n",
        "        total_loss += loss.data\n",
        "        total_loss += l2_lambda * l2_reg\n",
        "\n",
        "        if batch % interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                elapsed * 1000 / interval, cur_loss, math.exp(cur_loss), cur_loss / math.log(2)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n"
      ],
      "metadata": {
        "id": "dW2e51KCAm1Q"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import vector_to_parameters, parameters_to_vector\n",
        "lr = lr\n",
        "best_val_loss = None\n",
        "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "opts = 'SGD'\n",
        "# if opt == 'Adam':\n",
        "#     opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
        "#     lr = 0.001\n",
        "# if args.opt == 'Momentum':\n",
        "#     opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.8)\n",
        "# if args.opt == 'RMSprop':\n",
        "#     opt = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
        "#     lr = 0.001\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        model.to(device)\n",
        "\n",
        "        param_vector = parameters_to_vector(model.rnn.parameters())\n",
        "        param_vector.to(device)\n",
        "        n_params = len(param_vector)\n",
        "        noise = torch.distributions.Normal(loc=torch.tensor(0.), scale=torch.tensor(0.075)).sample_n(n_params)\n",
        "        param_vector.add_(noise.to(device))\n",
        "        \n",
        "        vector_to_parameters(param_vector, model.rnn.parameters())\n",
        "        model.to(device)\n",
        "        \n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | bpc {:8.3f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if opts == 'SGD' or opts == 'Momentum':\n",
        "                lr /= 4.0\n",
        "                for group in opt.param_groups:\n",
        "                    group['lr'] = lr\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AamuIRSowPk",
        "outputId": "4e7dd938-4897-405e-f340-90ef36a281f8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:161: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n",
            "  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  559 batches | lr 0.0001 | ms/batch 175.09 | loss  8.39 | ppl  4399.20 | bpc   12.103\n",
            "| epoch   1 |   100/  559 batches | lr 0.0001 | ms/batch 170.32 | loss  8.19 | ppl  3598.25 | bpc   11.813\n",
            "| epoch   1 |   150/  559 batches | lr 0.0001 | ms/batch 169.06 | loss  8.13 | ppl  3395.52 | bpc   11.729\n",
            "| epoch   1 |   200/  559 batches | lr 0.0001 | ms/batch 169.75 | loss  8.07 | ppl  3199.12 | bpc   11.643\n",
            "| epoch   1 |   250/  559 batches | lr 0.0001 | ms/batch 169.55 | loss  8.02 | ppl  3030.41 | bpc   11.565\n",
            "| epoch   1 |   300/  559 batches | lr 0.0001 | ms/batch 169.52 | loss  7.98 | ppl  2920.70 | bpc   11.512\n",
            "| epoch   1 |   350/  559 batches | lr 0.0001 | ms/batch 168.80 | loss  7.95 | ppl  2826.23 | bpc   11.465\n",
            "| epoch   1 |   400/  559 batches | lr 0.0001 | ms/batch 169.23 | loss  7.92 | ppl  2756.35 | bpc   11.429\n",
            "| epoch   1 |   450/  559 batches | lr 0.0001 | ms/batch 168.68 | loss  7.90 | ppl  2700.34 | bpc   11.399\n",
            "| epoch   1 |   500/  559 batches | lr 0.0001 | ms/batch 167.96 | loss  7.89 | ppl  2664.62 | bpc   11.380\n",
            "| epoch   1 |   550/  559 batches | lr 0.0001 | ms/batch 169.09 | loss  7.88 | ppl  2632.35 | bpc   11.362\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 97.29s | valid loss  3.08 | valid ppl    21.76 | bpc    4.444\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    50/  559 batches | lr 0.0001 | ms/batch 172.78 | loss  8.69 | ppl  5949.84 | bpc   12.539\n",
            "| epoch   2 |   100/  559 batches | lr 0.0001 | ms/batch 168.08 | loss  8.50 | ppl  4904.44 | bpc   12.260\n",
            "| epoch   2 |   150/  559 batches | lr 0.0001 | ms/batch 168.16 | loss  8.48 | ppl  4802.81 | bpc   12.230\n",
            "| epoch   2 |   200/  559 batches | lr 0.0001 | ms/batch 169.30 | loss  8.46 | ppl  4731.57 | bpc   12.208\n",
            "| epoch   2 |   250/  559 batches | lr 0.0001 | ms/batch 168.59 | loss  8.44 | ppl  4640.70 | bpc   12.180\n",
            "| epoch   2 |   300/  559 batches | lr 0.0001 | ms/batch 168.60 | loss  8.43 | ppl  4596.77 | bpc   12.166\n",
            "| epoch   2 |   350/  559 batches | lr 0.0001 | ms/batch 169.71 | loss  8.42 | ppl  4557.32 | bpc   12.154\n",
            "| epoch   2 |   400/  559 batches | lr 0.0001 | ms/batch 168.28 | loss  8.42 | ppl  4529.57 | bpc   12.145\n",
            "| epoch   2 |   450/  559 batches | lr 0.0001 | ms/batch 169.44 | loss  8.41 | ppl  4495.01 | bpc   12.134\n",
            "| epoch   2 |   500/  559 batches | lr 0.0001 | ms/batch 169.17 | loss  8.40 | ppl  4467.46 | bpc   12.125\n",
            "| epoch   2 |   550/  559 batches | lr 0.0001 | ms/batch 167.69 | loss  8.40 | ppl  4449.22 | bpc   12.119\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 96.88s | valid loss  3.06 | valid ppl    21.35 | bpc    4.416\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  559 batches | lr 0.0001 | ms/batch 171.79 | loss  9.14 | ppl  9352.66 | bpc   13.191\n",
            "| epoch   3 |   100/  559 batches | lr 0.0001 | ms/batch 168.68 | loss  8.95 | ppl  7727.93 | bpc   12.916\n",
            "| epoch   3 |   150/  559 batches | lr 0.0001 | ms/batch 168.43 | loss  8.94 | ppl  7651.05 | bpc   12.901\n",
            "| epoch   3 |   200/  559 batches | lr 0.0001 | ms/batch 169.57 | loss  8.94 | ppl  7601.86 | bpc   12.892\n",
            "| epoch   3 |   250/  559 batches | lr 0.0001 | ms/batch 168.33 | loss  8.92 | ppl  7504.73 | bpc   12.874\n",
            "| epoch   3 |   300/  559 batches | lr 0.0001 | ms/batch 169.85 | loss  8.92 | ppl  7473.55 | bpc   12.868\n",
            "| epoch   3 |   350/  559 batches | lr 0.0001 | ms/batch 168.68 | loss  8.91 | ppl  7440.21 | bpc   12.861\n",
            "| epoch   3 |   400/  559 batches | lr 0.0001 | ms/batch 169.29 | loss  8.91 | ppl  7411.62 | bpc   12.856\n",
            "| epoch   3 |   450/  559 batches | lr 0.0001 | ms/batch 169.18 | loss  8.90 | ppl  7367.03 | bpc   12.847\n",
            "| epoch   3 |   500/  559 batches | lr 0.0001 | ms/batch 168.67 | loss  8.90 | ppl  7344.07 | bpc   12.842\n",
            "| epoch   3 |   550/  559 batches | lr 0.0001 | ms/batch 169.35 | loss  8.90 | ppl  7318.25 | bpc   12.837\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 97.00s | valid loss  3.06 | valid ppl    21.27 | bpc    4.411\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  559 batches | lr 0.0001 | ms/batch 173.16 | loss  9.60 | ppl 14825.90 | bpc   13.856\n",
            "| epoch   4 |   100/  559 batches | lr 0.0001 | ms/batch 168.93 | loss  9.40 | ppl 12142.48 | bpc   13.568\n",
            "| epoch   4 |   150/  559 batches | lr 0.0001 | ms/batch 169.04 | loss  9.40 | ppl 12075.79 | bpc   13.560\n",
            "| epoch   4 |   200/  559 batches | lr 0.0001 | ms/batch 168.41 | loss  9.39 | ppl 12023.58 | bpc   13.554\n",
            "| epoch   4 |   250/  559 batches | lr 0.0001 | ms/batch 169.24 | loss  9.38 | ppl 11841.12 | bpc   13.532\n",
            "| epoch   4 |   300/  559 batches | lr 0.0001 | ms/batch 168.69 | loss  9.37 | ppl 11743.53 | bpc   13.520\n",
            "| epoch   4 |   350/  559 batches | lr 0.0001 | ms/batch 168.81 | loss  9.36 | ppl 11654.79 | bpc   13.509\n",
            "| epoch   4 |   400/  559 batches | lr 0.0001 | ms/batch 169.36 | loss  9.36 | ppl 11611.99 | bpc   13.503\n",
            "| epoch   4 |   450/  559 batches | lr 0.0001 | ms/batch 168.68 | loss  9.36 | ppl 11568.00 | bpc   13.498\n",
            "| epoch   4 |   500/  559 batches | lr 0.0001 | ms/batch 167.71 | loss  9.35 | ppl 11538.93 | bpc   13.494\n",
            "| epoch   4 |   550/  559 batches | lr 0.0001 | ms/batch 170.07 | loss  9.35 | ppl 11489.92 | bpc   13.488\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 97.03s | valid loss  3.05 | valid ppl    21.12 | bpc    4.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  559 batches | lr 0.0001 | ms/batch 172.88 | loss 10.03 | ppl 22609.70 | bpc   14.465\n",
            "| epoch   5 |   100/  559 batches | lr 0.0001 | ms/batch 169.99 | loss  9.82 | ppl 18344.05 | bpc   14.163\n",
            "| epoch   5 |   150/  559 batches | lr 0.0001 | ms/batch 169.06 | loss  9.81 | ppl 18261.42 | bpc   14.157\n",
            "| epoch   5 |   200/  559 batches | lr 0.0001 | ms/batch 170.16 | loss  9.81 | ppl 18298.95 | bpc   14.159\n",
            "| epoch   5 |   250/  559 batches | lr 0.0001 | ms/batch 169.72 | loss  9.81 | ppl 18218.52 | bpc   14.153\n",
            "| epoch   5 |   300/  559 batches | lr 0.0001 | ms/batch 169.52 | loss  9.80 | ppl 18100.52 | bpc   14.144\n",
            "| epoch   5 |   350/  559 batches | lr 0.0001 | ms/batch 170.17 | loss  9.80 | ppl 17948.89 | bpc   14.132\n",
            "| epoch   5 |   400/  559 batches | lr 0.0001 | ms/batch 169.22 | loss  9.79 | ppl 17841.06 | bpc   14.123\n",
            "| epoch   5 |   450/  559 batches | lr 0.0001 | ms/batch 169.71 | loss  9.78 | ppl 17751.22 | bpc   14.116\n",
            "| epoch   5 |   500/  559 batches | lr 0.0001 | ms/batch 169.18 | loss  9.78 | ppl 17746.70 | bpc   14.115\n",
            "| epoch   5 |   550/  559 batches | lr 0.0001 | ms/batch 169.49 | loss  9.78 | ppl 17663.08 | bpc   14.108\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 97.36s | valid loss  3.05 | valid ppl    21.14 | bpc    4.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  559 batches | lr 0.0000 | ms/batch 171.07 | loss 10.41 | ppl 33286.39 | bpc   15.023\n",
            "| epoch   6 |   100/  559 batches | lr 0.0000 | ms/batch 169.42 | loss 10.21 | ppl 27051.47 | bpc   14.723\n",
            "| epoch   6 |   150/  559 batches | lr 0.0000 | ms/batch 168.60 | loss 10.21 | ppl 27097.87 | bpc   14.726\n",
            "| epoch   6 |   200/  559 batches | lr 0.0000 | ms/batch 169.41 | loss 10.21 | ppl 27146.27 | bpc   14.728\n",
            "| epoch   6 |   250/  559 batches | lr 0.0000 | ms/batch 168.74 | loss 10.21 | ppl 27052.35 | bpc   14.723\n",
            "| epoch   6 |   300/  559 batches | lr 0.0000 | ms/batch 168.40 | loss 10.21 | ppl 27106.87 | bpc   14.726\n",
            "| epoch   6 |   350/  559 batches | lr 0.0000 | ms/batch 169.32 | loss 10.20 | ppl 27006.52 | bpc   14.721\n",
            "| epoch   6 |   400/  559 batches | lr 0.0000 | ms/batch 168.52 | loss 10.20 | ppl 26846.15 | bpc   14.712\n",
            "| epoch   6 |   450/  559 batches | lr 0.0000 | ms/batch 167.93 | loss 10.19 | ppl 26680.81 | bpc   14.704\n",
            "| epoch   6 |   500/  559 batches | lr 0.0000 | ms/batch 169.07 | loss 10.19 | ppl 26618.24 | bpc   14.700\n",
            "| epoch   6 |   550/  559 batches | lr 0.0000 | ms/batch 169.71 | loss 10.18 | ppl 26497.25 | bpc   14.694\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 96.90s | valid loss  3.06 | valid ppl    21.24 | bpc    4.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  559 batches | lr 0.0000 | ms/batch 172.96 | loss 10.81 | ppl 49611.05 | bpc   15.598\n",
            "| epoch   7 |   100/  559 batches | lr 0.0000 | ms/batch 169.01 | loss 10.60 | ppl 40020.65 | bpc   15.288\n",
            "| epoch   7 |   150/  559 batches | lr 0.0000 | ms/batch 169.66 | loss 10.60 | ppl 40036.41 | bpc   15.289\n",
            "| epoch   7 |   200/  559 batches | lr 0.0000 | ms/batch 168.33 | loss 10.60 | ppl 40187.09 | bpc   15.294\n",
            "| epoch   7 |   250/  559 batches | lr 0.0000 | ms/batch 169.92 | loss 10.60 | ppl 40037.44 | bpc   15.289\n",
            "| epoch   7 |   300/  559 batches | lr 0.0000 | ms/batch 169.06 | loss 10.60 | ppl 40024.31 | bpc   15.289\n",
            "| epoch   7 |   350/  559 batches | lr 0.0000 | ms/batch 169.94 | loss 10.60 | ppl 40004.81 | bpc   15.288\n",
            "| epoch   7 |   400/  559 batches | lr 0.0000 | ms/batch 168.95 | loss 10.59 | ppl 39916.28 | bpc   15.285\n",
            "| epoch   7 |   450/  559 batches | lr 0.0000 | ms/batch 169.85 | loss 10.59 | ppl 39873.82 | bpc   15.283\n",
            "| epoch   7 |   500/  559 batches | lr 0.0000 | ms/batch 168.63 | loss 10.59 | ppl 39839.58 | bpc   15.282\n",
            "| epoch   7 |   550/  559 batches | lr 0.0000 | ms/batch 169.68 | loss 10.59 | ppl 39796.40 | bpc   15.280\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 97.19s | valid loss  3.08 | valid ppl    21.74 | bpc    4.443\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  559 batches | lr 0.0000 | ms/batch 171.31 | loss 11.19 | ppl 72151.16 | bpc   16.139\n",
            "| epoch   8 |   100/  559 batches | lr 0.0000 | ms/batch 169.39 | loss 10.97 | ppl 57900.88 | bpc   15.821\n",
            "| epoch   8 |   150/  559 batches | lr 0.0000 | ms/batch 168.90 | loss 10.97 | ppl 57944.58 | bpc   15.822\n",
            "| epoch   8 |   200/  559 batches | lr 0.0000 | ms/batch 168.57 | loss 10.97 | ppl 57952.26 | bpc   15.823\n",
            "| epoch   8 |   250/  559 batches | lr 0.0000 | ms/batch 169.44 | loss 10.96 | ppl 57767.85 | bpc   15.818\n",
            "| epoch   8 |   300/  559 batches | lr 0.0000 | ms/batch 168.46 | loss 10.96 | ppl 57798.76 | bpc   15.819\n",
            "| epoch   8 |   350/  559 batches | lr 0.0000 | ms/batch 169.36 | loss 10.96 | ppl 57781.68 | bpc   15.818\n",
            "| epoch   8 |   400/  559 batches | lr 0.0000 | ms/batch 169.09 | loss 10.96 | ppl 57807.91 | bpc   15.819\n",
            "| epoch   8 |   450/  559 batches | lr 0.0000 | ms/batch 169.17 | loss 10.96 | ppl 57802.29 | bpc   15.819\n",
            "| epoch   8 |   500/  559 batches | lr 0.0000 | ms/batch 169.00 | loss 10.96 | ppl 57795.23 | bpc   15.819\n",
            "| epoch   8 |   550/  559 batches | lr 0.0000 | ms/batch 168.27 | loss 10.96 | ppl 57730.78 | bpc   15.817\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 96.95s | valid loss  3.09 | valid ppl    22.02 | bpc    4.461\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  559 batches | lr 0.0000 | ms/batch 172.31 | loss 11.54 | ppl 103223.62 | bpc   16.655\n",
            "| epoch   9 |   100/  559 batches | lr 0.0000 | ms/batch 168.75 | loss 11.32 | ppl 82261.26 | bpc   16.328\n",
            "| epoch   9 |   150/  559 batches | lr 0.0000 | ms/batch 168.42 | loss 11.32 | ppl 82443.23 | bpc   16.331\n",
            "| epoch   9 |   200/  559 batches | lr 0.0000 | ms/batch 169.43 | loss 11.32 | ppl 82567.94 | bpc   16.333\n",
            "| epoch   9 |   250/  559 batches | lr 0.0000 | ms/batch 168.39 | loss 11.32 | ppl 82262.59 | bpc   16.328\n",
            "| epoch   9 |   300/  559 batches | lr 0.0000 | ms/batch 168.07 | loss 11.32 | ppl 82384.60 | bpc   16.330\n",
            "| epoch   9 |   350/  559 batches | lr 0.0000 | ms/batch 169.02 | loss 11.32 | ppl 82218.91 | bpc   16.327\n",
            "| epoch   9 |   400/  559 batches | lr 0.0000 | ms/batch 168.66 | loss 11.32 | ppl 82289.74 | bpc   16.328\n",
            "| epoch   9 |   450/  559 batches | lr 0.0000 | ms/batch 168.72 | loss 11.32 | ppl 82234.51 | bpc   16.327\n",
            "| epoch   9 |   500/  559 batches | lr 0.0000 | ms/batch 169.17 | loss 11.32 | ppl 82381.06 | bpc   16.330\n",
            "| epoch   9 |   550/  559 batches | lr 0.0000 | ms/batch 168.60 | loss 11.32 | ppl 82274.13 | bpc   16.328\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 96.88s | valid loss  3.10 | valid ppl    22.21 | bpc    4.473\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  559 batches | lr 0.0000 | ms/batch 172.46 | loss 11.92 | ppl 150011.97 | bpc   17.195\n",
            "| epoch  10 |   100/  559 batches | lr 0.0000 | ms/batch 168.87 | loss 11.68 | ppl 118773.51 | bpc   16.858\n",
            "| epoch  10 |   150/  559 batches | lr 0.0000 | ms/batch 167.77 | loss 11.69 | ppl 118787.79 | bpc   16.858\n",
            "| epoch  10 |   200/  559 batches | lr 0.0000 | ms/batch 167.77 | loss 11.69 | ppl 119134.83 | bpc   16.862\n",
            "| epoch  10 |   250/  559 batches | lr 0.0000 | ms/batch 168.10 | loss 11.68 | ppl 118583.94 | bpc   16.856\n",
            "| epoch  10 |   300/  559 batches | lr 0.0000 | ms/batch 168.94 | loss 11.68 | ppl 118730.71 | bpc   16.857\n",
            "| epoch  10 |   350/  559 batches | lr 0.0000 | ms/batch 168.58 | loss 11.68 | ppl 118510.00 | bpc   16.855\n",
            "| epoch  10 |   400/  559 batches | lr 0.0000 | ms/batch 168.29 | loss 11.68 | ppl 118738.07 | bpc   16.857\n",
            "| epoch  10 |   450/  559 batches | lr 0.0000 | ms/batch 168.60 | loss 11.68 | ppl 118766.04 | bpc   16.858\n",
            "| epoch  10 |   500/  559 batches | lr 0.0000 | ms/batch 169.07 | loss 11.68 | ppl 118712.59 | bpc   16.857\n",
            "| epoch  10 |   550/  559 batches | lr 0.0000 | ms/batch 168.38 | loss 11.68 | ppl 118610.97 | bpc   16.856\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 96.78s | valid loss  3.13 | valid ppl    22.89 | bpc    4.516\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  3.06 | test ppl    21.28 | bpc    4.412\n",
            "=========================================================================================\n"
          ]
        }
      ]
    }
  ]
}