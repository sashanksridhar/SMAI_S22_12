{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PennTreebankExperiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!echo \"=== Acquiring datasets ===\"\n",
        "!echo \"---\"\n",
        "\n",
        "!mkdir -p data\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgHmdlPhL8oF",
        "outputId": "aa6336f4-446a-4e23-e69b-76ef7f67cfee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Acquiring datasets ===\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz_r7xiOI_6s",
        "outputId": "edc3838e-cbce-4dfa-efec-7f8a1cbc1a01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "L0wfV1YeMKok"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"- Downloading Penn Treebank (PTB)\"\n",
        "!wget --quiet --continue http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "!tar -xzf simple-examples.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK7K7MACI4nY",
        "outputId": "73c09e7d-b960-4b3c-d7f3-d71398222e19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Downloading Penn Treebank (PTB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p penn\n",
        "%cd penn\n",
        "!mv ../simple-examples/data/ptb.train.txt train.txt\n",
        "!mv ../simple-examples/data/ptb.test.txt test.txt\n",
        "!mv ../simple-examples/data/ptb.valid.txt valid.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdDffaaGJAer",
        "outputId": "1eb554c1-348a-48a5-f8c7-f51aa7f1e6b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data/penn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"- Downloading Penn Treebank (Character)\"\n",
        "!mkdir -p ../pennchar\n",
        "%cd ../pennchar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCTp_5tbNPhS",
        "outputId": "ae20e2a6-7912-42de-bd4d-4a52e24453d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Downloading Penn Treebank (Character)\n",
            "/content/data/pennchar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ../simple-examples/data/ptb.char.train.txt train.txt\n",
        "!mv ../simple-examples/data/ptb.char.test.txt test.txt\n",
        "!mv ../simple-examples/data/ptb.char.valid.txt valid.txt"
      ],
      "metadata": {
        "id": "74fCeou3JAqF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ../simple-examples/"
      ],
      "metadata": {
        "id": "5-sFaSl5NbMX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Character - Without Noise**"
      ],
      "metadata": {
        "id": "6rnPPAw67682"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp) # Token2Embeddings\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout) #(seq_len, batch_size, emb_size)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        self.init_weights()\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.05\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input size(bptt, bsz)\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        # emb size(bptt, bsz, embsize)\n",
        "        # hid size(layers, bsz, nhid)\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # output size(bptt, bsz, nhid)\n",
        "        output = self.drop(output)\n",
        "        # decoder: nhid -> ntoken\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        # LSTM h and c\n",
        "        weight = next(self.parameters()).data\n",
        "        return weight.new_zeros(self.nlayers, bsz, self.nhid), weight.new_zeros(self.nlayers, bsz, self.nhid)"
      ],
      "metadata": {
        "id": "m_kekY5EAKbx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = Counter()\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        token_id = self.word2idx[word]\n",
        "        self.counter[token_id] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids"
      ],
      "metadata": {
        "id": "3_xiIWpEAOGD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import math"
      ],
      "metadata": {
        "id": "RhzjMOhhAQxf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9y8nlquAgmg",
        "outputId": "033baaac-e2c5-4702-c648-d2c3a9e2622d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOf8u9RhAlcT",
        "outputId": "65c30ab5-2de1-42c4-d8bb-c17da3adf157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = '/content/data/pennchar'\n",
        "batch_size = 256\n",
        "emsize = 256\n",
        "nlayers = 1\n",
        "nhid = 1000\n",
        "lr = 0.0001\n",
        "dropout = 0.5\n",
        "checkpoint = ''\n",
        "clip = 1\n",
        "bptt = 35\n",
        "epochs = 10\n",
        "save = '/content/output/model_test_character_none.pt'\n",
        "\n",
        "torch.manual_seed(1111)\n",
        "\n",
        "# Load data\n",
        "corpus = Corpus(data)\n"
      ],
      "metadata": {
        "id": "pzlWhsBYAYMl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data"
      ],
      "metadata": {
        "id": "6FvrRumyA1L7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_batch_size = 256\n",
        "train_data = batchify(corpus.train, batch_size) # size(total_len//bsz, bsz)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ],
      "metadata": {
        "id": "bGLbW8TzAbRw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNXQXMK9BA4B",
        "outputId": "c5a072d6-129f-4399-de1b-1600076cec1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byMAy3UfBCKQ",
        "outputId": "fc894476-d76b-4a5d-cfed-ca493ba4152a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  9,  ...,  3, 20,  0],\n",
              "        [ 1,  2, 10,  ...,  7,  7, 24],\n",
              "        [ 2,  3,  7,  ..., 17, 13,  0],\n",
              "        ...,\n",
              "        [ 8,  4,  7,  ..., 21, 28, 26],\n",
              "        [ 7, 10, 15,  ...,  3,  3, 10],\n",
              "        [ 4,  9,  2,  ..., 16, 12,  5]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data.to(device)\n",
        "test_data.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1NZXjd9BLM7",
        "outputId": "7f4a296a-0a65-4716-dbe6-6b41e93d14e2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5,  3,  3,  ...,  7,  3, 14],\n",
              "        [ 7, 29, 24,  ..., 15, 18,  3],\n",
              "        [ 3,  3,  0,  ...,  5,  2, 33],\n",
              "        ...,\n",
              "        [ 3,  3,  3,  ...,  7,  2, 17],\n",
              "        [ 7,  0,  8,  ..., 18, 21,  3],\n",
              "        [ 2, 16, 20,  ...,  1,  0, 13]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "interval = 50 # interval to report\n",
        "ntokens = len(corpus.dictionary) # 10000\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout)\n",
        "\n",
        "# Load checkpoint\n",
        "if checkpoint != '':\n",
        "    model = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "\n",
        "print(model)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-dm-QF-BQlt",
        "outputId": "30eae925-e0ed-484c-8518-3fc9e17156ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(50, 256)\n",
            "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
            "  (decoder): Linear(in_features=1000, out_features=50, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Sd1VoOBYb3",
        "outputId": "72beda04-ea32-420c-9b4e-25f3b0bfd020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (drop): Dropout(p=0.5, inplace=False)\n",
              "  (encoder): Embedding(50, 256)\n",
              "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
              "  (decoder): Linear(in_features=1000, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    # detach\n",
        "    return tuple(v.clone().detach() for v in h)\n"
      ],
      "metadata": {
        "id": "a4NH_KQaBbW-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_batch(source, i):\n",
        "    # source: size(total_len//bsz, bsz)\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    #data = torch.tensor(source[i:i+seq_len]) # size(bptt, bsz)\n",
        "    data = source[i:i+seq_len].clone().detach()\n",
        "    target = source[i+1:i+1+seq_len].clone().detach().view(-1)\n",
        "    #target = torch.tensor(source[i+1:i+1+seq_len].view(-1)) # size(bptt * bsz)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "YolhdEasBgCt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        ntokens = len(corpus.dictionary)\n",
        "        hidden = model.init_hidden(eval_batch_size) #hidden size(nlayers, bsz, hdsize)\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):# iterate over every timestep\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data.to(device), hidden)\n",
        "            # model input and output\n",
        "            # inputdata size(bptt, bsz), and size(bptt, bsz, embsize) after embedding\n",
        "            # output size(bptt*bsz, ntoken)\n",
        "            total_loss += len(data) * criterion(output.to(device), targets.to(device)).data\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        return total_loss / len(data_source)\n"
      ],
      "metadata": {
        "id": "RyWNpGhkBgyt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # choose a optimizer\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    # train_data size(batchcnt, bsz)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        # print(hidden.to(device))\n",
        "        output, hidden = model(data.to(device), hidden)\n",
        "        loss = criterion(output.to(device), targets.to(device))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_value_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                elapsed * 1000 / interval, cur_loss, math.exp(cur_loss), cur_loss / math.log(2)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n"
      ],
      "metadata": {
        "id": "37Y1ELGCBiaF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of tokens:\")\n",
        "print(\"Train: \", len(corpus.train))\n",
        "print(\"Valid: \", len(corpus.valid))\n",
        "print(\"Test:  \", len(corpus.test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIOn7vmfBtif",
        "outputId": "a549bd30-8a60-4e2c-bb64-9151c4d41b28"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens:\n",
            "Train:  5017483\n",
            "Valid:  393043\n",
            "Test:   442424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = lr\n",
        "best_val_loss = None\n",
        "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "opts = 'SGD'\n",
        "# if opt == 'Adam':\n",
        "#     opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
        "#     lr = 0.001\n",
        "# if args.opt == 'Momentum':\n",
        "#     opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.8)\n",
        "# if args.opt == 'RMSprop':\n",
        "#     opt = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
        "#     lr = 0.001\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | bpc {:8.3f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if opts == 'SGD' or opts == 'Momentum':\n",
        "                lr /= 4.0\n",
        "                for group in opt.param_groups:\n",
        "                    group['lr'] = lr\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCKEoCZjBk9q",
        "outputId": "07266aa7-1694-4a72-c758-1f9fbaebaa7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  559 batches | lr 0.0001 | ms/batch 179.50 | loss  3.99 | ppl    53.92 | bpc    5.753\n",
            "| epoch   1 |   100/  559 batches | lr 0.0001 | ms/batch 175.88 | loss  3.89 | ppl    49.09 | bpc    5.617\n",
            "| epoch   1 |   150/  559 batches | lr 0.0001 | ms/batch 175.70 | loss  3.87 | ppl    47.90 | bpc    5.582\n",
            "| epoch   1 |   200/  559 batches | lr 0.0001 | ms/batch 175.62 | loss  3.84 | ppl    46.49 | bpc    5.539\n",
            "| epoch   1 |   250/  559 batches | lr 0.0001 | ms/batch 175.46 | loss  3.81 | ppl    44.95 | bpc    5.490\n",
            "| epoch   1 |   300/  559 batches | lr 0.0001 | ms/batch 175.06 | loss  3.77 | ppl    43.41 | bpc    5.440\n",
            "| epoch   1 |   350/  559 batches | lr 0.0001 | ms/batch 174.96 | loss  3.73 | ppl    41.87 | bpc    5.388\n",
            "| epoch   1 |   400/  559 batches | lr 0.0001 | ms/batch 174.64 | loss  3.70 | ppl    40.36 | bpc    5.335\n",
            "| epoch   1 |   450/  559 batches | lr 0.0001 | ms/batch 174.95 | loss  3.66 | ppl    38.87 | bpc    5.281\n",
            "| epoch   1 |   500/  559 batches | lr 0.0001 | ms/batch 174.03 | loss  3.62 | ppl    37.40 | bpc    5.225\n",
            "| epoch   1 |   550/  559 batches | lr 0.0001 | ms/batch 174.58 | loss  3.58 | ppl    35.95 | bpc    5.168\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 100.52s | valid loss  3.55 | valid ppl    34.86 | bpc    5.123\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    50/  559 batches | lr 0.0001 | ms/batch 177.98 | loss  3.61 | ppl    36.79 | bpc    5.201\n",
            "| epoch   2 |   100/  559 batches | lr 0.0001 | ms/batch 174.51 | loss  3.49 | ppl    32.87 | bpc    5.039\n",
            "| epoch   2 |   150/  559 batches | lr 0.0001 | ms/batch 174.44 | loss  3.45 | ppl    31.54 | bpc    4.979\n",
            "| epoch   2 |   200/  559 batches | lr 0.0001 | ms/batch 174.92 | loss  3.41 | ppl    30.26 | bpc    4.919\n",
            "| epoch   2 |   250/  559 batches | lr 0.0001 | ms/batch 174.30 | loss  3.37 | ppl    28.96 | bpc    4.856\n",
            "| epoch   2 |   300/  559 batches | lr 0.0001 | ms/batch 174.86 | loss  3.33 | ppl    27.85 | bpc    4.799\n",
            "| epoch   2 |   350/  559 batches | lr 0.0001 | ms/batch 174.53 | loss  3.29 | ppl    26.85 | bpc    4.747\n",
            "| epoch   2 |   400/  559 batches | lr 0.0001 | ms/batch 174.82 | loss  3.26 | ppl    26.01 | bpc    4.701\n",
            "| epoch   2 |   450/  559 batches | lr 0.0001 | ms/batch 174.55 | loss  3.23 | ppl    25.27 | bpc    4.659\n",
            "| epoch   2 |   500/  559 batches | lr 0.0001 | ms/batch 174.85 | loss  3.20 | ppl    24.58 | bpc    4.619\n",
            "| epoch   2 |   550/  559 batches | lr 0.0001 | ms/batch 174.57 | loss  3.18 | ppl    23.93 | bpc    4.581\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 100.26s | valid loss  3.15 | valid ppl    23.45 | bpc    4.551\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  559 batches | lr 0.0001 | ms/batch 178.02 | loss  3.21 | ppl    24.85 | bpc    4.635\n",
            "| epoch   3 |   100/  559 batches | lr 0.0001 | ms/batch 174.62 | loss  3.13 | ppl    22.86 | bpc    4.515\n",
            "| epoch   3 |   150/  559 batches | lr 0.0001 | ms/batch 174.59 | loss  3.12 | ppl    22.56 | bpc    4.496\n",
            "| epoch   3 |   200/  559 batches | lr 0.0001 | ms/batch 174.49 | loss  3.11 | ppl    22.38 | bpc    4.484\n",
            "| epoch   3 |   250/  559 batches | lr 0.0001 | ms/batch 174.68 | loss  3.09 | ppl    22.03 | bpc    4.462\n",
            "| epoch   3 |   300/  559 batches | lr 0.0001 | ms/batch 174.65 | loss  3.09 | ppl    21.89 | bpc    4.452\n",
            "| epoch   3 |   350/  559 batches | lr 0.0001 | ms/batch 174.90 | loss  3.08 | ppl    21.71 | bpc    4.440\n",
            "| epoch   3 |   400/  559 batches | lr 0.0001 | ms/batch 174.55 | loss  3.07 | ppl    21.59 | bpc    4.432\n",
            "| epoch   3 |   450/  559 batches | lr 0.0001 | ms/batch 174.85 | loss  3.07 | ppl    21.44 | bpc    4.422\n",
            "| epoch   3 |   500/  559 batches | lr 0.0001 | ms/batch 174.34 | loss  3.06 | ppl    21.34 | bpc    4.415\n",
            "| epoch   3 |   550/  559 batches | lr 0.0001 | ms/batch 174.17 | loss  3.05 | ppl    21.21 | bpc    4.407\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 100.18s | valid loss  3.05 | valid ppl    21.08 | bpc    4.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  559 batches | lr 0.0001 | ms/batch 177.84 | loss  3.11 | ppl    22.52 | bpc    4.493\n",
            "| epoch   4 |   100/  559 batches | lr 0.0001 | ms/batch 174.76 | loss  3.05 | ppl    21.07 | bpc    4.397\n",
            "| epoch   4 |   150/  559 batches | lr 0.0001 | ms/batch 174.30 | loss  3.05 | ppl    21.03 | bpc    4.394\n",
            "| epoch   4 |   200/  559 batches | lr 0.0001 | ms/batch 174.12 | loss  3.05 | ppl    21.05 | bpc    4.396\n",
            "| epoch   4 |   250/  559 batches | lr 0.0001 | ms/batch 174.58 | loss  3.04 | ppl    20.90 | bpc    4.385\n",
            "| epoch   4 |   300/  559 batches | lr 0.0001 | ms/batch 173.77 | loss  3.04 | ppl    20.90 | bpc    4.385\n",
            "| epoch   4 |   350/  559 batches | lr 0.0001 | ms/batch 175.24 | loss  3.04 | ppl    20.84 | bpc    4.382\n",
            "| epoch   4 |   400/  559 batches | lr 0.0001 | ms/batch 174.72 | loss  3.04 | ppl    20.82 | bpc    4.380\n",
            "| epoch   4 |   450/  559 batches | lr 0.0001 | ms/batch 174.96 | loss  3.03 | ppl    20.76 | bpc    4.376\n",
            "| epoch   4 |   500/  559 batches | lr 0.0001 | ms/batch 174.81 | loss  3.03 | ppl    20.75 | bpc    4.375\n",
            "| epoch   4 |   550/  559 batches | lr 0.0001 | ms/batch 175.39 | loss  3.03 | ppl    20.67 | bpc    4.369\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 100.20s | valid loss  3.02 | valid ppl    20.54 | bpc    4.361\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  559 batches | lr 0.0001 | ms/batch 177.61 | loss  3.09 | ppl    22.02 | bpc    4.461\n",
            "| epoch   5 |   100/  559 batches | lr 0.0001 | ms/batch 174.36 | loss  3.03 | ppl    20.64 | bpc    4.367\n",
            "| epoch   5 |   150/  559 batches | lr 0.0001 | ms/batch 174.63 | loss  3.03 | ppl    20.64 | bpc    4.368\n",
            "| epoch   5 |   200/  559 batches | lr 0.0001 | ms/batch 174.04 | loss  3.03 | ppl    20.71 | bpc    4.372\n",
            "| epoch   5 |   250/  559 batches | lr 0.0001 | ms/batch 174.92 | loss  3.03 | ppl    20.60 | bpc    4.365\n",
            "| epoch   5 |   300/  559 batches | lr 0.0001 | ms/batch 174.93 | loss  3.03 | ppl    20.61 | bpc    4.365\n",
            "| epoch   5 |   350/  559 batches | lr 0.0001 | ms/batch 174.97 | loss  3.02 | ppl    20.59 | bpc    4.364\n",
            "| epoch   5 |   400/  559 batches | lr 0.0001 | ms/batch 173.88 | loss  3.02 | ppl    20.59 | bpc    4.364\n",
            "| epoch   5 |   450/  559 batches | lr 0.0001 | ms/batch 173.62 | loss  3.02 | ppl    20.56 | bpc    4.362\n",
            "| epoch   5 |   500/  559 batches | lr 0.0001 | ms/batch 173.70 | loss  3.02 | ppl    20.56 | bpc    4.362\n",
            "| epoch   5 |   550/  559 batches | lr 0.0001 | ms/batch 173.96 | loss  3.02 | ppl    20.51 | bpc    4.358\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 100.02s | valid loss  3.01 | valid ppl    20.36 | bpc    4.348\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  559 batches | lr 0.0001 | ms/batch 177.26 | loss  3.08 | ppl    21.85 | bpc    4.450\n",
            "| epoch   6 |   100/  559 batches | lr 0.0001 | ms/batch 173.92 | loss  3.02 | ppl    20.50 | bpc    4.358\n",
            "| epoch   6 |   150/  559 batches | lr 0.0001 | ms/batch 173.89 | loss  3.02 | ppl    20.51 | bpc    4.358\n",
            "| epoch   6 |   200/  559 batches | lr 0.0001 | ms/batch 173.80 | loss  3.02 | ppl    20.59 | bpc    4.364\n",
            "| epoch   6 |   250/  559 batches | lr 0.0001 | ms/batch 174.10 | loss  3.02 | ppl    20.49 | bpc    4.357\n",
            "| epoch   6 |   300/  559 batches | lr 0.0001 | ms/batch 173.86 | loss  3.02 | ppl    20.50 | bpc    4.358\n",
            "| epoch   6 |   350/  559 batches | lr 0.0001 | ms/batch 173.78 | loss  3.02 | ppl    20.49 | bpc    4.357\n",
            "| epoch   6 |   400/  559 batches | lr 0.0001 | ms/batch 174.44 | loss  3.02 | ppl    20.50 | bpc    4.357\n",
            "| epoch   6 |   450/  559 batches | lr 0.0001 | ms/batch 174.11 | loss  3.02 | ppl    20.48 | bpc    4.356\n",
            "| epoch   6 |   500/  559 batches | lr 0.0001 | ms/batch 174.20 | loss  3.02 | ppl    20.49 | bpc    4.357\n",
            "| epoch   6 |   550/  559 batches | lr 0.0001 | ms/batch 174.52 | loss  3.02 | ppl    20.44 | bpc    4.353\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 99.87s | valid loss  3.01 | valid ppl    20.28 | bpc    4.342\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  559 batches | lr 0.0001 | ms/batch 177.70 | loss  3.08 | ppl    21.78 | bpc    4.445\n",
            "| epoch   7 |   100/  559 batches | lr 0.0001 | ms/batch 174.71 | loss  3.02 | ppl    20.44 | bpc    4.353\n",
            "| epoch   7 |   150/  559 batches | lr 0.0001 | ms/batch 175.09 | loss  3.02 | ppl    20.45 | bpc    4.354\n",
            "| epoch   7 |   200/  559 batches | lr 0.0001 | ms/batch 174.21 | loss  3.02 | ppl    20.52 | bpc    4.359\n",
            "| epoch   7 |   250/  559 batches | lr 0.0001 | ms/batch 174.42 | loss  3.02 | ppl    20.44 | bpc    4.353\n",
            "| epoch   7 |   300/  559 batches | lr 0.0001 | ms/batch 174.47 | loss  3.02 | ppl    20.45 | bpc    4.354\n",
            "| epoch   7 |   350/  559 batches | lr 0.0001 | ms/batch 173.77 | loss  3.02 | ppl    20.44 | bpc    4.353\n",
            "| epoch   7 |   400/  559 batches | lr 0.0001 | ms/batch 174.20 | loss  3.02 | ppl    20.45 | bpc    4.354\n",
            "| epoch   7 |   450/  559 batches | lr 0.0001 | ms/batch 173.45 | loss  3.02 | ppl    20.43 | bpc    4.353\n",
            "| epoch   7 |   500/  559 batches | lr 0.0001 | ms/batch 175.15 | loss  3.02 | ppl    20.43 | bpc    4.353\n",
            "| epoch   7 |   550/  559 batches | lr 0.0001 | ms/batch 173.90 | loss  3.02 | ppl    20.39 | bpc    4.350\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 100.10s | valid loss  3.01 | valid ppl    20.24 | bpc    4.339\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  559 batches | lr 0.0001 | ms/batch 177.33 | loss  3.08 | ppl    21.72 | bpc    4.441\n",
            "| epoch   8 |   100/  559 batches | lr 0.0001 | ms/batch 174.10 | loss  3.02 | ppl    20.40 | bpc    4.350\n",
            "| epoch   8 |   150/  559 batches | lr 0.0001 | ms/batch 173.87 | loss  3.02 | ppl    20.42 | bpc    4.352\n",
            "| epoch   8 |   200/  559 batches | lr 0.0001 | ms/batch 173.89 | loss  3.02 | ppl    20.49 | bpc    4.357\n",
            "| epoch   8 |   250/  559 batches | lr 0.0001 | ms/batch 173.98 | loss  3.02 | ppl    20.39 | bpc    4.350\n",
            "| epoch   8 |   300/  559 batches | lr 0.0001 | ms/batch 174.46 | loss  3.02 | ppl    20.40 | bpc    4.351\n",
            "| epoch   8 |   350/  559 batches | lr 0.0001 | ms/batch 174.87 | loss  3.02 | ppl    20.41 | bpc    4.351\n",
            "| epoch   8 |   400/  559 batches | lr 0.0001 | ms/batch 175.24 | loss  3.02 | ppl    20.41 | bpc    4.351\n",
            "| epoch   8 |   450/  559 batches | lr 0.0001 | ms/batch 175.31 | loss  3.01 | ppl    20.39 | bpc    4.350\n",
            "| epoch   8 |   500/  559 batches | lr 0.0001 | ms/batch 174.52 | loss  3.02 | ppl    20.39 | bpc    4.350\n",
            "| epoch   8 |   550/  559 batches | lr 0.0001 | ms/batch 175.07 | loss  3.01 | ppl    20.35 | bpc    4.347\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 100.12s | valid loss  3.01 | valid ppl    20.21 | bpc    4.337\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  559 batches | lr 0.0001 | ms/batch 176.95 | loss  3.08 | ppl    21.69 | bpc    4.439\n",
            "| epoch   9 |   100/  559 batches | lr 0.0001 | ms/batch 174.44 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   150/  559 batches | lr 0.0001 | ms/batch 174.30 | loss  3.01 | ppl    20.38 | bpc    4.349\n",
            "| epoch   9 |   200/  559 batches | lr 0.0001 | ms/batch 173.97 | loss  3.02 | ppl    20.45 | bpc    4.354\n",
            "| epoch   9 |   250/  559 batches | lr 0.0001 | ms/batch 174.01 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   300/  559 batches | lr 0.0001 | ms/batch 173.85 | loss  3.01 | ppl    20.37 | bpc    4.348\n",
            "| epoch   9 |   350/  559 batches | lr 0.0001 | ms/batch 174.16 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   400/  559 batches | lr 0.0001 | ms/batch 174.45 | loss  3.01 | ppl    20.38 | bpc    4.349\n",
            "| epoch   9 |   450/  559 batches | lr 0.0001 | ms/batch 174.83 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   500/  559 batches | lr 0.0001 | ms/batch 174.67 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   550/  559 batches | lr 0.0001 | ms/batch 174.81 | loss  3.01 | ppl    20.32 | bpc    4.345\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 100.00s | valid loss  3.00 | valid ppl    20.18 | bpc    4.335\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  559 batches | lr 0.0001 | ms/batch 177.22 | loss  3.07 | ppl    21.65 | bpc    4.436\n",
            "| epoch  10 |   100/  559 batches | lr 0.0001 | ms/batch 173.61 | loss  3.01 | ppl    20.33 | bpc    4.345\n",
            "| epoch  10 |   150/  559 batches | lr 0.0001 | ms/batch 174.10 | loss  3.01 | ppl    20.34 | bpc    4.346\n",
            "| epoch  10 |   200/  559 batches | lr 0.0001 | ms/batch 174.18 | loss  3.02 | ppl    20.42 | bpc    4.352\n",
            "| epoch  10 |   250/  559 batches | lr 0.0001 | ms/batch 174.44 | loss  3.01 | ppl    20.33 | bpc    4.345\n",
            "| epoch  10 |   300/  559 batches | lr 0.0001 | ms/batch 174.68 | loss  3.01 | ppl    20.34 | bpc    4.346\n",
            "| epoch  10 |   350/  559 batches | lr 0.0001 | ms/batch 174.36 | loss  3.01 | ppl    20.35 | bpc    4.347\n",
            "| epoch  10 |   400/  559 batches | lr 0.0001 | ms/batch 173.96 | loss  3.01 | ppl    20.34 | bpc    4.347\n",
            "| epoch  10 |   450/  559 batches | lr 0.0001 | ms/batch 174.00 | loss  3.01 | ppl    20.33 | bpc    4.346\n",
            "| epoch  10 |   500/  559 batches | lr 0.0001 | ms/batch 174.38 | loss  3.01 | ppl    20.33 | bpc    4.345\n",
            "| epoch  10 |   550/  559 batches | lr 0.0001 | ms/batch 173.86 | loss  3.01 | ppl    20.30 | bpc    4.343\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 99.92s | valid loss  3.00 | valid ppl    20.16 | bpc    4.333\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  3.01 | test ppl    20.29 | bpc    4.343\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bptt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4NSlqBvSoVv",
        "outputId": "fd48f5a0-30fd-486b-c63d-67e24ecf3cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "interval = 50 # interval to report\n",
        "ntokens = len(corpus.dictionary) # 10000\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout)\n",
        "save = '/content/output/model_test_character_noise.pt'\n",
        "checkpoint = \"/content/output/model_test_character_none.pt\"\n",
        "\n",
        "# Load checkpoint\n",
        "if checkpoint != '':\n",
        "    model = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "\n",
        "print(model)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gPNHLTnVSLT",
        "outputId": "9897f2e7-ffd0-487e-e875-0a938a273e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(50, 256)\n",
            "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
            "  (decoder): Linear(in_features=1000, out_features=50, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.rnn.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nml5HMPKQ0G0",
        "outputId": "f053b3ff-e95e-45c0-f727-d545c317aa37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f5749274150>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import vector_to_parameters, parameters_to_vector\n",
        "lr = lr\n",
        "best_val_loss = None\n",
        "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "opts = 'SGD'\n",
        "epochs = 25\n",
        "# if opt == 'Adam':\n",
        "#     opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
        "#     lr = 0.001\n",
        "# if args.opt == 'Momentum':\n",
        "#     opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.8)\n",
        "# if args.opt == 'RMSprop':\n",
        "#     opt = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
        "#     lr = 0.001\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        model.to(device)\n",
        "\n",
        "        param_vector = parameters_to_vector(model.rnn.parameters())\n",
        "        param_vector.to(device)\n",
        "        n_params = len(param_vector)\n",
        "        noise = torch.distributions.Normal(loc=torch.tensor(0.), scale=torch.tensor(0.075)).sample_n(n_params)\n",
        "        param_vector.add_(noise.to(device))\n",
        "        \n",
        "        vector_to_parameters(param_vector, model.rnn.parameters())\n",
        "        model.to(device)\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | bpc {:8.3f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if opts == 'SGD' or opts == 'Momentum':\n",
        "                lr /= 4.0\n",
        "                for group in opt.param_groups:\n",
        "                    group['lr'] = lr\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy-oBaJnV_4E",
        "outputId": "574778e1-e6a0-4f93-fde8-a30d1df1ae40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:161: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n",
            "  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  559 batches | lr 0.0000 | ms/batch 180.71 | loss  3.15 | ppl    23.41 | bpc    4.549\n",
            "| epoch   1 |   100/  559 batches | lr 0.0000 | ms/batch 176.41 | loss  3.09 | ppl    22.00 | bpc    4.460\n",
            "| epoch   1 |   150/  559 batches | lr 0.0000 | ms/batch 175.86 | loss  3.09 | ppl    22.03 | bpc    4.462\n",
            "| epoch   1 |   200/  559 batches | lr 0.0000 | ms/batch 175.55 | loss  3.09 | ppl    22.07 | bpc    4.464\n",
            "| epoch   1 |   250/  559 batches | lr 0.0000 | ms/batch 175.15 | loss  3.09 | ppl    22.04 | bpc    4.462\n",
            "| epoch   1 |   300/  559 batches | lr 0.0000 | ms/batch 175.00 | loss  3.09 | ppl    22.01 | bpc    4.460\n",
            "| epoch   1 |   350/  559 batches | lr 0.0000 | ms/batch 174.57 | loss  3.09 | ppl    22.01 | bpc    4.460\n",
            "| epoch   1 |   400/  559 batches | lr 0.0000 | ms/batch 175.26 | loss  3.09 | ppl    22.03 | bpc    4.461\n",
            "| epoch   1 |   450/  559 batches | lr 0.0000 | ms/batch 174.58 | loss  3.09 | ppl    22.02 | bpc    4.460\n",
            "| epoch   1 |   500/  559 batches | lr 0.0000 | ms/batch 174.22 | loss  3.09 | ppl    22.03 | bpc    4.462\n",
            "| epoch   1 |   550/  559 batches | lr 0.0000 | ms/batch 174.38 | loss  3.09 | ppl    21.97 | bpc    4.458\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 100.62s | valid loss  3.07 | valid ppl    21.45 | bpc    4.423\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    50/  559 batches | lr 0.0000 | ms/batch 177.73 | loss  3.37 | ppl    28.93 | bpc    4.855\n",
            "| epoch   2 |   100/  559 batches | lr 0.0000 | ms/batch 174.63 | loss  3.30 | ppl    27.07 | bpc    4.759\n",
            "| epoch   2 |   150/  559 batches | lr 0.0000 | ms/batch 175.47 | loss  3.30 | ppl    27.13 | bpc    4.762\n",
            "| epoch   2 |   200/  559 batches | lr 0.0000 | ms/batch 174.84 | loss  3.30 | ppl    27.16 | bpc    4.763\n",
            "| epoch   2 |   250/  559 batches | lr 0.0000 | ms/batch 174.86 | loss  3.30 | ppl    27.05 | bpc    4.758\n",
            "| epoch   2 |   300/  559 batches | lr 0.0000 | ms/batch 174.56 | loss  3.30 | ppl    27.06 | bpc    4.758\n",
            "| epoch   2 |   350/  559 batches | lr 0.0000 | ms/batch 174.77 | loss  3.30 | ppl    27.07 | bpc    4.759\n",
            "| epoch   2 |   400/  559 batches | lr 0.0000 | ms/batch 174.93 | loss  3.30 | ppl    27.10 | bpc    4.760\n",
            "| epoch   2 |   450/  559 batches | lr 0.0000 | ms/batch 174.90 | loss  3.30 | ppl    27.12 | bpc    4.761\n",
            "| epoch   2 |   500/  559 batches | lr 0.0000 | ms/batch 174.22 | loss  3.30 | ppl    27.06 | bpc    4.758\n",
            "| epoch   2 |   550/  559 batches | lr 0.0000 | ms/batch 174.83 | loss  3.30 | ppl    27.06 | bpc    4.758\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 100.39s | valid loss  3.26 | valid ppl    26.03 | bpc    4.702\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  559 batches | lr 0.0000 | ms/batch 177.88 | loss  3.48 | ppl    32.34 | bpc    5.015\n",
            "| epoch   3 |   100/  559 batches | lr 0.0000 | ms/batch 174.43 | loss  3.41 | ppl    30.22 | bpc    4.917\n",
            "| epoch   3 |   150/  559 batches | lr 0.0000 | ms/batch 174.90 | loss  3.41 | ppl    30.24 | bpc    4.918\n",
            "| epoch   3 |   200/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.41 | ppl    30.30 | bpc    4.921\n",
            "| epoch   3 |   250/  559 batches | lr 0.0000 | ms/batch 175.21 | loss  3.41 | ppl    30.24 | bpc    4.918\n",
            "| epoch   3 |   300/  559 batches | lr 0.0000 | ms/batch 174.79 | loss  3.41 | ppl    30.25 | bpc    4.919\n",
            "| epoch   3 |   350/  559 batches | lr 0.0000 | ms/batch 174.86 | loss  3.41 | ppl    30.27 | bpc    4.920\n",
            "| epoch   3 |   400/  559 batches | lr 0.0000 | ms/batch 174.46 | loss  3.41 | ppl    30.29 | bpc    4.921\n",
            "| epoch   3 |   450/  559 batches | lr 0.0000 | ms/batch 174.51 | loss  3.41 | ppl    30.32 | bpc    4.922\n",
            "| epoch   3 |   500/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.41 | ppl    30.28 | bpc    4.920\n",
            "| epoch   3 |   550/  559 batches | lr 0.0000 | ms/batch 174.67 | loss  3.41 | ppl    30.27 | bpc    4.920\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 100.34s | valid loss  3.36 | valid ppl    28.73 | bpc    4.845\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  559 batches | lr 0.0000 | ms/batch 178.12 | loss  3.56 | ppl    35.06 | bpc    5.132\n",
            "| epoch   4 |   100/  559 batches | lr 0.0000 | ms/batch 174.98 | loss  3.49 | ppl    32.66 | bpc    5.029\n",
            "| epoch   4 |   150/  559 batches | lr 0.0000 | ms/batch 174.92 | loss  3.49 | ppl    32.66 | bpc    5.030\n",
            "| epoch   4 |   200/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.49 | ppl    32.70 | bpc    5.031\n",
            "| epoch   4 |   250/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.49 | ppl    32.75 | bpc    5.033\n",
            "| epoch   4 |   300/  559 batches | lr 0.0000 | ms/batch 174.88 | loss  3.49 | ppl    32.76 | bpc    5.034\n",
            "| epoch   4 |   350/  559 batches | lr 0.0000 | ms/batch 174.70 | loss  3.49 | ppl    32.72 | bpc    5.032\n",
            "| epoch   4 |   400/  559 batches | lr 0.0000 | ms/batch 175.17 | loss  3.49 | ppl    32.75 | bpc    5.033\n",
            "| epoch   4 |   450/  559 batches | lr 0.0000 | ms/batch 174.61 | loss  3.49 | ppl    32.79 | bpc    5.035\n",
            "| epoch   4 |   500/  559 batches | lr 0.0000 | ms/batch 174.44 | loss  3.49 | ppl    32.72 | bpc    5.032\n",
            "| epoch   4 |   550/  559 batches | lr 0.0000 | ms/batch 174.56 | loss  3.49 | ppl    32.69 | bpc    5.031\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 100.38s | valid loss  3.43 | valid ppl    30.97 | bpc    4.953\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  559 batches | lr 0.0000 | ms/batch 177.92 | loss  3.57 | ppl    35.64 | bpc    5.155\n",
            "| epoch   5 |   100/  559 batches | lr 0.0000 | ms/batch 175.24 | loss  3.51 | ppl    33.31 | bpc    5.058\n",
            "| epoch   5 |   150/  559 batches | lr 0.0000 | ms/batch 174.42 | loss  3.51 | ppl    33.31 | bpc    5.058\n",
            "| epoch   5 |   200/  559 batches | lr 0.0000 | ms/batch 174.92 | loss  3.51 | ppl    33.32 | bpc    5.059\n",
            "| epoch   5 |   250/  559 batches | lr 0.0000 | ms/batch 174.85 | loss  3.51 | ppl    33.31 | bpc    5.058\n",
            "| epoch   5 |   300/  559 batches | lr 0.0000 | ms/batch 174.67 | loss  3.50 | ppl    33.18 | bpc    5.052\n",
            "| epoch   5 |   350/  559 batches | lr 0.0000 | ms/batch 174.47 | loss  3.50 | ppl    33.25 | bpc    5.055\n",
            "| epoch   5 |   400/  559 batches | lr 0.0000 | ms/batch 174.52 | loss  3.51 | ppl    33.32 | bpc    5.058\n",
            "| epoch   5 |   450/  559 batches | lr 0.0000 | ms/batch 174.36 | loss  3.50 | ppl    33.27 | bpc    5.056\n",
            "| epoch   5 |   500/  559 batches | lr 0.0000 | ms/batch 174.06 | loss  3.50 | ppl    33.23 | bpc    5.055\n",
            "| epoch   5 |   550/  559 batches | lr 0.0000 | ms/batch 174.41 | loss  3.50 | ppl    33.27 | bpc    5.056\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 100.23s | valid loss  3.45 | valid ppl    31.36 | bpc    4.971\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  559 batches | lr 0.0000 | ms/batch 177.64 | loss  3.64 | ppl    38.17 | bpc    5.254\n",
            "| epoch   6 |   100/  559 batches | lr 0.0000 | ms/batch 174.81 | loss  3.57 | ppl    35.51 | bpc    5.150\n",
            "| epoch   6 |   150/  559 batches | lr 0.0000 | ms/batch 174.54 | loss  3.57 | ppl    35.62 | bpc    5.155\n",
            "| epoch   6 |   200/  559 batches | lr 0.0000 | ms/batch 173.95 | loss  3.57 | ppl    35.57 | bpc    5.152\n",
            "| epoch   6 |   250/  559 batches | lr 0.0000 | ms/batch 174.26 | loss  3.57 | ppl    35.53 | bpc    5.151\n",
            "| epoch   6 |   300/  559 batches | lr 0.0000 | ms/batch 174.61 | loss  3.57 | ppl    35.48 | bpc    5.149\n",
            "| epoch   6 |   350/  559 batches | lr 0.0000 | ms/batch 174.56 | loss  3.57 | ppl    35.57 | bpc    5.153\n",
            "| epoch   6 |   400/  559 batches | lr 0.0000 | ms/batch 174.36 | loss  3.57 | ppl    35.48 | bpc    5.149\n",
            "| epoch   6 |   450/  559 batches | lr 0.0000 | ms/batch 174.34 | loss  3.57 | ppl    35.51 | bpc    5.150\n",
            "| epoch   6 |   500/  559 batches | lr 0.0000 | ms/batch 174.83 | loss  3.57 | ppl    35.55 | bpc    5.152\n",
            "| epoch   6 |   550/  559 batches | lr 0.0000 | ms/batch 174.97 | loss  3.57 | ppl    35.56 | bpc    5.152\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 100.25s | valid loss  3.51 | valid ppl    33.35 | bpc    5.060\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  559 batches | lr 0.0000 | ms/batch 178.48 | loss  3.65 | ppl    38.31 | bpc    5.260\n",
            "| epoch   7 |   100/  559 batches | lr 0.0000 | ms/batch 175.28 | loss  3.57 | ppl    35.68 | bpc    5.157\n",
            "| epoch   7 |   150/  559 batches | lr 0.0000 | ms/batch 175.13 | loss  3.58 | ppl    35.73 | bpc    5.159\n",
            "| epoch   7 |   200/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.58 | ppl    35.76 | bpc    5.160\n",
            "| epoch   7 |   250/  559 batches | lr 0.0000 | ms/batch 175.01 | loss  3.58 | ppl    35.77 | bpc    5.161\n",
            "| epoch   7 |   300/  559 batches | lr 0.0000 | ms/batch 175.28 | loss  3.58 | ppl    35.72 | bpc    5.159\n",
            "| epoch   7 |   350/  559 batches | lr 0.0000 | ms/batch 175.40 | loss  3.58 | ppl    35.74 | bpc    5.159\n",
            "| epoch   7 |   400/  559 batches | lr 0.0000 | ms/batch 174.66 | loss  3.58 | ppl    35.74 | bpc    5.159\n",
            "| epoch   7 |   450/  559 batches | lr 0.0000 | ms/batch 175.34 | loss  3.58 | ppl    35.72 | bpc    5.159\n",
            "| epoch   7 |   500/  559 batches | lr 0.0000 | ms/batch 175.25 | loss  3.58 | ppl    35.72 | bpc    5.159\n",
            "| epoch   7 |   550/  559 batches | lr 0.0000 | ms/batch 175.03 | loss  3.58 | ppl    35.74 | bpc    5.160\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 100.59s | valid loss  3.51 | valid ppl    33.48 | bpc    5.065\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  559 batches | lr 0.0000 | ms/batch 177.65 | loss  3.66 | ppl    38.77 | bpc    5.277\n",
            "| epoch   8 |   100/  559 batches | lr 0.0000 | ms/batch 174.62 | loss  3.59 | ppl    36.19 | bpc    5.177\n",
            "| epoch   8 |   150/  559 batches | lr 0.0000 | ms/batch 174.34 | loss  3.59 | ppl    36.24 | bpc    5.180\n",
            "| epoch   8 |   200/  559 batches | lr 0.0000 | ms/batch 174.20 | loss  3.59 | ppl    36.26 | bpc    5.181\n",
            "| epoch   8 |   250/  559 batches | lr 0.0000 | ms/batch 175.10 | loss  3.59 | ppl    36.16 | bpc    5.176\n",
            "| epoch   8 |   300/  559 batches | lr 0.0000 | ms/batch 174.68 | loss  3.59 | ppl    36.23 | bpc    5.179\n",
            "| epoch   8 |   350/  559 batches | lr 0.0000 | ms/batch 174.26 | loss  3.59 | ppl    36.18 | bpc    5.177\n",
            "| epoch   8 |   400/  559 batches | lr 0.0000 | ms/batch 174.30 | loss  3.59 | ppl    36.19 | bpc    5.178\n",
            "| epoch   8 |   450/  559 batches | lr 0.0000 | ms/batch 174.75 | loss  3.59 | ppl    36.19 | bpc    5.177\n",
            "| epoch   8 |   500/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.59 | ppl    36.25 | bpc    5.180\n",
            "| epoch   8 |   550/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.59 | ppl    36.16 | bpc    5.176\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 100.23s | valid loss  3.52 | valid ppl    33.88 | bpc    5.082\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  559 batches | lr 0.0000 | ms/batch 177.82 | loss  3.67 | ppl    39.44 | bpc    5.301\n",
            "| epoch   9 |   100/  559 batches | lr 0.0000 | ms/batch 174.57 | loss  3.60 | ppl    36.63 | bpc    5.195\n",
            "| epoch   9 |   150/  559 batches | lr 0.0000 | ms/batch 174.91 | loss  3.60 | ppl    36.72 | bpc    5.199\n",
            "| epoch   9 |   200/  559 batches | lr 0.0000 | ms/batch 174.89 | loss  3.60 | ppl    36.76 | bpc    5.200\n",
            "| epoch   9 |   250/  559 batches | lr 0.0000 | ms/batch 175.02 | loss  3.60 | ppl    36.74 | bpc    5.199\n",
            "| epoch   9 |   300/  559 batches | lr 0.0000 | ms/batch 175.06 | loss  3.60 | ppl    36.72 | bpc    5.198\n",
            "| epoch   9 |   350/  559 batches | lr 0.0000 | ms/batch 174.39 | loss  3.61 | ppl    36.78 | bpc    5.201\n",
            "| epoch   9 |   400/  559 batches | lr 0.0000 | ms/batch 174.92 | loss  3.60 | ppl    36.72 | bpc    5.199\n",
            "| epoch   9 |   450/  559 batches | lr 0.0000 | ms/batch 174.79 | loss  3.60 | ppl    36.71 | bpc    5.198\n",
            "| epoch   9 |   500/  559 batches | lr 0.0000 | ms/batch 175.78 | loss  3.60 | ppl    36.72 | bpc    5.198\n",
            "| epoch   9 |   550/  559 batches | lr 0.0000 | ms/batch 175.25 | loss  3.60 | ppl    36.68 | bpc    5.197\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 100.47s | valid loss  3.54 | valid ppl    34.38 | bpc    5.103\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  559 batches | lr 0.0000 | ms/batch 177.62 | loss  3.69 | ppl    40.12 | bpc    5.326\n",
            "| epoch  10 |   100/  559 batches | lr 0.0000 | ms/batch 174.51 | loss  3.62 | ppl    37.28 | bpc    5.220\n",
            "| epoch  10 |   150/  559 batches | lr 0.0000 | ms/batch 174.64 | loss  3.62 | ppl    37.40 | bpc    5.225\n",
            "| epoch  10 |   200/  559 batches | lr 0.0000 | ms/batch 174.85 | loss  3.62 | ppl    37.39 | bpc    5.225\n",
            "| epoch  10 |   250/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.62 | ppl    37.31 | bpc    5.221\n",
            "| epoch  10 |   300/  559 batches | lr 0.0000 | ms/batch 174.71 | loss  3.62 | ppl    37.33 | bpc    5.222\n",
            "| epoch  10 |   350/  559 batches | lr 0.0000 | ms/batch 174.44 | loss  3.62 | ppl    37.33 | bpc    5.222\n",
            "| epoch  10 |   400/  559 batches | lr 0.0000 | ms/batch 174.80 | loss  3.62 | ppl    37.29 | bpc    5.221\n",
            "| epoch  10 |   450/  559 batches | lr 0.0000 | ms/batch 174.42 | loss  3.62 | ppl    37.38 | bpc    5.224\n",
            "| epoch  10 |   500/  559 batches | lr 0.0000 | ms/batch 174.38 | loss  3.62 | ppl    37.35 | bpc    5.223\n",
            "| epoch  10 |   550/  559 batches | lr 0.0000 | ms/batch 174.44 | loss  3.62 | ppl    37.37 | bpc    5.224\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 100.21s | valid loss  3.55 | valid ppl    34.93 | bpc    5.127\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |    50/  559 batches | lr 0.0000 | ms/batch 177.88 | loss  3.69 | ppl    40.16 | bpc    5.328\n",
            "| epoch  11 |   100/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.62 | ppl    37.28 | bpc    5.220\n",
            "| epoch  11 |   150/  559 batches | lr 0.0000 | ms/batch 174.40 | loss  3.62 | ppl    37.32 | bpc    5.222\n",
            "| epoch  11 |   200/  559 batches | lr 0.0000 | ms/batch 174.39 | loss  3.62 | ppl    37.39 | bpc    5.225\n",
            "| epoch  11 |   250/  559 batches | lr 0.0000 | ms/batch 174.47 | loss  3.62 | ppl    37.37 | bpc    5.224\n",
            "| epoch  11 |   300/  559 batches | lr 0.0000 | ms/batch 174.13 | loss  3.62 | ppl    37.34 | bpc    5.222\n",
            "| epoch  11 |   350/  559 batches | lr 0.0000 | ms/batch 173.95 | loss  3.62 | ppl    37.35 | bpc    5.223\n",
            "| epoch  11 |   400/  559 batches | lr 0.0000 | ms/batch 174.37 | loss  3.62 | ppl    37.37 | bpc    5.224\n",
            "| epoch  11 |   450/  559 batches | lr 0.0000 | ms/batch 174.26 | loss  3.62 | ppl    37.35 | bpc    5.223\n",
            "| epoch  11 |   500/  559 batches | lr 0.0000 | ms/batch 175.08 | loss  3.62 | ppl    37.39 | bpc    5.225\n",
            "| epoch  11 |   550/  559 batches | lr 0.0000 | ms/batch 175.17 | loss  3.62 | ppl    37.28 | bpc    5.220\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 100.20s | valid loss  3.55 | valid ppl    34.88 | bpc    5.124\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    50/  559 batches | lr 0.0000 | ms/batch 178.37 | loss  3.69 | ppl    39.87 | bpc    5.317\n",
            "| epoch  12 |   100/  559 batches | lr 0.0000 | ms/batch 174.66 | loss  3.61 | ppl    37.08 | bpc    5.213\n",
            "| epoch  12 |   150/  559 batches | lr 0.0000 | ms/batch 175.25 | loss  3.61 | ppl    37.11 | bpc    5.214\n",
            "| epoch  12 |   200/  559 batches | lr 0.0000 | ms/batch 175.11 | loss  3.61 | ppl    37.14 | bpc    5.215\n",
            "| epoch  12 |   250/  559 batches | lr 0.0000 | ms/batch 175.44 | loss  3.61 | ppl    37.06 | bpc    5.212\n",
            "| epoch  12 |   300/  559 batches | lr 0.0000 | ms/batch 175.25 | loss  3.61 | ppl    37.03 | bpc    5.211\n",
            "| epoch  12 |   350/  559 batches | lr 0.0000 | ms/batch 175.32 | loss  3.61 | ppl    37.15 | bpc    5.215\n",
            "| epoch  12 |   400/  559 batches | lr 0.0000 | ms/batch 175.80 | loss  3.62 | ppl    37.17 | bpc    5.216\n",
            "| epoch  12 |   450/  559 batches | lr 0.0000 | ms/batch 174.70 | loss  3.61 | ppl    37.11 | bpc    5.214\n",
            "| epoch  12 |   500/  559 batches | lr 0.0000 | ms/batch 174.55 | loss  3.61 | ppl    37.11 | bpc    5.214\n",
            "| epoch  12 |   550/  559 batches | lr 0.0000 | ms/batch 174.62 | loss  3.61 | ppl    37.06 | bpc    5.212\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 100.50s | valid loss  3.55 | valid ppl    34.65 | bpc    5.115\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    50/  559 batches | lr 0.0000 | ms/batch 177.94 | loss  3.69 | ppl    40.08 | bpc    5.325\n",
            "| epoch  13 |   100/  559 batches | lr 0.0000 | ms/batch 174.33 | loss  3.62 | ppl    37.29 | bpc    5.221\n",
            "| epoch  13 |   150/  559 batches | lr 0.0000 | ms/batch 174.70 | loss  3.62 | ppl    37.37 | bpc    5.224\n",
            "| epoch  13 |   200/  559 batches | lr 0.0000 | ms/batch 174.81 | loss  3.62 | ppl    37.40 | bpc    5.225\n",
            "| epoch  13 |   250/  559 batches | lr 0.0000 | ms/batch 174.67 | loss  3.62 | ppl    37.34 | bpc    5.223\n",
            "| epoch  13 |   300/  559 batches | lr 0.0000 | ms/batch 174.38 | loss  3.62 | ppl    37.31 | bpc    5.221\n",
            "| epoch  13 |   350/  559 batches | lr 0.0000 | ms/batch 174.90 | loss  3.62 | ppl    37.34 | bpc    5.222\n",
            "| epoch  13 |   400/  559 batches | lr 0.0000 | ms/batch 174.87 | loss  3.62 | ppl    37.37 | bpc    5.224\n",
            "| epoch  13 |   450/  559 batches | lr 0.0000 | ms/batch 174.64 | loss  3.62 | ppl    37.29 | bpc    5.221\n",
            "| epoch  13 |   500/  559 batches | lr 0.0000 | ms/batch 174.43 | loss  3.62 | ppl    37.25 | bpc    5.219\n",
            "| epoch  13 |   550/  559 batches | lr 0.0000 | ms/batch 174.86 | loss  3.62 | ppl    37.32 | bpc    5.222\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 100.31s | valid loss  3.55 | valid ppl    34.89 | bpc    5.125\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |    50/  559 batches | lr 0.0000 | ms/batch 178.39 | loss  3.70 | ppl    40.52 | bpc    5.340\n",
            "| epoch  14 |   100/  559 batches | lr 0.0000 | ms/batch 174.96 | loss  3.63 | ppl    37.81 | bpc    5.241\n",
            "| epoch  14 |   150/  559 batches | lr 0.0000 | ms/batch 175.23 | loss  3.63 | ppl    37.75 | bpc    5.238\n",
            "| epoch  14 |   200/  559 batches | lr 0.0000 | ms/batch 174.77 | loss  3.63 | ppl    37.76 | bpc    5.239\n",
            "| epoch  14 |   250/  559 batches | lr 0.0000 | ms/batch 175.07 | loss  3.63 | ppl    37.69 | bpc    5.236\n",
            "| epoch  14 |   300/  559 batches | lr 0.0000 | ms/batch 175.23 | loss  3.63 | ppl    37.75 | bpc    5.238\n",
            "| epoch  14 |   350/  559 batches | lr 0.0000 | ms/batch 174.75 | loss  3.63 | ppl    37.73 | bpc    5.238\n",
            "| epoch  14 |   400/  559 batches | lr 0.0000 | ms/batch 174.74 | loss  3.63 | ppl    37.68 | bpc    5.236\n",
            "| epoch  14 |   450/  559 batches | lr 0.0000 | ms/batch 174.35 | loss  3.63 | ppl    37.78 | bpc    5.240\n",
            "| epoch  14 |   500/  559 batches | lr 0.0000 | ms/batch 174.73 | loss  3.63 | ppl    37.69 | bpc    5.236\n",
            "| epoch  14 |   550/  559 batches | lr 0.0000 | ms/batch 174.14 | loss  3.63 | ppl    37.78 | bpc    5.240\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 100.34s | valid loss  3.56 | valid ppl    35.25 | bpc    5.140\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |    50/  559 batches | lr 0.0000 | ms/batch 177.98 | loss  3.71 | ppl    41.05 | bpc    5.359\n",
            "| epoch  15 |   100/  559 batches | lr 0.0000 | ms/batch 174.34 | loss  3.64 | ppl    38.20 | bpc    5.255\n",
            "| epoch  15 |   150/  559 batches | lr 0.0000 | ms/batch 174.01 | loss  3.64 | ppl    38.19 | bpc    5.255\n",
            "| epoch  15 |   200/  559 batches | lr 0.0000 | ms/batch 173.91 | loss  3.64 | ppl    38.27 | bpc    5.258\n",
            "| epoch  15 |   250/  559 batches | lr 0.0000 | ms/batch 174.41 | loss  3.64 | ppl    38.24 | bpc    5.257\n",
            "| epoch  15 |   300/  559 batches | lr 0.0000 | ms/batch 174.30 | loss  3.64 | ppl    38.25 | bpc    5.257\n",
            "| epoch  15 |   350/  559 batches | lr 0.0000 | ms/batch 174.38 | loss  3.64 | ppl    38.25 | bpc    5.257\n",
            "| epoch  15 |   400/  559 batches | lr 0.0000 | ms/batch 174.16 | loss  3.64 | ppl    38.22 | bpc    5.256\n",
            "| epoch  15 |   450/  559 batches | lr 0.0000 | ms/batch 173.99 | loss  3.64 | ppl    38.24 | bpc    5.257\n",
            "| epoch  15 |   500/  559 batches | lr 0.0000 | ms/batch 174.14 | loss  3.64 | ppl    38.19 | bpc    5.255\n",
            "| epoch  15 |   550/  559 batches | lr 0.0000 | ms/batch 175.34 | loss  3.64 | ppl    38.20 | bpc    5.255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 100.19s | valid loss  3.57 | valid ppl    35.62 | bpc    5.155\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |    50/  559 batches | lr 0.0000 | ms/batch 178.55 | loss  3.73 | ppl    41.62 | bpc    5.379\n",
            "| epoch  16 |   100/  559 batches | lr 0.0000 | ms/batch 175.53 | loss  3.66 | ppl    38.72 | bpc    5.275\n",
            "| epoch  16 |   150/  559 batches | lr 0.0000 | ms/batch 175.53 | loss  3.66 | ppl    38.77 | bpc    5.277\n",
            "| epoch  16 |   200/  559 batches | lr 0.0000 | ms/batch 175.29 | loss  3.66 | ppl    38.83 | bpc    5.279\n",
            "| epoch  16 |   250/  559 batches | lr 0.0000 | ms/batch 175.47 | loss  3.66 | ppl    38.76 | bpc    5.276\n",
            "| epoch  16 |   300/  559 batches | lr 0.0000 | ms/batch 175.61 | loss  3.66 | ppl    38.79 | bpc    5.277\n",
            "| epoch  16 |   350/  559 batches | lr 0.0000 | ms/batch 175.45 | loss  3.66 | ppl    38.76 | bpc    5.277\n",
            "| epoch  16 |   400/  559 batches | lr 0.0000 | ms/batch 174.58 | loss  3.66 | ppl    38.78 | bpc    5.277\n",
            "| epoch  16 |   450/  559 batches | lr 0.0000 | ms/batch 175.25 | loss  3.65 | ppl    38.67 | bpc    5.273\n",
            "| epoch  16 |   500/  559 batches | lr 0.0000 | ms/batch 174.98 | loss  3.66 | ppl    38.71 | bpc    5.275\n",
            "| epoch  16 |   550/  559 batches | lr 0.0000 | ms/batch 175.29 | loss  3.65 | ppl    38.66 | bpc    5.273\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 100.63s | valid loss  3.59 | valid ppl    36.14 | bpc    5.175\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |    50/  559 batches | lr 0.0000 | ms/batch 178.45 | loss  3.73 | ppl    41.70 | bpc    5.382\n",
            "| epoch  17 |   100/  559 batches | lr 0.0000 | ms/batch 175.37 | loss  3.66 | ppl    38.78 | bpc    5.277\n",
            "| epoch  17 |   150/  559 batches | lr 0.0000 | ms/batch 175.69 | loss  3.66 | ppl    38.73 | bpc    5.275\n",
            "| epoch  17 |   200/  559 batches | lr 0.0000 | ms/batch 175.19 | loss  3.66 | ppl    38.80 | bpc    5.278\n",
            "| epoch  17 |   250/  559 batches | lr 0.0000 | ms/batch 175.08 | loss  3.66 | ppl    38.74 | bpc    5.276\n",
            "| epoch  17 |   300/  559 batches | lr 0.0000 | ms/batch 175.20 | loss  3.66 | ppl    38.74 | bpc    5.276\n",
            "| epoch  17 |   350/  559 batches | lr 0.0000 | ms/batch 175.26 | loss  3.66 | ppl    38.79 | bpc    5.278\n",
            "| epoch  17 |   400/  559 batches | lr 0.0000 | ms/batch 175.65 | loss  3.66 | ppl    38.74 | bpc    5.276\n",
            "| epoch  17 |   450/  559 batches | lr 0.0000 | ms/batch 175.14 | loss  3.66 | ppl    38.80 | bpc    5.278\n",
            "| epoch  17 |   500/  559 batches | lr 0.0000 | ms/batch 175.40 | loss  3.66 | ppl    38.82 | bpc    5.279\n",
            "| epoch  17 |   550/  559 batches | lr 0.0000 | ms/batch 175.77 | loss  3.66 | ppl    38.80 | bpc    5.278\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 100.76s | valid loss  3.59 | valid ppl    36.12 | bpc    5.175\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |    50/  559 batches | lr 0.0000 | ms/batch 178.41 | loss  3.74 | ppl    42.05 | bpc    5.394\n",
            "| epoch  18 |   100/  559 batches | lr 0.0000 | ms/batch 174.65 | loss  3.67 | ppl    39.08 | bpc    5.289\n",
            "| epoch  18 |   150/  559 batches | lr 0.0000 | ms/batch 174.77 | loss  3.67 | ppl    39.06 | bpc    5.288\n",
            "| epoch  18 |   200/  559 batches | lr 0.0000 | ms/batch 175.53 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "| epoch  18 |   250/  559 batches | lr 0.0000 | ms/batch 175.43 | loss  3.66 | ppl    39.03 | bpc    5.287\n",
            "| epoch  18 |   300/  559 batches | lr 0.0000 | ms/batch 175.23 | loss  3.67 | ppl    39.14 | bpc    5.290\n",
            "| epoch  18 |   350/  559 batches | lr 0.0000 | ms/batch 175.38 | loss  3.67 | ppl    39.06 | bpc    5.288\n",
            "| epoch  18 |   400/  559 batches | lr 0.0000 | ms/batch 175.12 | loss  3.66 | ppl    39.03 | bpc    5.286\n",
            "| epoch  18 |   450/  559 batches | lr 0.0000 | ms/batch 174.91 | loss  3.66 | ppl    39.00 | bpc    5.285\n",
            "| epoch  18 |   500/  559 batches | lr 0.0000 | ms/batch 175.08 | loss  3.67 | ppl    39.11 | bpc    5.289\n",
            "| epoch  18 |   550/  559 batches | lr 0.0000 | ms/batch 175.56 | loss  3.66 | ppl    39.05 | bpc    5.287\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 100.50s | valid loss  3.60 | valid ppl    36.43 | bpc    5.187\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |    50/  559 batches | lr 0.0000 | ms/batch 177.58 | loss  3.73 | ppl    41.81 | bpc    5.386\n",
            "| epoch  19 |   100/  559 batches | lr 0.0000 | ms/batch 174.97 | loss  3.66 | ppl    38.93 | bpc    5.283\n",
            "| epoch  19 |   150/  559 batches | lr 0.0000 | ms/batch 175.30 | loss  3.66 | ppl    38.88 | bpc    5.281\n",
            "| epoch  19 |   200/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.66 | ppl    38.83 | bpc    5.279\n",
            "| epoch  19 |   250/  559 batches | lr 0.0000 | ms/batch 174.06 | loss  3.66 | ppl    38.83 | bpc    5.279\n",
            "| epoch  19 |   300/  559 batches | lr 0.0000 | ms/batch 175.20 | loss  3.66 | ppl    38.89 | bpc    5.281\n",
            "| epoch  19 |   350/  559 batches | lr 0.0000 | ms/batch 174.54 | loss  3.66 | ppl    38.89 | bpc    5.281\n",
            "| epoch  19 |   400/  559 batches | lr 0.0000 | ms/batch 174.73 | loss  3.66 | ppl    38.88 | bpc    5.281\n",
            "| epoch  19 |   450/  559 batches | lr 0.0000 | ms/batch 174.57 | loss  3.66 | ppl    38.89 | bpc    5.281\n",
            "| epoch  19 |   500/  559 batches | lr 0.0000 | ms/batch 174.18 | loss  3.66 | ppl    38.88 | bpc    5.281\n",
            "| epoch  19 |   550/  559 batches | lr 0.0000 | ms/batch 174.58 | loss  3.66 | ppl    38.87 | bpc    5.281\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 100.31s | valid loss  3.59 | valid ppl    36.20 | bpc    5.178\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |    50/  559 batches | lr 0.0000 | ms/batch 178.39 | loss  3.73 | ppl    41.78 | bpc    5.385\n",
            "| epoch  20 |   100/  559 batches | lr 0.0000 | ms/batch 175.04 | loss  3.66 | ppl    38.82 | bpc    5.279\n",
            "| epoch  20 |   150/  559 batches | lr 0.0000 | ms/batch 174.71 | loss  3.66 | ppl    38.83 | bpc    5.279\n",
            "| epoch  20 |   200/  559 batches | lr 0.0000 | ms/batch 174.04 | loss  3.66 | ppl    38.84 | bpc    5.279\n",
            "| epoch  20 |   250/  559 batches | lr 0.0000 | ms/batch 174.46 | loss  3.66 | ppl    38.80 | bpc    5.278\n",
            "| epoch  20 |   300/  559 batches | lr 0.0000 | ms/batch 174.25 | loss  3.66 | ppl    38.91 | bpc    5.282\n",
            "| epoch  20 |   350/  559 batches | lr 0.0000 | ms/batch 174.21 | loss  3.66 | ppl    38.81 | bpc    5.278\n",
            "| epoch  20 |   400/  559 batches | lr 0.0000 | ms/batch 174.53 | loss  3.66 | ppl    38.79 | bpc    5.277\n",
            "| epoch  20 |   450/  559 batches | lr 0.0000 | ms/batch 174.66 | loss  3.66 | ppl    38.83 | bpc    5.279\n",
            "| epoch  20 |   500/  559 batches | lr 0.0000 | ms/batch 173.95 | loss  3.66 | ppl    38.85 | bpc    5.280\n",
            "| epoch  20 |   550/  559 batches | lr 0.0000 | ms/batch 174.43 | loss  3.66 | ppl    38.79 | bpc    5.278\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 100.20s | valid loss  3.59 | valid ppl    36.14 | bpc    5.175\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |    50/  559 batches | lr 0.0000 | ms/batch 177.71 | loss  3.74 | ppl    42.13 | bpc    5.397\n",
            "| epoch  21 |   100/  559 batches | lr 0.0000 | ms/batch 174.12 | loss  3.67 | ppl    39.10 | bpc    5.289\n",
            "| epoch  21 |   150/  559 batches | lr 0.0000 | ms/batch 174.27 | loss  3.67 | ppl    39.13 | bpc    5.290\n",
            "| epoch  21 |   200/  559 batches | lr 0.0000 | ms/batch 174.68 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "| epoch  21 |   250/  559 batches | lr 0.0000 | ms/batch 175.13 | loss  3.67 | ppl    39.14 | bpc    5.290\n",
            "| epoch  21 |   300/  559 batches | lr 0.0000 | ms/batch 173.82 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "| epoch  21 |   350/  559 batches | lr 0.0000 | ms/batch 174.14 | loss  3.67 | ppl    39.15 | bpc    5.291\n",
            "| epoch  21 |   400/  559 batches | lr 0.0000 | ms/batch 173.33 | loss  3.66 | ppl    39.02 | bpc    5.286\n",
            "| epoch  21 |   450/  559 batches | lr 0.0000 | ms/batch 173.92 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "| epoch  21 |   500/  559 batches | lr 0.0000 | ms/batch 172.97 | loss  3.66 | ppl    39.03 | bpc    5.286\n",
            "| epoch  21 |   550/  559 batches | lr 0.0000 | ms/batch 173.00 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 99.94s | valid loss  3.59 | valid ppl    36.38 | bpc    5.185\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |    50/  559 batches | lr 0.0000 | ms/batch 176.62 | loss  3.74 | ppl    42.31 | bpc    5.403\n",
            "| epoch  22 |   100/  559 batches | lr 0.0000 | ms/batch 173.65 | loss  3.67 | ppl    39.33 | bpc    5.298\n",
            "| epoch  22 |   150/  559 batches | lr 0.0000 | ms/batch 173.42 | loss  3.67 | ppl    39.29 | bpc    5.296\n",
            "| epoch  22 |   200/  559 batches | lr 0.0000 | ms/batch 173.25 | loss  3.67 | ppl    39.35 | bpc    5.298\n",
            "| epoch  22 |   250/  559 batches | lr 0.0000 | ms/batch 173.89 | loss  3.67 | ppl    39.35 | bpc    5.298\n",
            "| epoch  22 |   300/  559 batches | lr 0.0000 | ms/batch 173.82 | loss  3.67 | ppl    39.26 | bpc    5.295\n",
            "| epoch  22 |   350/  559 batches | lr 0.0000 | ms/batch 173.59 | loss  3.67 | ppl    39.31 | bpc    5.297\n",
            "| epoch  22 |   400/  559 batches | lr 0.0000 | ms/batch 173.35 | loss  3.67 | ppl    39.31 | bpc    5.297\n",
            "| epoch  22 |   450/  559 batches | lr 0.0000 | ms/batch 172.97 | loss  3.67 | ppl    39.31 | bpc    5.297\n",
            "| epoch  22 |   500/  559 batches | lr 0.0000 | ms/batch 173.43 | loss  3.67 | ppl    39.29 | bpc    5.296\n",
            "| epoch  22 |   550/  559 batches | lr 0.0000 | ms/batch 173.65 | loss  3.67 | ppl    39.36 | bpc    5.299\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 99.63s | valid loss  3.60 | valid ppl    36.65 | bpc    5.196\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |    50/  559 batches | lr 0.0000 | ms/batch 176.73 | loss  3.74 | ppl    42.03 | bpc    5.393\n",
            "| epoch  23 |   100/  559 batches | lr 0.0000 | ms/batch 173.57 | loss  3.66 | ppl    39.00 | bpc    5.286\n",
            "| epoch  23 |   150/  559 batches | lr 0.0000 | ms/batch 173.18 | loss  3.66 | ppl    39.04 | bpc    5.287\n",
            "| epoch  23 |   200/  559 batches | lr 0.0000 | ms/batch 173.78 | loss  3.67 | ppl    39.06 | bpc    5.288\n",
            "| epoch  23 |   250/  559 batches | lr 0.0000 | ms/batch 172.84 | loss  3.66 | ppl    39.05 | bpc    5.287\n",
            "| epoch  23 |   300/  559 batches | lr 0.0000 | ms/batch 173.53 | loss  3.67 | ppl    39.07 | bpc    5.288\n",
            "| epoch  23 |   350/  559 batches | lr 0.0000 | ms/batch 173.74 | loss  3.67 | ppl    39.06 | bpc    5.288\n",
            "| epoch  23 |   400/  559 batches | lr 0.0000 | ms/batch 172.90 | loss  3.66 | ppl    39.01 | bpc    5.286\n",
            "| epoch  23 |   450/  559 batches | lr 0.0000 | ms/batch 173.23 | loss  3.66 | ppl    39.01 | bpc    5.286\n",
            "| epoch  23 |   500/  559 batches | lr 0.0000 | ms/batch 173.71 | loss  3.67 | ppl    39.09 | bpc    5.289\n",
            "| epoch  23 |   550/  559 batches | lr 0.0000 | ms/batch 173.14 | loss  3.66 | ppl    39.00 | bpc    5.285\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 99.58s | valid loss  3.59 | valid ppl    36.36 | bpc    5.184\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |    50/  559 batches | lr 0.0000 | ms/batch 176.42 | loss  3.74 | ppl    42.27 | bpc    5.402\n",
            "| epoch  24 |   100/  559 batches | lr 0.0000 | ms/batch 173.14 | loss  3.67 | ppl    39.25 | bpc    5.295\n",
            "| epoch  24 |   150/  559 batches | lr 0.0000 | ms/batch 173.34 | loss  3.67 | ppl    39.27 | bpc    5.296\n",
            "| epoch  24 |   200/  559 batches | lr 0.0000 | ms/batch 172.81 | loss  3.67 | ppl    39.25 | bpc    5.295\n",
            "| epoch  24 |   250/  559 batches | lr 0.0000 | ms/batch 173.49 | loss  3.67 | ppl    39.23 | bpc    5.294\n",
            "| epoch  24 |   300/  559 batches | lr 0.0000 | ms/batch 173.25 | loss  3.67 | ppl    39.23 | bpc    5.294\n",
            "| epoch  24 |   350/  559 batches | lr 0.0000 | ms/batch 173.86 | loss  3.67 | ppl    39.21 | bpc    5.293\n",
            "| epoch  24 |   400/  559 batches | lr 0.0000 | ms/batch 174.10 | loss  3.67 | ppl    39.22 | bpc    5.293\n",
            "| epoch  24 |   450/  559 batches | lr 0.0000 | ms/batch 174.48 | loss  3.67 | ppl    39.25 | bpc    5.295\n",
            "| epoch  24 |   500/  559 batches | lr 0.0000 | ms/batch 174.13 | loss  3.67 | ppl    39.25 | bpc    5.295\n",
            "| epoch  24 |   550/  559 batches | lr 0.0000 | ms/batch 174.28 | loss  3.67 | ppl    39.22 | bpc    5.293\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 99.74s | valid loss  3.60 | valid ppl    36.53 | bpc    5.191\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |    50/  559 batches | lr 0.0000 | ms/batch 177.11 | loss  3.75 | ppl    42.55 | bpc    5.411\n",
            "| epoch  25 |   100/  559 batches | lr 0.0000 | ms/batch 173.78 | loss  3.68 | ppl    39.52 | bpc    5.305\n",
            "| epoch  25 |   150/  559 batches | lr 0.0000 | ms/batch 173.92 | loss  3.68 | ppl    39.51 | bpc    5.304\n",
            "| epoch  25 |   200/  559 batches | lr 0.0000 | ms/batch 174.14 | loss  3.68 | ppl    39.57 | bpc    5.306\n",
            "| epoch  25 |   250/  559 batches | lr 0.0000 | ms/batch 173.98 | loss  3.68 | ppl    39.54 | bpc    5.305\n",
            "| epoch  25 |   300/  559 batches | lr 0.0000 | ms/batch 174.05 | loss  3.67 | ppl    39.43 | bpc    5.301\n",
            "| epoch  25 |   350/  559 batches | lr 0.0000 | ms/batch 174.11 | loss  3.68 | ppl    39.51 | bpc    5.304\n",
            "| epoch  25 |   400/  559 batches | lr 0.0000 | ms/batch 174.05 | loss  3.68 | ppl    39.49 | bpc    5.304\n",
            "| epoch  25 |   450/  559 batches | lr 0.0000 | ms/batch 173.95 | loss  3.68 | ppl    39.47 | bpc    5.303\n",
            "| epoch  25 |   500/  559 batches | lr 0.0000 | ms/batch 173.55 | loss  3.68 | ppl    39.52 | bpc    5.304\n",
            "| epoch  25 |   550/  559 batches | lr 0.0000 | ms/batch 173.94 | loss  3.67 | ppl    39.43 | bpc    5.301\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 99.90s | valid loss  3.60 | valid ppl    36.74 | bpc    5.199\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  3.07 | test ppl    21.50 | bpc    4.426\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-NfcUZZDpPii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build the model\n",
        "interval = 50 # interval to report\n",
        "ntokens = len(corpus.dictionary) # 10000\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout)\n",
        "save = '/content/output/model_test_character_adaptive.pt'\n",
        "checkpoint = \"/content/output/model_test_character_noise.pt\"\n",
        "\n",
        "# Load checkpoint\n",
        "if checkpoint != '':\n",
        "    model = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "\n",
        "print(model)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-k5ABGNob_x",
        "outputId": "0a465905-2366-465b-8099-5987d4e7b1cb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(50, 256)\n",
            "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
            "  (decoder): Linear(in_features=1000, out_features=50, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # choose a optimizer\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    # train_data size(batchcnt, bsz)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        # print(hidden.to(device))\n",
        "        output, hidden = model(data.to(device), hidden)\n",
        "        loss = criterion(output.to(device), targets.to(device))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_value_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        l2_lambda = 0.01\n",
        "        l2_reg = torch.tensor(0.).to(device)\n",
        "        for param in model.rnn.parameters():\n",
        "            l2_reg += torch.norm(param.to(device))\n",
        "\n",
        "        total_loss += loss.data\n",
        "        total_loss += l2_lambda * l2_reg\n",
        "\n",
        "        if batch % interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                elapsed * 1000 / interval, cur_loss, math.exp(cur_loss), cur_loss / math.log(2)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n"
      ],
      "metadata": {
        "id": "dW2e51KCAm1Q"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import vector_to_parameters, parameters_to_vector\n",
        "lr = lr\n",
        "best_val_loss = None\n",
        "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "opts = 'SGD'\n",
        "# if opt == 'Adam':\n",
        "#     opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
        "#     lr = 0.001\n",
        "# if args.opt == 'Momentum':\n",
        "#     opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.8)\n",
        "# if args.opt == 'RMSprop':\n",
        "#     opt = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
        "#     lr = 0.001\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        model.to(device)\n",
        "\n",
        "        param_vector = parameters_to_vector(model.rnn.parameters())\n",
        "        param_vector.to(device)\n",
        "        n_params = len(param_vector)\n",
        "        noise = torch.distributions.Normal(loc=torch.tensor(0.), scale=torch.tensor(0.075)).sample_n(n_params)\n",
        "        param_vector.add_(noise.to(device))\n",
        "        \n",
        "        vector_to_parameters(param_vector, model.rnn.parameters())\n",
        "        model.to(device)\n",
        "        \n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | bpc {:8.3f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if opts == 'SGD' or opts == 'Momentum':\n",
        "                lr /= 4.0\n",
        "                for group in opt.param_groups:\n",
        "                    group['lr'] = lr\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AamuIRSowPk",
        "outputId": "4e7dd938-4897-405e-f340-90ef36a281f8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:161: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n",
            "  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  559 batches | lr 0.0001 | ms/batch 175.09 | loss  8.39 | ppl  4399.20 | bpc   12.103\n",
            "| epoch   1 |   100/  559 batches | lr 0.0001 | ms/batch 170.32 | loss  8.19 | ppl  3598.25 | bpc   11.813\n",
            "| epoch   1 |   150/  559 batches | lr 0.0001 | ms/batch 169.06 | loss  8.13 | ppl  3395.52 | bpc   11.729\n",
            "| epoch   1 |   200/  559 batches | lr 0.0001 | ms/batch 169.75 | loss  8.07 | ppl  3199.12 | bpc   11.643\n",
            "| epoch   1 |   250/  559 batches | lr 0.0001 | ms/batch 169.55 | loss  8.02 | ppl  3030.41 | bpc   11.565\n",
            "| epoch   1 |   300/  559 batches | lr 0.0001 | ms/batch 169.52 | loss  7.98 | ppl  2920.70 | bpc   11.512\n",
            "| epoch   1 |   350/  559 batches | lr 0.0001 | ms/batch 168.80 | loss  7.95 | ppl  2826.23 | bpc   11.465\n",
            "| epoch   1 |   400/  559 batches | lr 0.0001 | ms/batch 169.23 | loss  7.92 | ppl  2756.35 | bpc   11.429\n",
            "| epoch   1 |   450/  559 batches | lr 0.0001 | ms/batch 168.68 | loss  7.90 | ppl  2700.34 | bpc   11.399\n",
            "| epoch   1 |   500/  559 batches | lr 0.0001 | ms/batch 167.96 | loss  7.89 | ppl  2664.62 | bpc   11.380\n",
            "| epoch   1 |   550/  559 batches | lr 0.0001 | ms/batch 169.09 | loss  7.88 | ppl  2632.35 | bpc   11.362\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 97.29s | valid loss  3.08 | valid ppl    21.76 | bpc    4.444\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    50/  559 batches | lr 0.0001 | ms/batch 172.78 | loss  8.69 | ppl  5949.84 | bpc   12.539\n",
            "| epoch   2 |   100/  559 batches | lr 0.0001 | ms/batch 168.08 | loss  8.50 | ppl  4904.44 | bpc   12.260\n",
            "| epoch   2 |   150/  559 batches | lr 0.0001 | ms/batch 168.16 | loss  8.48 | ppl  4802.81 | bpc   12.230\n",
            "| epoch   2 |   200/  559 batches | lr 0.0001 | ms/batch 169.30 | loss  8.46 | ppl  4731.57 | bpc   12.208\n",
            "| epoch   2 |   250/  559 batches | lr 0.0001 | ms/batch 168.59 | loss  8.44 | ppl  4640.70 | bpc   12.180\n",
            "| epoch   2 |   300/  559 batches | lr 0.0001 | ms/batch 168.60 | loss  8.43 | ppl  4596.77 | bpc   12.166\n",
            "| epoch   2 |   350/  559 batches | lr 0.0001 | ms/batch 169.71 | loss  8.42 | ppl  4557.32 | bpc   12.154\n",
            "| epoch   2 |   400/  559 batches | lr 0.0001 | ms/batch 168.28 | loss  8.42 | ppl  4529.57 | bpc   12.145\n",
            "| epoch   2 |   450/  559 batches | lr 0.0001 | ms/batch 169.44 | loss  8.41 | ppl  4495.01 | bpc   12.134\n",
            "| epoch   2 |   500/  559 batches | lr 0.0001 | ms/batch 169.17 | loss  8.40 | ppl  4467.46 | bpc   12.125\n",
            "| epoch   2 |   550/  559 batches | lr 0.0001 | ms/batch 167.69 | loss  8.40 | ppl  4449.22 | bpc   12.119\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 96.88s | valid loss  3.06 | valid ppl    21.35 | bpc    4.416\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  559 batches | lr 0.0001 | ms/batch 171.79 | loss  9.14 | ppl  9352.66 | bpc   13.191\n",
            "| epoch   3 |   100/  559 batches | lr 0.0001 | ms/batch 168.68 | loss  8.95 | ppl  7727.93 | bpc   12.916\n",
            "| epoch   3 |   150/  559 batches | lr 0.0001 | ms/batch 168.43 | loss  8.94 | ppl  7651.05 | bpc   12.901\n",
            "| epoch   3 |   200/  559 batches | lr 0.0001 | ms/batch 169.57 | loss  8.94 | ppl  7601.86 | bpc   12.892\n",
            "| epoch   3 |   250/  559 batches | lr 0.0001 | ms/batch 168.33 | loss  8.92 | ppl  7504.73 | bpc   12.874\n",
            "| epoch   3 |   300/  559 batches | lr 0.0001 | ms/batch 169.85 | loss  8.92 | ppl  7473.55 | bpc   12.868\n",
            "| epoch   3 |   350/  559 batches | lr 0.0001 | ms/batch 168.68 | loss  8.91 | ppl  7440.21 | bpc   12.861\n",
            "| epoch   3 |   400/  559 batches | lr 0.0001 | ms/batch 169.29 | loss  8.91 | ppl  7411.62 | bpc   12.856\n",
            "| epoch   3 |   450/  559 batches | lr 0.0001 | ms/batch 169.18 | loss  8.90 | ppl  7367.03 | bpc   12.847\n",
            "| epoch   3 |   500/  559 batches | lr 0.0001 | ms/batch 168.67 | loss  8.90 | ppl  7344.07 | bpc   12.842\n",
            "| epoch   3 |   550/  559 batches | lr 0.0001 | ms/batch 169.35 | loss  8.90 | ppl  7318.25 | bpc   12.837\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 97.00s | valid loss  3.06 | valid ppl    21.27 | bpc    4.411\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  559 batches | lr 0.0001 | ms/batch 173.16 | loss  9.60 | ppl 14825.90 | bpc   13.856\n",
            "| epoch   4 |   100/  559 batches | lr 0.0001 | ms/batch 168.93 | loss  9.40 | ppl 12142.48 | bpc   13.568\n",
            "| epoch   4 |   150/  559 batches | lr 0.0001 | ms/batch 169.04 | loss  9.40 | ppl 12075.79 | bpc   13.560\n",
            "| epoch   4 |   200/  559 batches | lr 0.0001 | ms/batch 168.41 | loss  9.39 | ppl 12023.58 | bpc   13.554\n",
            "| epoch   4 |   250/  559 batches | lr 0.0001 | ms/batch 169.24 | loss  9.38 | ppl 11841.12 | bpc   13.532\n",
            "| epoch   4 |   300/  559 batches | lr 0.0001 | ms/batch 168.69 | loss  9.37 | ppl 11743.53 | bpc   13.520\n",
            "| epoch   4 |   350/  559 batches | lr 0.0001 | ms/batch 168.81 | loss  9.36 | ppl 11654.79 | bpc   13.509\n",
            "| epoch   4 |   400/  559 batches | lr 0.0001 | ms/batch 169.36 | loss  9.36 | ppl 11611.99 | bpc   13.503\n",
            "| epoch   4 |   450/  559 batches | lr 0.0001 | ms/batch 168.68 | loss  9.36 | ppl 11568.00 | bpc   13.498\n",
            "| epoch   4 |   500/  559 batches | lr 0.0001 | ms/batch 167.71 | loss  9.35 | ppl 11538.93 | bpc   13.494\n",
            "| epoch   4 |   550/  559 batches | lr 0.0001 | ms/batch 170.07 | loss  9.35 | ppl 11489.92 | bpc   13.488\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 97.03s | valid loss  3.05 | valid ppl    21.12 | bpc    4.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  559 batches | lr 0.0001 | ms/batch 172.88 | loss 10.03 | ppl 22609.70 | bpc   14.465\n",
            "| epoch   5 |   100/  559 batches | lr 0.0001 | ms/batch 169.99 | loss  9.82 | ppl 18344.05 | bpc   14.163\n",
            "| epoch   5 |   150/  559 batches | lr 0.0001 | ms/batch 169.06 | loss  9.81 | ppl 18261.42 | bpc   14.157\n",
            "| epoch   5 |   200/  559 batches | lr 0.0001 | ms/batch 170.16 | loss  9.81 | ppl 18298.95 | bpc   14.159\n",
            "| epoch   5 |   250/  559 batches | lr 0.0001 | ms/batch 169.72 | loss  9.81 | ppl 18218.52 | bpc   14.153\n",
            "| epoch   5 |   300/  559 batches | lr 0.0001 | ms/batch 169.52 | loss  9.80 | ppl 18100.52 | bpc   14.144\n",
            "| epoch   5 |   350/  559 batches | lr 0.0001 | ms/batch 170.17 | loss  9.80 | ppl 17948.89 | bpc   14.132\n",
            "| epoch   5 |   400/  559 batches | lr 0.0001 | ms/batch 169.22 | loss  9.79 | ppl 17841.06 | bpc   14.123\n",
            "| epoch   5 |   450/  559 batches | lr 0.0001 | ms/batch 169.71 | loss  9.78 | ppl 17751.22 | bpc   14.116\n",
            "| epoch   5 |   500/  559 batches | lr 0.0001 | ms/batch 169.18 | loss  9.78 | ppl 17746.70 | bpc   14.115\n",
            "| epoch   5 |   550/  559 batches | lr 0.0001 | ms/batch 169.49 | loss  9.78 | ppl 17663.08 | bpc   14.108\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 97.36s | valid loss  3.05 | valid ppl    21.14 | bpc    4.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  559 batches | lr 0.0000 | ms/batch 171.07 | loss 10.41 | ppl 33286.39 | bpc   15.023\n",
            "| epoch   6 |   100/  559 batches | lr 0.0000 | ms/batch 169.42 | loss 10.21 | ppl 27051.47 | bpc   14.723\n",
            "| epoch   6 |   150/  559 batches | lr 0.0000 | ms/batch 168.60 | loss 10.21 | ppl 27097.87 | bpc   14.726\n",
            "| epoch   6 |   200/  559 batches | lr 0.0000 | ms/batch 169.41 | loss 10.21 | ppl 27146.27 | bpc   14.728\n",
            "| epoch   6 |   250/  559 batches | lr 0.0000 | ms/batch 168.74 | loss 10.21 | ppl 27052.35 | bpc   14.723\n",
            "| epoch   6 |   300/  559 batches | lr 0.0000 | ms/batch 168.40 | loss 10.21 | ppl 27106.87 | bpc   14.726\n",
            "| epoch   6 |   350/  559 batches | lr 0.0000 | ms/batch 169.32 | loss 10.20 | ppl 27006.52 | bpc   14.721\n",
            "| epoch   6 |   400/  559 batches | lr 0.0000 | ms/batch 168.52 | loss 10.20 | ppl 26846.15 | bpc   14.712\n",
            "| epoch   6 |   450/  559 batches | lr 0.0000 | ms/batch 167.93 | loss 10.19 | ppl 26680.81 | bpc   14.704\n",
            "| epoch   6 |   500/  559 batches | lr 0.0000 | ms/batch 169.07 | loss 10.19 | ppl 26618.24 | bpc   14.700\n",
            "| epoch   6 |   550/  559 batches | lr 0.0000 | ms/batch 169.71 | loss 10.18 | ppl 26497.25 | bpc   14.694\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 96.90s | valid loss  3.06 | valid ppl    21.24 | bpc    4.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  559 batches | lr 0.0000 | ms/batch 172.96 | loss 10.81 | ppl 49611.05 | bpc   15.598\n",
            "| epoch   7 |   100/  559 batches | lr 0.0000 | ms/batch 169.01 | loss 10.60 | ppl 40020.65 | bpc   15.288\n",
            "| epoch   7 |   150/  559 batches | lr 0.0000 | ms/batch 169.66 | loss 10.60 | ppl 40036.41 | bpc   15.289\n",
            "| epoch   7 |   200/  559 batches | lr 0.0000 | ms/batch 168.33 | loss 10.60 | ppl 40187.09 | bpc   15.294\n",
            "| epoch   7 |   250/  559 batches | lr 0.0000 | ms/batch 169.92 | loss 10.60 | ppl 40037.44 | bpc   15.289\n",
            "| epoch   7 |   300/  559 batches | lr 0.0000 | ms/batch 169.06 | loss 10.60 | ppl 40024.31 | bpc   15.289\n",
            "| epoch   7 |   350/  559 batches | lr 0.0000 | ms/batch 169.94 | loss 10.60 | ppl 40004.81 | bpc   15.288\n",
            "| epoch   7 |   400/  559 batches | lr 0.0000 | ms/batch 168.95 | loss 10.59 | ppl 39916.28 | bpc   15.285\n",
            "| epoch   7 |   450/  559 batches | lr 0.0000 | ms/batch 169.85 | loss 10.59 | ppl 39873.82 | bpc   15.283\n",
            "| epoch   7 |   500/  559 batches | lr 0.0000 | ms/batch 168.63 | loss 10.59 | ppl 39839.58 | bpc   15.282\n",
            "| epoch   7 |   550/  559 batches | lr 0.0000 | ms/batch 169.68 | loss 10.59 | ppl 39796.40 | bpc   15.280\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 97.19s | valid loss  3.08 | valid ppl    21.74 | bpc    4.443\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  559 batches | lr 0.0000 | ms/batch 171.31 | loss 11.19 | ppl 72151.16 | bpc   16.139\n",
            "| epoch   8 |   100/  559 batches | lr 0.0000 | ms/batch 169.39 | loss 10.97 | ppl 57900.88 | bpc   15.821\n",
            "| epoch   8 |   150/  559 batches | lr 0.0000 | ms/batch 168.90 | loss 10.97 | ppl 57944.58 | bpc   15.822\n",
            "| epoch   8 |   200/  559 batches | lr 0.0000 | ms/batch 168.57 | loss 10.97 | ppl 57952.26 | bpc   15.823\n",
            "| epoch   8 |   250/  559 batches | lr 0.0000 | ms/batch 169.44 | loss 10.96 | ppl 57767.85 | bpc   15.818\n",
            "| epoch   8 |   300/  559 batches | lr 0.0000 | ms/batch 168.46 | loss 10.96 | ppl 57798.76 | bpc   15.819\n",
            "| epoch   8 |   350/  559 batches | lr 0.0000 | ms/batch 169.36 | loss 10.96 | ppl 57781.68 | bpc   15.818\n",
            "| epoch   8 |   400/  559 batches | lr 0.0000 | ms/batch 169.09 | loss 10.96 | ppl 57807.91 | bpc   15.819\n",
            "| epoch   8 |   450/  559 batches | lr 0.0000 | ms/batch 169.17 | loss 10.96 | ppl 57802.29 | bpc   15.819\n",
            "| epoch   8 |   500/  559 batches | lr 0.0000 | ms/batch 169.00 | loss 10.96 | ppl 57795.23 | bpc   15.819\n",
            "| epoch   8 |   550/  559 batches | lr 0.0000 | ms/batch 168.27 | loss 10.96 | ppl 57730.78 | bpc   15.817\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 96.95s | valid loss  3.09 | valid ppl    22.02 | bpc    4.461\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  559 batches | lr 0.0000 | ms/batch 172.31 | loss 11.54 | ppl 103223.62 | bpc   16.655\n",
            "| epoch   9 |   100/  559 batches | lr 0.0000 | ms/batch 168.75 | loss 11.32 | ppl 82261.26 | bpc   16.328\n",
            "| epoch   9 |   150/  559 batches | lr 0.0000 | ms/batch 168.42 | loss 11.32 | ppl 82443.23 | bpc   16.331\n",
            "| epoch   9 |   200/  559 batches | lr 0.0000 | ms/batch 169.43 | loss 11.32 | ppl 82567.94 | bpc   16.333\n",
            "| epoch   9 |   250/  559 batches | lr 0.0000 | ms/batch 168.39 | loss 11.32 | ppl 82262.59 | bpc   16.328\n",
            "| epoch   9 |   300/  559 batches | lr 0.0000 | ms/batch 168.07 | loss 11.32 | ppl 82384.60 | bpc   16.330\n",
            "| epoch   9 |   350/  559 batches | lr 0.0000 | ms/batch 169.02 | loss 11.32 | ppl 82218.91 | bpc   16.327\n",
            "| epoch   9 |   400/  559 batches | lr 0.0000 | ms/batch 168.66 | loss 11.32 | ppl 82289.74 | bpc   16.328\n",
            "| epoch   9 |   450/  559 batches | lr 0.0000 | ms/batch 168.72 | loss 11.32 | ppl 82234.51 | bpc   16.327\n",
            "| epoch   9 |   500/  559 batches | lr 0.0000 | ms/batch 169.17 | loss 11.32 | ppl 82381.06 | bpc   16.330\n",
            "| epoch   9 |   550/  559 batches | lr 0.0000 | ms/batch 168.60 | loss 11.32 | ppl 82274.13 | bpc   16.328\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 96.88s | valid loss  3.10 | valid ppl    22.21 | bpc    4.473\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  559 batches | lr 0.0000 | ms/batch 172.46 | loss 11.92 | ppl 150011.97 | bpc   17.195\n",
            "| epoch  10 |   100/  559 batches | lr 0.0000 | ms/batch 168.87 | loss 11.68 | ppl 118773.51 | bpc   16.858\n",
            "| epoch  10 |   150/  559 batches | lr 0.0000 | ms/batch 167.77 | loss 11.69 | ppl 118787.79 | bpc   16.858\n",
            "| epoch  10 |   200/  559 batches | lr 0.0000 | ms/batch 167.77 | loss 11.69 | ppl 119134.83 | bpc   16.862\n",
            "| epoch  10 |   250/  559 batches | lr 0.0000 | ms/batch 168.10 | loss 11.68 | ppl 118583.94 | bpc   16.856\n",
            "| epoch  10 |   300/  559 batches | lr 0.0000 | ms/batch 168.94 | loss 11.68 | ppl 118730.71 | bpc   16.857\n",
            "| epoch  10 |   350/  559 batches | lr 0.0000 | ms/batch 168.58 | loss 11.68 | ppl 118510.00 | bpc   16.855\n",
            "| epoch  10 |   400/  559 batches | lr 0.0000 | ms/batch 168.29 | loss 11.68 | ppl 118738.07 | bpc   16.857\n",
            "| epoch  10 |   450/  559 batches | lr 0.0000 | ms/batch 168.60 | loss 11.68 | ppl 118766.04 | bpc   16.858\n",
            "| epoch  10 |   500/  559 batches | lr 0.0000 | ms/batch 169.07 | loss 11.68 | ppl 118712.59 | bpc   16.857\n",
            "| epoch  10 |   550/  559 batches | lr 0.0000 | ms/batch 168.38 | loss 11.68 | ppl 118610.97 | bpc   16.856\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 96.78s | valid loss  3.13 | valid ppl    22.89 | bpc    4.516\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  3.06 | test ppl    21.28 | bpc    4.412\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0rhqYahOFUZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mWYUPwb_FUdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = '/content/data/penn'\n",
        "batch_size = 64\n",
        "emsize = 256\n",
        "nlayers = 1\n",
        "nhid = 1000\n",
        "lr = 0.0001\n",
        "dropout = 0.5\n",
        "checkpoint = ''\n",
        "clip = 1\n",
        "bptt = 35\n",
        "epochs = 10\n",
        "save = '/content/output/model_test_word_none.pt'\n",
        "\n",
        "torch.manual_seed(1111)\n",
        "\n",
        "# Load data\n",
        "corpus = Corpus(data)\n"
      ],
      "metadata": {
        "id": "cIVqLt5OFlnN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data"
      ],
      "metadata": {
        "id": "z5VSLOTdFlnN"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_batch_size = 64\n",
        "train_data = batchify(corpus.train, batch_size) # size(total_len//bsz, bsz)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ],
      "metadata": {
        "id": "nIhXbQwwFlnN"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d488e02-a5cd-453c-e493-32719c90ce04",
        "id": "0i2Jjp0zFlnN"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aaa7218-7307-419d-8893-e6af2ce293d7",
        "id": "jFfBvbfdFlnO"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0,  988,   48,  ...,   32, 3490,  556],\n",
              "        [   1,   40,   32,  ..., 6789,  119,   27],\n",
              "        [   2, 2756,  189,  ..., 1168,  129, 1880],\n",
              "        ...,\n",
              "        [1825,   54,   32,  ...,  416,   26,   35],\n",
              "        [  35, 3940, 2361,  ...,   27,  373,  198],\n",
              "        [ 101, 1305, 4923,  ...,   24,   42,   42]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data.to(device)\n",
        "test_data.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a7cf7c-f87f-49b0-acc8-f704e8abf216",
        "id": "zP8AXSokFlnO"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 142,  712,  439,  ..., 1940,   64, 3981],\n",
              "        [  78, 4480,   48,  ...,   64, 4500,  500],\n",
              "        [  54,  556,   40,  ...,  872,  398,   32],\n",
              "        ...,\n",
              "        [ 555,   64, 2380,  ...,  801,   32,   26],\n",
              "        [1319,   26,  301,  ..., 2030, 6851,   64],\n",
              "        [ 410,  119,   32,  ...,  159,  548,  220]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "interval = 50 # interval to report\n",
        "ntokens = len(corpus.dictionary) # 10000\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout)\n",
        "\n",
        "# Load checkpoint\n",
        "if checkpoint != '':\n",
        "    model = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "\n",
        "print(model)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca558a66-1b81-45df-ea71-4171cd4c0696",
        "id": "WOWwV7sdFlnO"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(10000, 256)\n",
            "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
            "  (decoder): Linear(in_features=1000, out_features=10000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f364522-52db-46f8-dbaa-9b74156395e0",
        "id": "UfrpT7ooFlnO"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (drop): Dropout(p=0.5, inplace=False)\n",
              "  (encoder): Embedding(10000, 256)\n",
              "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
              "  (decoder): Linear(in_features=1000, out_features=10000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    # detach\n",
        "    return tuple(v.clone().detach() for v in h)\n"
      ],
      "metadata": {
        "id": "mzQDckWKFlnO"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_batch(source, i):\n",
        "    # source: size(total_len//bsz, bsz)\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    #data = torch.tensor(source[i:i+seq_len]) # size(bptt, bsz)\n",
        "    data = source[i:i+seq_len].clone().detach()\n",
        "    target = source[i+1:i+1+seq_len].clone().detach().view(-1)\n",
        "    #target = torch.tensor(source[i+1:i+1+seq_len].view(-1)) # size(bptt * bsz)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "89PfPgfPFlnP"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        ntokens = len(corpus.dictionary)\n",
        "        hidden = model.init_hidden(eval_batch_size) #hidden size(nlayers, bsz, hdsize)\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):# iterate over every timestep\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data.to(device), hidden)\n",
        "            # model input and output\n",
        "            # inputdata size(bptt, bsz), and size(bptt, bsz, embsize) after embedding\n",
        "            # output size(bptt*bsz, ntoken)\n",
        "            total_loss += len(data) * criterion(output.to(device), targets.to(device)).data\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        return total_loss / len(data_source)\n"
      ],
      "metadata": {
        "id": "PEdnvP0WFlnP"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # choose a optimizer\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    # train_data size(batchcnt, bsz)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        # print(hidden.to(device))\n",
        "        output, hidden = model(data.to(device), hidden)\n",
        "        loss = criterion(output.to(device), targets.to(device))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_value_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                elapsed * 1000 / interval, cur_loss, math.exp(cur_loss), cur_loss / math.log(2)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n"
      ],
      "metadata": {
        "id": "DWo320JXFlnP"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of tokens:\")\n",
        "print(\"Train: \", len(corpus.train))\n",
        "print(\"Valid: \", len(corpus.valid))\n",
        "print(\"Test:  \", len(corpus.test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78778ff9-f9f0-43ec-f33c-124033326c93",
        "id": "KqWZrhvQFlnP"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens:\n",
            "Train:  929589\n",
            "Valid:  73760\n",
            "Test:   82430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = lr\n",
        "best_val_loss = None\n",
        "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "opts = 'SGD'\n",
        "# if opt == 'Adam':\n",
        "#     opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
        "#     lr = 0.001\n",
        "# if args.opt == 'Momentum':\n",
        "#     opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.8)\n",
        "# if args.opt == 'RMSprop':\n",
        "#     opt = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
        "#     lr = 0.001\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | bpc {:8.3f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if opts == 'SGD' or opts == 'Momentum':\n",
        "                lr /= 4.0\n",
        "                for group in opt.param_groups:\n",
        "                    group['lr'] = lr\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b6d63e-28e0-4eca-ed5d-0adfce46e368",
        "id": "hb7POEtPFlnP"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  414 batches | lr 0.0001 | ms/batch 135.47 | loss  9.40 | ppl 12033.63 | bpc   13.555\n",
            "| epoch   1 |   100/  414 batches | lr 0.0001 | ms/batch 133.15 | loss  9.21 | ppl  9968.75 | bpc   13.283\n",
            "| epoch   1 |   150/  414 batches | lr 0.0001 | ms/batch 132.97 | loss  9.20 | ppl  9903.14 | bpc   13.274\n",
            "| epoch   1 |   200/  414 batches | lr 0.0001 | ms/batch 132.67 | loss  9.19 | ppl  9825.97 | bpc   13.262\n",
            "| epoch   1 |   250/  414 batches | lr 0.0001 | ms/batch 133.06 | loss  9.18 | ppl  9735.82 | bpc   13.249\n",
            "| epoch   1 |   300/  414 batches | lr 0.0001 | ms/batch 132.34 | loss  9.17 | ppl  9644.49 | bpc   13.235\n",
            "| epoch   1 |   350/  414 batches | lr 0.0001 | ms/batch 132.51 | loss  9.16 | ppl  9546.44 | bpc   13.221\n",
            "| epoch   1 |   400/  414 batches | lr 0.0001 | ms/batch 132.47 | loss  9.15 | ppl  9447.08 | bpc   13.206\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 57.40s | valid loss  9.14 | valid ppl  9295.13 | bpc   13.182\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    50/  414 batches | lr 0.0001 | ms/batch 135.22 | loss  9.32 | ppl 11186.70 | bpc   13.449\n",
            "| epoch   2 |   100/  414 batches | lr 0.0001 | ms/batch 133.05 | loss  9.13 | ppl  9216.03 | bpc   13.170\n",
            "| epoch   2 |   150/  414 batches | lr 0.0001 | ms/batch 132.76 | loss  9.12 | ppl  9102.51 | bpc   13.152\n",
            "| epoch   2 |   200/  414 batches | lr 0.0001 | ms/batch 132.58 | loss  9.11 | ppl  9014.79 | bpc   13.138\n",
            "| epoch   2 |   250/  414 batches | lr 0.0001 | ms/batch 133.48 | loss  9.09 | ppl  8900.68 | bpc   13.120\n",
            "| epoch   2 |   300/  414 batches | lr 0.0001 | ms/batch 132.36 | loss  9.08 | ppl  8790.06 | bpc   13.102\n",
            "| epoch   2 |   350/  414 batches | lr 0.0001 | ms/batch 132.51 | loss  9.07 | ppl  8671.78 | bpc   13.082\n",
            "| epoch   2 |   400/  414 batches | lr 0.0001 | ms/batch 132.78 | loss  9.05 | ppl  8546.38 | bpc   13.061\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 57.34s | valid loss  9.03 | valid ppl  8386.22 | bpc   13.034\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  414 batches | lr 0.0001 | ms/batch 134.95 | loss  9.22 | ppl 10049.12 | bpc   13.295\n",
            "| epoch   3 |   100/  414 batches | lr 0.0001 | ms/batch 132.40 | loss  9.02 | ppl  8255.24 | bpc   13.011\n",
            "| epoch   3 |   150/  414 batches | lr 0.0001 | ms/batch 132.52 | loss  9.00 | ppl  8091.69 | bpc   12.982\n",
            "| epoch   3 |   200/  414 batches | lr 0.0001 | ms/batch 132.68 | loss  8.98 | ppl  7977.08 | bpc   12.962\n",
            "| epoch   3 |   250/  414 batches | lr 0.0001 | ms/batch 132.23 | loss  8.96 | ppl  7800.07 | bpc   12.929\n",
            "| epoch   3 |   300/  414 batches | lr 0.0001 | ms/batch 132.29 | loss  8.94 | ppl  7624.07 | bpc   12.896\n",
            "| epoch   3 |   350/  414 batches | lr 0.0001 | ms/batch 132.42 | loss  8.91 | ppl  7421.36 | bpc   12.857\n",
            "| epoch   3 |   400/  414 batches | lr 0.0001 | ms/batch 132.15 | loss  8.88 | ppl  7188.66 | bpc   12.812\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 57.19s | valid loss  8.85 | valid ppl  6947.05 | bpc   12.762\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  414 batches | lr 0.0001 | ms/batch 135.08 | loss  9.01 | ppl  8190.74 | bpc   13.000\n",
            "| epoch   4 |   100/  414 batches | lr 0.0001 | ms/batch 132.15 | loss  8.79 | ppl  6557.45 | bpc   12.679\n",
            "| epoch   4 |   150/  414 batches | lr 0.0001 | ms/batch 132.85 | loss  8.72 | ppl  6148.73 | bpc   12.586\n",
            "| epoch   4 |   200/  414 batches | lr 0.0001 | ms/batch 132.52 | loss  8.66 | ppl  5753.54 | bpc   12.490\n",
            "| epoch   4 |   250/  414 batches | lr 0.0001 | ms/batch 132.28 | loss  8.54 | ppl  5133.99 | bpc   12.326\n",
            "| epoch   4 |   300/  414 batches | lr 0.0001 | ms/batch 132.32 | loss  8.38 | ppl  4362.23 | bpc   12.091\n",
            "| epoch   4 |   350/  414 batches | lr 0.0001 | ms/batch 132.30 | loss  8.13 | ppl  3398.80 | bpc   11.731\n",
            "| epoch   4 |   400/  414 batches | lr 0.0001 | ms/batch 133.46 | loss  7.87 | ppl  2629.92 | bpc   11.361\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 57.27s | valid loss  7.79 | valid ppl  2415.35 | bpc   11.238\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  414 batches | lr 0.0001 | ms/batch 135.00 | loss  7.93 | ppl  2787.54 | bpc   11.445\n",
            "| epoch   5 |   100/  414 batches | lr 0.0001 | ms/batch 133.00 | loss  7.65 | ppl  2096.35 | bpc   11.034\n",
            "| epoch   5 |   150/  414 batches | lr 0.0001 | ms/batch 132.76 | loss  7.54 | ppl  1879.54 | bpc   10.876\n",
            "| epoch   5 |   200/  414 batches | lr 0.0001 | ms/batch 132.88 | loss  7.51 | ppl  1827.64 | bpc   10.836\n",
            "| epoch   5 |   250/  414 batches | lr 0.0001 | ms/batch 132.82 | loss  7.48 | ppl  1775.26 | bpc   10.794\n",
            "| epoch   5 |   300/  414 batches | lr 0.0001 | ms/batch 132.26 | loss  7.42 | ppl  1673.14 | bpc   10.708\n",
            "| epoch   5 |   350/  414 batches | lr 0.0001 | ms/batch 132.43 | loss  7.39 | ppl  1613.22 | bpc   10.656\n",
            "| epoch   5 |   400/  414 batches | lr 0.0001 | ms/batch 132.89 | loss  7.34 | ppl  1535.04 | bpc   10.584\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 57.31s | valid loss  7.28 | valid ppl  1452.44 | bpc   10.504\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  414 batches | lr 0.0001 | ms/batch 135.35 | loss  7.45 | ppl  1713.12 | bpc   10.742\n",
            "| epoch   6 |   100/  414 batches | lr 0.0001 | ms/batch 132.55 | loss  7.27 | ppl  1439.44 | bpc   10.491\n",
            "| epoch   6 |   150/  414 batches | lr 0.0001 | ms/batch 132.72 | loss  7.20 | ppl  1343.68 | bpc   10.392\n",
            "| epoch   6 |   200/  414 batches | lr 0.0001 | ms/batch 132.81 | loss  7.20 | ppl  1341.96 | bpc   10.390\n",
            "| epoch   6 |   250/  414 batches | lr 0.0001 | ms/batch 132.59 | loss  7.19 | ppl  1325.55 | bpc   10.372\n",
            "| epoch   6 |   300/  414 batches | lr 0.0001 | ms/batch 132.66 | loss  7.15 | ppl  1278.21 | bpc   10.320\n",
            "| epoch   6 |   350/  414 batches | lr 0.0001 | ms/batch 132.39 | loss  7.13 | ppl  1244.92 | bpc   10.282\n",
            "| epoch   6 |   400/  414 batches | lr 0.0001 | ms/batch 133.11 | loss  7.09 | ppl  1205.41 | bpc   10.235\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 57.31s | valid loss  7.04 | valid ppl  1138.20 | bpc   10.153\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  414 batches | lr 0.0001 | ms/batch 135.24 | loss  7.23 | ppl  1375.00 | bpc   10.425\n",
            "| epoch   7 |   100/  414 batches | lr 0.0001 | ms/batch 132.61 | loss  7.07 | ppl  1177.08 | bpc   10.201\n",
            "| epoch   7 |   150/  414 batches | lr 0.0001 | ms/batch 132.36 | loss  7.02 | ppl  1116.10 | bpc   10.124\n",
            "| epoch   7 |   200/  414 batches | lr 0.0001 | ms/batch 132.96 | loss  7.03 | ppl  1128.93 | bpc   10.141\n",
            "| epoch   7 |   250/  414 batches | lr 0.0001 | ms/batch 132.62 | loss  7.04 | ppl  1136.81 | bpc   10.151\n",
            "| epoch   7 |   300/  414 batches | lr 0.0001 | ms/batch 132.66 | loss  7.01 | ppl  1104.58 | bpc   10.109\n",
            "| epoch   7 |   350/  414 batches | lr 0.0001 | ms/batch 132.17 | loss  6.99 | ppl  1090.02 | bpc   10.090\n",
            "| epoch   7 |   400/  414 batches | lr 0.0001 | ms/batch 132.88 | loss  6.97 | ppl  1062.71 | bpc   10.054\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 57.28s | valid loss  6.91 | valid ppl  1002.11 | bpc    9.969\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  414 batches | lr 0.0001 | ms/batch 135.83 | loss  7.11 | ppl  1229.43 | bpc   10.264\n",
            "| epoch   8 |   100/  414 batches | lr 0.0001 | ms/batch 133.05 | loss  6.97 | ppl  1059.84 | bpc   10.050\n",
            "| epoch   8 |   150/  414 batches | lr 0.0001 | ms/batch 132.44 | loss  6.92 | ppl  1011.33 | bpc    9.982\n",
            "| epoch   8 |   200/  414 batches | lr 0.0001 | ms/batch 132.92 | loss  6.93 | ppl  1027.18 | bpc   10.004\n",
            "| epoch   8 |   250/  414 batches | lr 0.0001 | ms/batch 132.81 | loss  6.95 | ppl  1039.74 | bpc   10.022\n",
            "| epoch   8 |   300/  414 batches | lr 0.0001 | ms/batch 132.61 | loss  6.93 | ppl  1018.70 | bpc    9.993\n",
            "| epoch   8 |   350/  414 batches | lr 0.0001 | ms/batch 132.68 | loss  6.91 | ppl  1005.50 | bpc    9.974\n",
            "| epoch   8 |   400/  414 batches | lr 0.0001 | ms/batch 132.93 | loss  6.89 | ppl   982.26 | bpc    9.940\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 57.37s | valid loss  6.83 | valid ppl   924.18 | bpc    9.852\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  414 batches | lr 0.0001 | ms/batch 135.07 | loss  7.04 | ppl  1145.13 | bpc   10.161\n",
            "| epoch   9 |   100/  414 batches | lr 0.0001 | ms/batch 132.54 | loss  6.90 | ppl   988.16 | bpc    9.949\n",
            "| epoch   9 |   150/  414 batches | lr 0.0001 | ms/batch 132.30 | loss  6.85 | ppl   948.11 | bpc    9.889\n",
            "| epoch   9 |   200/  414 batches | lr 0.0001 | ms/batch 132.49 | loss  6.87 | ppl   963.80 | bpc    9.913\n",
            "| epoch   9 |   250/  414 batches | lr 0.0001 | ms/batch 132.57 | loss  6.89 | ppl   979.53 | bpc    9.936\n",
            "| epoch   9 |   300/  414 batches | lr 0.0001 | ms/batch 132.28 | loss  6.87 | ppl   965.54 | bpc    9.915\n",
            "| epoch   9 |   350/  414 batches | lr 0.0001 | ms/batch 132.62 | loss  6.86 | ppl   952.14 | bpc    9.895\n",
            "| epoch   9 |   400/  414 batches | lr 0.0001 | ms/batch 132.55 | loss  6.84 | ppl   932.04 | bpc    9.864\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 57.22s | valid loss  6.77 | valid ppl   872.11 | bpc    9.768\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  414 batches | lr 0.0001 | ms/batch 135.09 | loss  6.99 | ppl  1086.28 | bpc   10.085\n",
            "| epoch  10 |   100/  414 batches | lr 0.0001 | ms/batch 132.68 | loss  6.85 | ppl   940.80 | bpc    9.878\n",
            "| epoch  10 |   150/  414 batches | lr 0.0001 | ms/batch 132.29 | loss  6.81 | ppl   903.74 | bpc    9.820\n",
            "| epoch  10 |   200/  414 batches | lr 0.0001 | ms/batch 132.57 | loss  6.83 | ppl   921.19 | bpc    9.847\n",
            "| epoch  10 |   250/  414 batches | lr 0.0001 | ms/batch 132.87 | loss  6.84 | ppl   938.06 | bpc    9.874\n",
            "| epoch  10 |   300/  414 batches | lr 0.0001 | ms/batch 132.66 | loss  6.83 | ppl   926.81 | bpc    9.856\n",
            "| epoch  10 |   350/  414 batches | lr 0.0001 | ms/batch 132.11 | loss  6.82 | ppl   911.83 | bpc    9.833\n",
            "| epoch  10 |   400/  414 batches | lr 0.0001 | ms/batch 132.52 | loss  6.80 | ppl   893.49 | bpc    9.803\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 57.25s | valid loss  6.73 | valid ppl   835.68 | bpc    9.707\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.68 | test ppl   800.29 | bpc    9.644\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XPrFSXzRJ_2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fXJ4DpbGJ__i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "interval = 50 # interval to report\n",
        "ntokens = len(corpus.dictionary) # 10000\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout)\n",
        "save = '/content/output/model_test_word_noise.pt'\n",
        "checkpoint = \"/content/output/model_test_word_none.pt\"\n",
        "\n",
        "# Load checkpoint\n",
        "if checkpoint != '':\n",
        "    model = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "\n",
        "print(model)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb69048-3f7c-4c73-e234-79c3d88d0161",
        "id": "XHhzL3OtKAGG"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(10000, 256)\n",
            "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
            "  (decoder): Linear(in_features=1000, out_features=10000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import vector_to_parameters, parameters_to_vector\n",
        "lr = lr\n",
        "best_val_loss = None\n",
        "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "opts = 'SGD'\n",
        "epochs = 25\n",
        "# if opt == 'Adam':\n",
        "#     opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
        "#     lr = 0.001\n",
        "# if args.opt == 'Momentum':\n",
        "#     opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.8)\n",
        "# if args.opt == 'RMSprop':\n",
        "#     opt = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
        "#     lr = 0.001\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        model.to(device)\n",
        "\n",
        "        param_vector = parameters_to_vector(model.rnn.parameters())\n",
        "        param_vector.to(device)\n",
        "        n_params = len(param_vector)\n",
        "        noise = torch.distributions.Normal(loc=torch.tensor(0.), scale=torch.tensor(0.075)).sample_n(n_params)\n",
        "        param_vector.add_(noise.to(device))\n",
        "        \n",
        "        vector_to_parameters(param_vector, model.rnn.parameters())\n",
        "        model.to(device)\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | bpc {:8.3f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if opts == 'SGD' or opts == 'Momentum':\n",
        "                lr /= 4.0\n",
        "                for group in opt.param_groups:\n",
        "                    group['lr'] = lr\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17bdf323-d010-40d5-a097-50b463caa42e",
        "id": "lBCZxgDQKAGG"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:161: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n",
            "  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  414 batches | lr 0.0001 | ms/batch 136.33 | loss  7.64 | ppl  2083.54 | bpc   11.025\n",
            "| epoch   1 |   100/  414 batches | lr 0.0001 | ms/batch 133.73 | loss  7.01 | ppl  1106.82 | bpc   10.112\n",
            "| epoch   1 |   150/  414 batches | lr 0.0001 | ms/batch 133.12 | loss  6.90 | ppl   997.01 | bpc    9.961\n",
            "| epoch   1 |   200/  414 batches | lr 0.0001 | ms/batch 133.57 | loss  6.89 | ppl   983.50 | bpc    9.942\n",
            "| epoch   1 |   250/  414 batches | lr 0.0001 | ms/batch 133.09 | loss  6.90 | ppl   988.29 | bpc    9.949\n",
            "| epoch   1 |   300/  414 batches | lr 0.0001 | ms/batch 132.93 | loss  6.87 | ppl   959.97 | bpc    9.907\n",
            "| epoch   1 |   350/  414 batches | lr 0.0001 | ms/batch 133.00 | loss  6.85 | ppl   939.19 | bpc    9.875\n",
            "| epoch   1 |   400/  414 batches | lr 0.0001 | ms/batch 132.95 | loss  6.82 | ppl   915.84 | bpc    9.839\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 57.63s | valid loss  6.74 | valid ppl   843.24 | bpc    9.720\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    50/  414 batches | lr 0.0001 | ms/batch 135.47 | loss  7.86 | ppl  2598.58 | bpc   11.344\n",
            "| epoch   2 |   100/  414 batches | lr 0.0001 | ms/batch 133.12 | loss  7.09 | ppl  1202.93 | bpc   10.232\n",
            "| epoch   2 |   150/  414 batches | lr 0.0001 | ms/batch 133.36 | loss  6.91 | ppl  1001.17 | bpc    9.967\n",
            "| epoch   2 |   200/  414 batches | lr 0.0001 | ms/batch 132.20 | loss  6.90 | ppl   987.50 | bpc    9.948\n",
            "| epoch   2 |   250/  414 batches | lr 0.0001 | ms/batch 132.43 | loss  6.90 | ppl   992.66 | bpc    9.955\n",
            "| epoch   2 |   300/  414 batches | lr 0.0001 | ms/batch 132.63 | loss  6.87 | ppl   959.61 | bpc    9.906\n",
            "| epoch   2 |   350/  414 batches | lr 0.0001 | ms/batch 132.42 | loss  6.85 | ppl   940.73 | bpc    9.878\n",
            "| epoch   2 |   400/  414 batches | lr 0.0001 | ms/batch 132.58 | loss  6.82 | ppl   915.75 | bpc    9.839\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 57.35s | valid loss  6.73 | valid ppl   836.64 | bpc    9.708\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  414 batches | lr 0.0001 | ms/batch 135.53 | loss  7.68 | ppl  2166.94 | bpc   11.081\n",
            "| epoch   3 |   100/  414 batches | lr 0.0001 | ms/batch 133.42 | loss  7.02 | ppl  1117.49 | bpc   10.126\n",
            "| epoch   3 |   150/  414 batches | lr 0.0001 | ms/batch 133.11 | loss  6.90 | ppl   991.58 | bpc    9.954\n",
            "| epoch   3 |   200/  414 batches | lr 0.0001 | ms/batch 132.73 | loss  6.88 | ppl   973.43 | bpc    9.927\n",
            "| epoch   3 |   250/  414 batches | lr 0.0001 | ms/batch 132.90 | loss  6.88 | ppl   976.81 | bpc    9.932\n",
            "| epoch   3 |   300/  414 batches | lr 0.0001 | ms/batch 132.70 | loss  6.86 | ppl   955.61 | bpc    9.900\n",
            "| epoch   3 |   350/  414 batches | lr 0.0001 | ms/batch 132.81 | loss  6.84 | ppl   934.81 | bpc    9.869\n",
            "| epoch   3 |   400/  414 batches | lr 0.0001 | ms/batch 132.78 | loss  6.81 | ppl   908.76 | bpc    9.828\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 57.46s | valid loss  6.71 | valid ppl   823.80 | bpc    9.686\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  414 batches | lr 0.0001 | ms/batch 135.17 | loss  7.86 | ppl  2603.80 | bpc   11.346\n",
            "| epoch   4 |   100/  414 batches | lr 0.0001 | ms/batch 132.52 | loss  7.11 | ppl  1223.80 | bpc   10.257\n",
            "| epoch   4 |   150/  414 batches | lr 0.0001 | ms/batch 132.12 | loss  6.89 | ppl   984.75 | bpc    9.944\n",
            "| epoch   4 |   200/  414 batches | lr 0.0001 | ms/batch 132.96 | loss  6.88 | ppl   973.70 | bpc    9.927\n",
            "| epoch   4 |   250/  414 batches | lr 0.0001 | ms/batch 132.65 | loss  6.88 | ppl   969.34 | bpc    9.921\n",
            "| epoch   4 |   300/  414 batches | lr 0.0001 | ms/batch 132.25 | loss  6.85 | ppl   944.49 | bpc    9.883\n",
            "| epoch   4 |   350/  414 batches | lr 0.0001 | ms/batch 132.45 | loss  6.83 | ppl   922.03 | bpc    9.849\n",
            "| epoch   4 |   400/  414 batches | lr 0.0001 | ms/batch 132.60 | loss  6.80 | ppl   901.10 | bpc    9.816\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 57.28s | valid loss  6.71 | valid ppl   820.59 | bpc    9.681\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  414 batches | lr 0.0001 | ms/batch 135.38 | loss  7.15 | ppl  1267.95 | bpc   10.308\n",
            "| epoch   5 |   100/  414 batches | lr 0.0001 | ms/batch 132.63 | loss  6.89 | ppl   986.74 | bpc    9.947\n",
            "| epoch   5 |   150/  414 batches | lr 0.0001 | ms/batch 132.70 | loss  6.83 | ppl   926.46 | bpc    9.856\n",
            "| epoch   5 |   200/  414 batches | lr 0.0001 | ms/batch 132.87 | loss  6.83 | ppl   923.10 | bpc    9.850\n",
            "| epoch   5 |   250/  414 batches | lr 0.0001 | ms/batch 132.78 | loss  6.85 | ppl   940.08 | bpc    9.877\n",
            "| epoch   5 |   300/  414 batches | lr 0.0001 | ms/batch 133.27 | loss  6.82 | ppl   919.75 | bpc    9.845\n",
            "| epoch   5 |   350/  414 batches | lr 0.0001 | ms/batch 133.00 | loss  6.80 | ppl   900.33 | bpc    9.814\n",
            "| epoch   5 |   400/  414 batches | lr 0.0001 | ms/batch 132.89 | loss  6.78 | ppl   880.91 | bpc    9.783\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 57.46s | valid loss  6.69 | valid ppl   803.92 | bpc    9.651\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  414 batches | lr 0.0001 | ms/batch 135.72 | loss  7.28 | ppl  1452.15 | bpc   10.504\n",
            "| epoch   6 |   100/  414 batches | lr 0.0001 | ms/batch 132.66 | loss  6.92 | ppl  1014.13 | bpc    9.986\n",
            "| epoch   6 |   150/  414 batches | lr 0.0001 | ms/batch 132.53 | loss  6.83 | ppl   928.68 | bpc    9.859\n",
            "| epoch   6 |   200/  414 batches | lr 0.0001 | ms/batch 132.53 | loss  6.83 | ppl   922.41 | bpc    9.849\n",
            "| epoch   6 |   250/  414 batches | lr 0.0001 | ms/batch 132.64 | loss  6.84 | ppl   938.25 | bpc    9.874\n",
            "| epoch   6 |   300/  414 batches | lr 0.0001 | ms/batch 132.49 | loss  6.83 | ppl   922.89 | bpc    9.850\n",
            "| epoch   6 |   350/  414 batches | lr 0.0001 | ms/batch 132.46 | loss  6.80 | ppl   898.54 | bpc    9.811\n",
            "| epoch   6 |   400/  414 batches | lr 0.0001 | ms/batch 132.75 | loss  6.78 | ppl   879.91 | bpc    9.781\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 57.37s | valid loss  6.68 | valid ppl   796.60 | bpc    9.638\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  414 batches | lr 0.0001 | ms/batch 135.49 | loss  7.24 | ppl  1396.90 | bpc   10.448\n",
            "| epoch   7 |   100/  414 batches | lr 0.0001 | ms/batch 133.12 | loss  6.94 | ppl  1028.10 | bpc   10.006\n",
            "| epoch   7 |   150/  414 batches | lr 0.0001 | ms/batch 132.68 | loss  6.85 | ppl   944.12 | bpc    9.883\n",
            "| epoch   7 |   200/  414 batches | lr 0.0001 | ms/batch 132.93 | loss  6.85 | ppl   940.55 | bpc    9.877\n",
            "| epoch   7 |   250/  414 batches | lr 0.0001 | ms/batch 132.92 | loss  6.86 | ppl   949.36 | bpc    9.891\n",
            "| epoch   7 |   300/  414 batches | lr 0.0001 | ms/batch 133.00 | loss  6.83 | ppl   926.49 | bpc    9.856\n",
            "| epoch   7 |   350/  414 batches | lr 0.0001 | ms/batch 132.67 | loss  6.81 | ppl   905.79 | bpc    9.823\n",
            "| epoch   7 |   400/  414 batches | lr 0.0001 | ms/batch 132.61 | loss  6.79 | ppl   886.04 | bpc    9.791\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 57.46s | valid loss  6.69 | valid ppl   801.88 | bpc    9.647\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  414 batches | lr 0.0000 | ms/batch 135.43 | loss  7.45 | ppl  1725.74 | bpc   10.753\n",
            "| epoch   8 |   100/  414 batches | lr 0.0000 | ms/batch 132.98 | loss  7.19 | ppl  1321.32 | bpc   10.368\n",
            "| epoch   8 |   150/  414 batches | lr 0.0000 | ms/batch 132.80 | loss  6.98 | ppl  1080.15 | bpc   10.077\n",
            "| epoch   8 |   200/  414 batches | lr 0.0000 | ms/batch 133.00 | loss  6.90 | ppl   991.83 | bpc    9.954\n",
            "| epoch   8 |   250/  414 batches | lr 0.0000 | ms/batch 132.71 | loss  6.89 | ppl   981.30 | bpc    9.939\n",
            "| epoch   8 |   300/  414 batches | lr 0.0000 | ms/batch 133.03 | loss  6.87 | ppl   964.38 | bpc    9.913\n",
            "| epoch   8 |   350/  414 batches | lr 0.0000 | ms/batch 132.30 | loss  6.84 | ppl   938.44 | bpc    9.874\n",
            "| epoch   8 |   400/  414 batches | lr 0.0000 | ms/batch 132.71 | loss  6.82 | ppl   916.50 | bpc    9.840\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 57.42s | valid loss  6.72 | valid ppl   830.98 | bpc    9.699\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  414 batches | lr 0.0000 | ms/batch 135.00 | loss  7.72 | ppl  2259.61 | bpc   11.142\n",
            "| epoch   9 |   100/  414 batches | lr 0.0000 | ms/batch 133.07 | loss  7.58 | ppl  1965.07 | bpc   10.940\n",
            "| epoch   9 |   150/  414 batches | lr 0.0000 | ms/batch 132.98 | loss  7.47 | ppl  1754.39 | bpc   10.777\n",
            "| epoch   9 |   200/  414 batches | lr 0.0000 | ms/batch 132.76 | loss  7.42 | ppl  1663.25 | bpc   10.700\n",
            "| epoch   9 |   250/  414 batches | lr 0.0000 | ms/batch 132.61 | loss  7.34 | ppl  1542.56 | bpc   10.591\n",
            "| epoch   9 |   300/  414 batches | lr 0.0000 | ms/batch 133.10 | loss  7.24 | ppl  1398.19 | bpc   10.449\n",
            "| epoch   9 |   350/  414 batches | lr 0.0000 | ms/batch 132.29 | loss  7.19 | ppl  1329.77 | bpc   10.377\n",
            "| epoch   9 |   400/  414 batches | lr 0.0000 | ms/batch 133.01 | loss  7.12 | ppl  1241.02 | bpc   10.277\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 57.41s | valid loss  7.00 | valid ppl  1091.73 | bpc   10.092\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  414 batches | lr 0.0000 | ms/batch 134.97 | loss  7.98 | ppl  2909.06 | bpc   11.506\n",
            "| epoch  10 |   100/  414 batches | lr 0.0000 | ms/batch 132.85 | loss  7.84 | ppl  2534.01 | bpc   11.307\n",
            "| epoch  10 |   150/  414 batches | lr 0.0000 | ms/batch 132.76 | loss  7.75 | ppl  2323.90 | bpc   11.182\n",
            "| epoch  10 |   200/  414 batches | lr 0.0000 | ms/batch 133.35 | loss  7.76 | ppl  2346.59 | bpc   11.196\n",
            "| epoch  10 |   250/  414 batches | lr 0.0000 | ms/batch 132.83 | loss  7.71 | ppl  2237.15 | bpc   11.127\n",
            "| epoch  10 |   300/  414 batches | lr 0.0000 | ms/batch 132.53 | loss  7.68 | ppl  2157.90 | bpc   11.075\n",
            "| epoch  10 |   350/  414 batches | lr 0.0000 | ms/batch 132.87 | loss  7.64 | ppl  2082.60 | bpc   11.024\n",
            "| epoch  10 |   400/  414 batches | lr 0.0000 | ms/batch 132.84 | loss  7.61 | ppl  2020.05 | bpc   10.980\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 57.43s | valid loss  7.47 | valid ppl  1759.97 | bpc   10.781\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |    50/  414 batches | lr 0.0000 | ms/batch 135.70 | loss  8.47 | ppl  4781.33 | bpc   12.223\n",
            "| epoch  11 |   100/  414 batches | lr 0.0000 | ms/batch 133.24 | loss  8.31 | ppl  4050.53 | bpc   11.984\n",
            "| epoch  11 |   150/  414 batches | lr 0.0000 | ms/batch 132.94 | loss  8.32 | ppl  4118.72 | bpc   12.008\n",
            "| epoch  11 |   200/  414 batches | lr 0.0000 | ms/batch 133.44 | loss  8.31 | ppl  4071.61 | bpc   11.991\n",
            "| epoch  11 |   250/  414 batches | lr 0.0000 | ms/batch 132.58 | loss  8.33 | ppl  4159.01 | bpc   12.022\n",
            "| epoch  11 |   300/  414 batches | lr 0.0000 | ms/batch 132.92 | loss  8.33 | ppl  4136.23 | bpc   12.014\n",
            "| epoch  11 |   350/  414 batches | lr 0.0000 | ms/batch 132.92 | loss  8.31 | ppl  4068.13 | bpc   11.990\n",
            "| epoch  11 |   400/  414 batches | lr 0.0000 | ms/batch 133.14 | loss  8.31 | ppl  4049.40 | bpc   11.983\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 57.50s | valid loss  8.20 | valid ppl  3624.29 | bpc   11.823\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    50/  414 batches | lr 0.0000 | ms/batch 135.36 | loss  8.59 | ppl  5368.18 | bpc   12.390\n",
            "| epoch  12 |   100/  414 batches | lr 0.0000 | ms/batch 132.95 | loss  8.43 | ppl  4592.77 | bpc   12.165\n",
            "| epoch  12 |   150/  414 batches | lr 0.0000 | ms/batch 133.09 | loss  8.43 | ppl  4582.03 | bpc   12.162\n",
            "| epoch  12 |   200/  414 batches | lr 0.0000 | ms/batch 133.03 | loss  8.45 | ppl  4682.91 | bpc   12.193\n",
            "| epoch  12 |   250/  414 batches | lr 0.0000 | ms/batch 132.39 | loss  8.44 | ppl  4639.73 | bpc   12.180\n",
            "| epoch  12 |   300/  414 batches | lr 0.0000 | ms/batch 132.76 | loss  8.44 | ppl  4609.55 | bpc   12.170\n",
            "| epoch  12 |   350/  414 batches | lr 0.0000 | ms/batch 133.03 | loss  8.44 | ppl  4637.88 | bpc   12.179\n",
            "| epoch  12 |   400/  414 batches | lr 0.0000 | ms/batch 132.75 | loss  8.43 | ppl  4599.65 | bpc   12.167\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 57.43s | valid loss  8.36 | valid ppl  4272.48 | bpc   12.061\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    50/  414 batches | lr 0.0000 | ms/batch 135.46 | loss  8.75 | ppl  6328.25 | bpc   12.628\n",
            "| epoch  13 |   100/  414 batches | lr 0.0000 | ms/batch 133.53 | loss  8.59 | ppl  5393.03 | bpc   12.397\n",
            "| epoch  13 |   150/  414 batches | lr 0.0000 | ms/batch 132.76 | loss  8.58 | ppl  5331.61 | bpc   12.380\n",
            "| epoch  13 |   200/  414 batches | lr 0.0000 | ms/batch 133.14 | loss  8.59 | ppl  5359.34 | bpc   12.388\n",
            "| epoch  13 |   250/  414 batches | lr 0.0000 | ms/batch 132.38 | loss  8.59 | ppl  5403.33 | bpc   12.400\n",
            "| epoch  13 |   300/  414 batches | lr 0.0000 | ms/batch 133.17 | loss  8.59 | ppl  5386.58 | bpc   12.395\n",
            "| epoch  13 |   350/  414 batches | lr 0.0000 | ms/batch 133.16 | loss  8.58 | ppl  5305.69 | bpc   12.373\n",
            "| epoch  13 |   400/  414 batches | lr 0.0000 | ms/batch 132.91 | loss  8.58 | ppl  5338.35 | bpc   12.382\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 57.50s | valid loss  8.51 | valid ppl  4947.50 | bpc   12.272\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |    50/  414 batches | lr 0.0000 | ms/batch 135.24 | loss  8.81 | ppl  6702.72 | bpc   12.711\n",
            "| epoch  14 |   100/  414 batches | lr 0.0000 | ms/batch 133.02 | loss  8.66 | ppl  5781.25 | bpc   12.497\n",
            "| epoch  14 |   150/  414 batches | lr 0.0000 | ms/batch 133.27 | loss  8.65 | ppl  5735.82 | bpc   12.486\n",
            "| epoch  14 |   200/  414 batches | lr 0.0000 | ms/batch 132.90 | loss  8.65 | ppl  5735.62 | bpc   12.486\n",
            "| epoch  14 |   250/  414 batches | lr 0.0000 | ms/batch 132.94 | loss  8.67 | ppl  5803.61 | bpc   12.503\n",
            "| epoch  14 |   300/  414 batches | lr 0.0000 | ms/batch 132.76 | loss  8.65 | ppl  5702.28 | bpc   12.477\n",
            "| epoch  14 |   350/  414 batches | lr 0.0000 | ms/batch 132.87 | loss  8.65 | ppl  5730.26 | bpc   12.484\n",
            "| epoch  14 |   400/  414 batches | lr 0.0000 | ms/batch 132.82 | loss  8.65 | ppl  5716.19 | bpc   12.481\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 57.48s | valid loss  8.57 | valid ppl  5289.92 | bpc   12.369\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |    50/  414 batches | lr 0.0000 | ms/batch 135.41 | loss  8.93 | ppl  7576.26 | bpc   12.887\n",
            "| epoch  15 |   100/  414 batches | lr 0.0000 | ms/batch 132.97 | loss  8.77 | ppl  6427.41 | bpc   12.650\n",
            "| epoch  15 |   150/  414 batches | lr 0.0000 | ms/batch 133.13 | loss  8.77 | ppl  6411.90 | bpc   12.647\n",
            "| epoch  15 |   200/  414 batches | lr 0.0000 | ms/batch 133.19 | loss  8.76 | ppl  6387.66 | bpc   12.641\n",
            "| epoch  15 |   250/  414 batches | lr 0.0000 | ms/batch 132.86 | loss  8.78 | ppl  6482.47 | bpc   12.662\n",
            "| epoch  15 |   300/  414 batches | lr 0.0000 | ms/batch 132.96 | loss  8.77 | ppl  6414.05 | bpc   12.647\n",
            "| epoch  15 |   350/  414 batches | lr 0.0000 | ms/batch 132.66 | loss  8.77 | ppl  6423.03 | bpc   12.649\n",
            "| epoch  15 |   400/  414 batches | lr 0.0000 | ms/batch 133.13 | loss  8.77 | ppl  6437.00 | bpc   12.652\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 57.49s | valid loss  8.68 | valid ppl  5909.31 | bpc   12.529\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |    50/  414 batches | lr 0.0000 | ms/batch 135.31 | loss  8.95 | ppl  7717.92 | bpc   12.914\n",
            "| epoch  16 |   100/  414 batches | lr 0.0000 | ms/batch 133.16 | loss  8.78 | ppl  6499.04 | bpc   12.666\n",
            "| epoch  16 |   150/  414 batches | lr 0.0000 | ms/batch 132.90 | loss  8.77 | ppl  6434.48 | bpc   12.652\n",
            "| epoch  16 |   200/  414 batches | lr 0.0000 | ms/batch 132.90 | loss  8.78 | ppl  6496.36 | bpc   12.665\n",
            "| epoch  16 |   250/  414 batches | lr 0.0000 | ms/batch 133.13 | loss  8.78 | ppl  6513.61 | bpc   12.669\n",
            "| epoch  16 |   300/  414 batches | lr 0.0000 | ms/batch 132.64 | loss  8.78 | ppl  6476.97 | bpc   12.661\n",
            "| epoch  16 |   350/  414 batches | lr 0.0000 | ms/batch 132.90 | loss  8.79 | ppl  6549.62 | bpc   12.677\n",
            "| epoch  16 |   400/  414 batches | lr 0.0000 | ms/batch 132.75 | loss  8.78 | ppl  6490.87 | bpc   12.664\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 57.44s | valid loss  8.69 | valid ppl  5939.56 | bpc   12.536\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |    50/  414 batches | lr 0.0000 | ms/batch 135.50 | loss  8.99 | ppl  7984.47 | bpc   12.963\n",
            "| epoch  17 |   100/  414 batches | lr 0.0000 | ms/batch 133.14 | loss  8.82 | ppl  6769.81 | bpc   12.725\n",
            "| epoch  17 |   150/  414 batches | lr 0.0000 | ms/batch 132.39 | loss  8.81 | ppl  6668.00 | bpc   12.703\n",
            "| epoch  17 |   200/  414 batches | lr 0.0000 | ms/batch 133.23 | loss  8.81 | ppl  6679.22 | bpc   12.705\n",
            "| epoch  17 |   250/  414 batches | lr 0.0000 | ms/batch 132.87 | loss  8.81 | ppl  6718.24 | bpc   12.714\n",
            "| epoch  17 |   300/  414 batches | lr 0.0000 | ms/batch 133.06 | loss  8.81 | ppl  6711.21 | bpc   12.712\n",
            "| epoch  17 |   350/  414 batches | lr 0.0000 | ms/batch 132.68 | loss  8.81 | ppl  6694.90 | bpc   12.709\n",
            "| epoch  17 |   400/  414 batches | lr 0.0000 | ms/batch 132.96 | loss  8.81 | ppl  6707.64 | bpc   12.712\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 57.48s | valid loss  8.73 | valid ppl  6195.91 | bpc   12.597\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |    50/  414 batches | lr 0.0000 | ms/batch 135.47 | loss  9.04 | ppl  8446.24 | bpc   13.044\n",
            "| epoch  18 |   100/  414 batches | lr 0.0000 | ms/batch 132.39 | loss  8.87 | ppl  7123.04 | bpc   12.798\n",
            "| epoch  18 |   150/  414 batches | lr 0.0000 | ms/batch 132.89 | loss  8.86 | ppl  7050.09 | bpc   12.783\n",
            "| epoch  18 |   200/  414 batches | lr 0.0000 | ms/batch 133.29 | loss  8.87 | ppl  7131.89 | bpc   12.800\n",
            "| epoch  18 |   250/  414 batches | lr 0.0000 | ms/batch 132.69 | loss  8.87 | ppl  7102.13 | bpc   12.794\n",
            "| epoch  18 |   300/  414 batches | lr 0.0000 | ms/batch 133.20 | loss  8.87 | ppl  7144.38 | bpc   12.803\n",
            "| epoch  18 |   350/  414 batches | lr 0.0000 | ms/batch 132.33 | loss  8.87 | ppl  7128.05 | bpc   12.799\n",
            "| epoch  18 |   400/  414 batches | lr 0.0000 | ms/batch 132.55 | loss  8.87 | ppl  7084.26 | bpc   12.790\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 57.40s | valid loss  8.79 | valid ppl  6582.09 | bpc   12.684\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |    50/  414 batches | lr 0.0000 | ms/batch 134.91 | loss  9.06 | ppl  8633.11 | bpc   13.076\n",
            "| epoch  19 |   100/  414 batches | lr 0.0000 | ms/batch 133.03 | loss  8.89 | ppl  7262.32 | bpc   12.826\n",
            "| epoch  19 |   150/  414 batches | lr 0.0000 | ms/batch 133.06 | loss  8.89 | ppl  7280.53 | bpc   12.830\n",
            "| epoch  19 |   200/  414 batches | lr 0.0000 | ms/batch 133.26 | loss  8.90 | ppl  7304.78 | bpc   12.835\n",
            "| epoch  19 |   250/  414 batches | lr 0.0000 | ms/batch 133.14 | loss  8.90 | ppl  7299.00 | bpc   12.833\n",
            "| epoch  19 |   300/  414 batches | lr 0.0000 | ms/batch 133.03 | loss  8.89 | ppl  7288.84 | bpc   12.831\n",
            "| epoch  19 |   350/  414 batches | lr 0.0000 | ms/batch 133.17 | loss  8.90 | ppl  7308.15 | bpc   12.835\n",
            "| epoch  19 |   400/  414 batches | lr 0.0000 | ms/batch 132.97 | loss  8.90 | ppl  7302.70 | bpc   12.834\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 57.49s | valid loss  8.81 | valid ppl  6721.16 | bpc   12.714\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |    50/  414 batches | lr 0.0000 | ms/batch 135.33 | loss  9.08 | ppl  8798.06 | bpc   13.103\n",
            "| epoch  20 |   100/  414 batches | lr 0.0000 | ms/batch 132.70 | loss  8.90 | ppl  7351.18 | bpc   12.844\n",
            "| epoch  20 |   150/  414 batches | lr 0.0000 | ms/batch 132.64 | loss  8.90 | ppl  7351.01 | bpc   12.844\n",
            "| epoch  20 |   200/  414 batches | lr 0.0000 | ms/batch 132.67 | loss  8.91 | ppl  7390.77 | bpc   12.852\n",
            "| epoch  20 |   250/  414 batches | lr 0.0000 | ms/batch 133.07 | loss  8.91 | ppl  7398.61 | bpc   12.853\n",
            "| epoch  20 |   300/  414 batches | lr 0.0000 | ms/batch 132.65 | loss  8.90 | ppl  7354.51 | bpc   12.844\n",
            "| epoch  20 |   350/  414 batches | lr 0.0000 | ms/batch 133.44 | loss  8.91 | ppl  7397.92 | bpc   12.853\n",
            "| epoch  20 |   400/  414 batches | lr 0.0000 | ms/batch 132.87 | loss  8.90 | ppl  7349.93 | bpc   12.844\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 57.45s | valid loss  8.83 | valid ppl  6804.12 | bpc   12.732\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |    50/  414 batches | lr 0.0000 | ms/batch 135.48 | loss  9.10 | ppl  8940.12 | bpc   13.126\n",
            "| epoch  21 |   100/  414 batches | lr 0.0000 | ms/batch 132.99 | loss  8.92 | ppl  7443.36 | bpc   12.862\n",
            "| epoch  21 |   150/  414 batches | lr 0.0000 | ms/batch 132.92 | loss  8.91 | ppl  7427.86 | bpc   12.859\n",
            "| epoch  21 |   200/  414 batches | lr 0.0000 | ms/batch 133.16 | loss  8.92 | ppl  7472.20 | bpc   12.867\n",
            "| epoch  21 |   250/  414 batches | lr 0.0000 | ms/batch 132.75 | loss  8.92 | ppl  7476.39 | bpc   12.868\n",
            "| epoch  21 |   300/  414 batches | lr 0.0000 | ms/batch 133.20 | loss  8.92 | ppl  7476.59 | bpc   12.868\n",
            "| epoch  21 |   350/  414 batches | lr 0.0000 | ms/batch 132.88 | loss  8.92 | ppl  7477.52 | bpc   12.868\n",
            "| epoch  21 |   400/  414 batches | lr 0.0000 | ms/batch 132.62 | loss  8.92 | ppl  7449.20 | bpc   12.863\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 57.48s | valid loss  8.84 | valid ppl  6877.12 | bpc   12.748\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |    50/  414 batches | lr 0.0000 | ms/batch 135.13 | loss  9.08 | ppl  8781.98 | bpc   13.100\n",
            "| epoch  22 |   100/  414 batches | lr 0.0000 | ms/batch 132.76 | loss  8.92 | ppl  7461.59 | bpc   12.865\n",
            "| epoch  22 |   150/  414 batches | lr 0.0000 | ms/batch 132.64 | loss  8.91 | ppl  7369.77 | bpc   12.847\n",
            "| epoch  22 |   200/  414 batches | lr 0.0000 | ms/batch 132.44 | loss  8.92 | ppl  7463.61 | bpc   12.866\n",
            "| epoch  22 |   250/  414 batches | lr 0.0000 | ms/batch 132.82 | loss  8.92 | ppl  7456.12 | bpc   12.864\n",
            "| epoch  22 |   300/  414 batches | lr 0.0000 | ms/batch 132.61 | loss  8.91 | ppl  7438.40 | bpc   12.861\n",
            "| epoch  22 |   350/  414 batches | lr 0.0000 | ms/batch 132.79 | loss  8.91 | ppl  7408.41 | bpc   12.855\n",
            "| epoch  22 |   400/  414 batches | lr 0.0000 | ms/batch 132.85 | loss  8.91 | ppl  7386.09 | bpc   12.851\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 57.38s | valid loss  8.83 | valid ppl  6825.62 | bpc   12.737\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |    50/  414 batches | lr 0.0000 | ms/batch 135.46 | loss  9.11 | ppl  9085.80 | bpc   13.149\n",
            "| epoch  23 |   100/  414 batches | lr 0.0000 | ms/batch 133.25 | loss  8.93 | ppl  7562.54 | bpc   12.885\n",
            "| epoch  23 |   150/  414 batches | lr 0.0000 | ms/batch 132.83 | loss  8.93 | ppl  7575.06 | bpc   12.887\n",
            "| epoch  23 |   200/  414 batches | lr 0.0000 | ms/batch 132.82 | loss  8.94 | ppl  7628.92 | bpc   12.897\n",
            "| epoch  23 |   250/  414 batches | lr 0.0000 | ms/batch 133.06 | loss  8.94 | ppl  7602.68 | bpc   12.892\n",
            "| epoch  23 |   300/  414 batches | lr 0.0000 | ms/batch 133.13 | loss  8.93 | ppl  7589.99 | bpc   12.890\n",
            "| epoch  23 |   350/  414 batches | lr 0.0000 | ms/batch 132.72 | loss  8.93 | ppl  7576.70 | bpc   12.887\n",
            "| epoch  23 |   400/  414 batches | lr 0.0000 | ms/batch 132.90 | loss  8.94 | ppl  7633.65 | bpc   12.898\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 57.48s | valid loss  8.86 | valid ppl  7024.35 | bpc   12.778\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |    50/  414 batches | lr 0.0000 | ms/batch 135.32 | loss  9.11 | ppl  9052.40 | bpc   13.144\n",
            "| epoch  24 |   100/  414 batches | lr 0.0000 | ms/batch 133.28 | loss  8.93 | ppl  7546.55 | bpc   12.882\n",
            "| epoch  24 |   150/  414 batches | lr 0.0000 | ms/batch 132.79 | loss  8.93 | ppl  7539.94 | bpc   12.880\n",
            "| epoch  24 |   200/  414 batches | lr 0.0000 | ms/batch 133.04 | loss  8.93 | ppl  7588.64 | bpc   12.890\n",
            "| epoch  24 |   250/  414 batches | lr 0.0000 | ms/batch 132.93 | loss  8.93 | ppl  7587.70 | bpc   12.889\n",
            "| epoch  24 |   300/  414 batches | lr 0.0000 | ms/batch 133.02 | loss  8.93 | ppl  7589.87 | bpc   12.890\n",
            "| epoch  24 |   350/  414 batches | lr 0.0000 | ms/batch 132.86 | loss  8.93 | ppl  7575.54 | bpc   12.887\n",
            "| epoch  24 |   400/  414 batches | lr 0.0000 | ms/batch 132.83 | loss  8.93 | ppl  7532.02 | bpc   12.879\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 57.48s | valid loss  8.86 | valid ppl  7025.67 | bpc   12.778\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |    50/  414 batches | lr 0.0000 | ms/batch 135.83 | loss  9.12 | ppl  9095.94 | bpc   13.151\n",
            "| epoch  25 |   100/  414 batches | lr 0.0000 | ms/batch 132.86 | loss  8.94 | ppl  7598.38 | bpc   12.891\n",
            "| epoch  25 |   150/  414 batches | lr 0.0000 | ms/batch 133.11 | loss  8.94 | ppl  7598.22 | bpc   12.891\n",
            "| epoch  25 |   200/  414 batches | lr 0.0000 | ms/batch 132.74 | loss  8.94 | ppl  7619.70 | bpc   12.896\n",
            "| epoch  25 |   250/  414 batches | lr 0.0000 | ms/batch 132.71 | loss  8.94 | ppl  7614.92 | bpc   12.895\n",
            "| epoch  25 |   300/  414 batches | lr 0.0000 | ms/batch 133.08 | loss  8.95 | ppl  7672.47 | bpc   12.905\n",
            "| epoch  25 |   350/  414 batches | lr 0.0000 | ms/batch 133.04 | loss  8.94 | ppl  7607.20 | bpc   12.893\n",
            "| epoch  25 |   400/  414 batches | lr 0.0000 | ms/batch 132.85 | loss  8.94 | ppl  7615.91 | bpc   12.895\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 57.48s | valid loss  8.86 | valid ppl  7047.22 | bpc   12.783\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.63 | test ppl   761.17 | bpc    9.572\n",
            "=========================================================================================\n"
          ]
        }
      ]
    }
  ]
}