{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PennTreebankExperiments.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!echo \"=== Acquiring datasets ===\"\n",
        "!echo \"---\"\n",
        "\n",
        "!mkdir -p data\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgHmdlPhL8oF",
        "outputId": "b24f0ccf-1cb7-4779-cee3-570460205c95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Acquiring datasets ===\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz_r7xiOI_6s",
        "outputId": "315971dd-45bc-41f3-f65a-2b1e62ae8064"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "L0wfV1YeMKok"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"- Downloading Penn Treebank (PTB)\"\n",
        "!wget --quiet --continue http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "!tar -xzf simple-examples.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK7K7MACI4nY",
        "outputId": "628d23f2-ae5b-46f3-ebb2-8865b3196129"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Downloading Penn Treebank (PTB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p penn\n",
        "%cd penn\n",
        "!mv ../simple-examples/data/ptb.train.txt train.txt\n",
        "!mv ../simple-examples/data/ptb.test.txt test.txt\n",
        "!mv ../simple-examples/data/ptb.valid.txt valid.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdDffaaGJAer",
        "outputId": "1d431bed-eb92-4f10-b619-1e2b8cb60f2b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data/penn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"- Downloading Penn Treebank (Character)\"\n",
        "!mkdir -p ../pennchar\n",
        "%cd ../pennchar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCTp_5tbNPhS",
        "outputId": "81ac6b81-7684-4f64-8c07-c8ff5c476015"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Downloading Penn Treebank (Character)\n",
            "/content/data/pennchar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ../simple-examples/data/ptb.char.train.txt train.txt\n",
        "!mv ../simple-examples/data/ptb.char.test.txt test.txt\n",
        "!mv ../simple-examples/data/ptb.char.valid.txt valid.txt"
      ],
      "metadata": {
        "id": "74fCeou3JAqF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ../simple-examples/"
      ],
      "metadata": {
        "id": "5-sFaSl5NbMX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp) # Token2Embeddings\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout) #(seq_len, batch_size, emb_size)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        self.init_weights()\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.05\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input size(bptt, bsz)\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        # emb size(bptt, bsz, embsize)\n",
        "        # hid size(layers, bsz, nhid)\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        # output size(bptt, bsz, nhid)\n",
        "        output = self.drop(output)\n",
        "        # decoder: nhid -> ntoken\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        # LSTM h and c\n",
        "        weight = next(self.parameters()).data\n",
        "        return weight.new_zeros(self.nlayers, bsz, self.nhid), weight.new_zeros(self.nlayers, bsz, self.nhid)"
      ],
      "metadata": {
        "id": "m_kekY5EAKbx"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = Counter()\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        token_id = self.word2idx[word]\n",
        "        self.counter[token_id] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids"
      ],
      "metadata": {
        "id": "3_xiIWpEAOGD"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import math"
      ],
      "metadata": {
        "id": "RhzjMOhhAQxf"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9y8nlquAgmg",
        "outputId": "033baaac-e2c5-4702-c648-d2c3a9e2622d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOf8u9RhAlcT",
        "outputId": "65c30ab5-2de1-42c4-d8bb-c17da3adf157"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = '/content/data/pennchar'\n",
        "batch_size = 256\n",
        "emsize = 256\n",
        "nlayers = 1\n",
        "nhid = 1000\n",
        "lr = 0.0001\n",
        "dropout = 0.5\n",
        "checkpoint = ''\n",
        "clip = 1\n",
        "bptt = 35\n",
        "epochs = 10\n",
        "save = '/content/output/model_test.pt'\n",
        "\n",
        "torch.manual_seed(1111)\n",
        "\n",
        "# Load data\n",
        "corpus = Corpus(data)\n"
      ],
      "metadata": {
        "id": "pzlWhsBYAYMl"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data"
      ],
      "metadata": {
        "id": "6FvrRumyA1L7"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_batch_size = 256\n",
        "train_data = batchify(corpus.train, batch_size) # size(total_len//bsz, bsz)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ],
      "metadata": {
        "id": "bGLbW8TzAbRw"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNXQXMK9BA4B",
        "outputId": "debbb1b5-00f2-4b44-bd96-b30301da4e69"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byMAy3UfBCKQ",
        "outputId": "d84627c0-7b0c-4a2a-d295-97c62b63ab30"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  9,  ...,  3, 20,  0],\n",
              "        [ 1,  2, 10,  ...,  7,  7, 24],\n",
              "        [ 2,  3,  7,  ..., 17, 13,  0],\n",
              "        ...,\n",
              "        [ 8,  4,  7,  ..., 21, 28, 26],\n",
              "        [ 7, 10, 15,  ...,  3,  3, 10],\n",
              "        [ 4,  9,  2,  ..., 16, 12,  5]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data.to(device)\n",
        "test_data.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1NZXjd9BLM7",
        "outputId": "194fe91a-2a9e-4cc1-cafe-96bc563cfd85"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5,  3,  3,  ...,  7,  3, 14],\n",
              "        [ 7, 29, 24,  ..., 15, 18,  3],\n",
              "        [ 3,  3,  0,  ...,  5,  2, 33],\n",
              "        ...,\n",
              "        [ 3,  3,  3,  ...,  7,  2, 17],\n",
              "        [ 7,  0,  8,  ..., 18, 21,  3],\n",
              "        [ 2, 16, 20,  ...,  1,  0, 13]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "interval = 50 # interval to report\n",
        "ntokens = len(corpus.dictionary) # 10000\n",
        "model = RNNModel(ntokens, emsize, nhid, nlayers, dropout)\n",
        "\n",
        "# Load checkpoint\n",
        "if checkpoint != '':\n",
        "    model = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "\n",
        "print(model)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-dm-QF-BQlt",
        "outputId": "262dacc4-93ee-4517-f92b-8a2558cc3cec"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(50, 256)\n",
            "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
            "  (decoder): Linear(in_features=1000, out_features=50, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Sd1VoOBYb3",
        "outputId": "cde1e849-3d6b-4d95-9cf9-f786f907a7cb"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (drop): Dropout(p=0.5, inplace=False)\n",
              "  (encoder): Embedding(50, 256)\n",
              "  (rnn): LSTM(256, 1000, dropout=0.5)\n",
              "  (decoder): Linear(in_features=1000, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    # detach\n",
        "    return tuple(v.clone().detach() for v in h)\n"
      ],
      "metadata": {
        "id": "a4NH_KQaBbW-"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_batch(source, i):\n",
        "    # source: size(total_len//bsz, bsz)\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    #data = torch.tensor(source[i:i+seq_len]) # size(bptt, bsz)\n",
        "    data = source[i:i+seq_len].clone().detach()\n",
        "    target = source[i+1:i+1+seq_len].clone().detach().view(-1)\n",
        "    #target = torch.tensor(source[i+1:i+1+seq_len].view(-1)) # size(bptt * bsz)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "YolhdEasBgCt"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        ntokens = len(corpus.dictionary)\n",
        "        hidden = model.init_hidden(eval_batch_size) #hidden size(nlayers, bsz, hdsize)\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):# iterate over every timestep\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data.to(device), hidden)\n",
        "            # model input and output\n",
        "            # inputdata size(bptt, bsz), and size(bptt, bsz, embsize) after embedding\n",
        "            # output size(bptt*bsz, ntoken)\n",
        "            total_loss += len(data) * criterion(output.to(device), targets.to(device)).data\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        return total_loss / len(data_source)\n"
      ],
      "metadata": {
        "id": "RyWNpGhkBgyt"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # choose a optimizer\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    # train_data size(batchcnt, bsz)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        # print(hidden.to(device))\n",
        "        output, hidden = model(data.to(device), hidden)\n",
        "        loss = criterion(output.to(device), targets.to(device))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_value_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                elapsed * 1000 / interval, cur_loss, math.exp(cur_loss), cur_loss / math.log(2)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n"
      ],
      "metadata": {
        "id": "37Y1ELGCBiaF"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = lr\n",
        "best_val_loss = None\n",
        "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "opts = 'SGD'\n",
        "# if opt == 'Adam':\n",
        "#     opt = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
        "#     lr = 0.001\n",
        "# if args.opt == 'Momentum':\n",
        "#     opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.8)\n",
        "# if args.opt == 'RMSprop':\n",
        "#     opt = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
        "#     lr = 0.001\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | bpc {:8.3f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss), val_loss / math.log(2)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if opts == 'SGD' or opts == 'Momentum':\n",
        "                lr /= 4.0\n",
        "                for group in opt.param_groups:\n",
        "                    group['lr'] = lr\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f} | bpc {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss), test_loss / math.log(2)))\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCKEoCZjBk9q",
        "outputId": "84f8a4be-09a9-45de-9036-e8243d89e78e"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  559 batches | lr 0.0001 | ms/batch 78.50 | loss  3.99 | ppl    54.01 | bpc    5.755\n",
            "| epoch   1 |   100/  559 batches | lr 0.0001 | ms/batch 76.49 | loss  3.90 | ppl    49.16 | bpc    5.619\n",
            "| epoch   1 |   150/  559 batches | lr 0.0001 | ms/batch 76.98 | loss  3.87 | ppl    47.93 | bpc    5.583\n",
            "| epoch   1 |   200/  559 batches | lr 0.0001 | ms/batch 77.79 | loss  3.84 | ppl    46.49 | bpc    5.539\n",
            "| epoch   1 |   250/  559 batches | lr 0.0001 | ms/batch 78.49 | loss  3.81 | ppl    44.93 | bpc    5.489\n",
            "| epoch   1 |   300/  559 batches | lr 0.0001 | ms/batch 79.27 | loss  3.77 | ppl    43.36 | bpc    5.438\n",
            "| epoch   1 |   350/  559 batches | lr 0.0001 | ms/batch 79.58 | loss  3.73 | ppl    41.80 | bpc    5.385\n",
            "| epoch   1 |   400/  559 batches | lr 0.0001 | ms/batch 79.00 | loss  3.70 | ppl    40.28 | bpc    5.332\n",
            "| epoch   1 |   450/  559 batches | lr 0.0001 | ms/batch 78.86 | loss  3.66 | ppl    38.78 | bpc    5.277\n",
            "| epoch   1 |   500/  559 batches | lr 0.0001 | ms/batch 80.04 | loss  3.62 | ppl    37.30 | bpc    5.221\n",
            "| epoch   1 |   550/  559 batches | lr 0.0001 | ms/batch 80.20 | loss  3.58 | ppl    35.86 | bpc    5.164\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 45.22s | valid loss  3.55 | valid ppl    34.76 | bpc    5.119\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    50/  559 batches | lr 0.0001 | ms/batch 81.74 | loss  3.60 | ppl    36.69 | bpc    5.197\n",
            "| epoch   2 |   100/  559 batches | lr 0.0001 | ms/batch 80.89 | loss  3.49 | ppl    32.79 | bpc    5.035\n",
            "| epoch   2 |   150/  559 batches | lr 0.0001 | ms/batch 81.34 | loss  3.45 | ppl    31.48 | bpc    4.977\n",
            "| epoch   2 |   200/  559 batches | lr 0.0001 | ms/batch 81.29 | loss  3.41 | ppl    30.23 | bpc    4.918\n",
            "| epoch   2 |   250/  559 batches | lr 0.0001 | ms/batch 81.67 | loss  3.37 | ppl    28.97 | bpc    4.856\n",
            "| epoch   2 |   300/  559 batches | lr 0.0001 | ms/batch 82.47 | loss  3.33 | ppl    27.90 | bpc    4.802\n",
            "| epoch   2 |   350/  559 batches | lr 0.0001 | ms/batch 82.43 | loss  3.29 | ppl    26.93 | bpc    4.751\n",
            "| epoch   2 |   400/  559 batches | lr 0.0001 | ms/batch 82.80 | loss  3.26 | ppl    26.12 | bpc    4.707\n",
            "| epoch   2 |   450/  559 batches | lr 0.0001 | ms/batch 82.05 | loss  3.23 | ppl    25.38 | bpc    4.666\n",
            "| epoch   2 |   500/  559 batches | lr 0.0001 | ms/batch 81.31 | loss  3.21 | ppl    24.69 | bpc    4.626\n",
            "| epoch   2 |   550/  559 batches | lr 0.0001 | ms/batch 81.23 | loss  3.18 | ppl    24.03 | bpc    4.587\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 46.93s | valid loss  3.16 | valid ppl    23.54 | bpc    4.557\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  559 batches | lr 0.0001 | ms/batch 82.96 | loss  3.22 | ppl    24.95 | bpc    4.641\n",
            "| epoch   3 |   100/  559 batches | lr 0.0001 | ms/batch 81.14 | loss  3.13 | ppl    22.94 | bpc    4.520\n",
            "| epoch   3 |   150/  559 batches | lr 0.0001 | ms/batch 80.97 | loss  3.12 | ppl    22.64 | bpc    4.501\n",
            "| epoch   3 |   200/  559 batches | lr 0.0001 | ms/batch 81.29 | loss  3.11 | ppl    22.46 | bpc    4.489\n",
            "| epoch   3 |   250/  559 batches | lr 0.0001 | ms/batch 81.24 | loss  3.10 | ppl    22.11 | bpc    4.467\n",
            "| epoch   3 |   300/  559 batches | lr 0.0001 | ms/batch 81.23 | loss  3.09 | ppl    21.97 | bpc    4.457\n",
            "| epoch   3 |   350/  559 batches | lr 0.0001 | ms/batch 81.21 | loss  3.08 | ppl    21.78 | bpc    4.445\n",
            "| epoch   3 |   400/  559 batches | lr 0.0001 | ms/batch 81.15 | loss  3.07 | ppl    21.64 | bpc    4.436\n",
            "| epoch   3 |   450/  559 batches | lr 0.0001 | ms/batch 81.27 | loss  3.07 | ppl    21.49 | bpc    4.426\n",
            "| epoch   3 |   500/  559 batches | lr 0.0001 | ms/batch 81.29 | loss  3.06 | ppl    21.38 | bpc    4.418\n",
            "| epoch   3 |   550/  559 batches | lr 0.0001 | ms/batch 81.41 | loss  3.06 | ppl    21.26 | bpc    4.410\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 46.72s | valid loss  3.05 | valid ppl    21.12 | bpc    4.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  559 batches | lr 0.0001 | ms/batch 82.72 | loss  3.12 | ppl    22.56 | bpc    4.496\n",
            "| epoch   4 |   100/  559 batches | lr 0.0001 | ms/batch 81.25 | loss  3.05 | ppl    21.10 | bpc    4.399\n",
            "| epoch   4 |   150/  559 batches | lr 0.0001 | ms/batch 81.23 | loss  3.05 | ppl    21.06 | bpc    4.397\n",
            "| epoch   4 |   200/  559 batches | lr 0.0001 | ms/batch 81.18 | loss  3.05 | ppl    21.10 | bpc    4.399\n",
            "| epoch   4 |   250/  559 batches | lr 0.0001 | ms/batch 81.15 | loss  3.04 | ppl    20.93 | bpc    4.387\n",
            "| epoch   4 |   300/  559 batches | lr 0.0001 | ms/batch 81.21 | loss  3.04 | ppl    20.93 | bpc    4.387\n",
            "| epoch   4 |   350/  559 batches | lr 0.0001 | ms/batch 81.21 | loss  3.04 | ppl    20.87 | bpc    4.384\n",
            "| epoch   4 |   400/  559 batches | lr 0.0001 | ms/batch 81.16 | loss  3.04 | ppl    20.84 | bpc    4.381\n",
            "| epoch   4 |   450/  559 batches | lr 0.0001 | ms/batch 81.12 | loss  3.03 | ppl    20.78 | bpc    4.377\n",
            "| epoch   4 |   500/  559 batches | lr 0.0001 | ms/batch 81.15 | loss  3.03 | ppl    20.77 | bpc    4.376\n",
            "| epoch   4 |   550/  559 batches | lr 0.0001 | ms/batch 81.08 | loss  3.03 | ppl    20.69 | bpc    4.371\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 46.69s | valid loss  3.02 | valid ppl    20.57 | bpc    4.362\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  559 batches | lr 0.0001 | ms/batch 82.81 | loss  3.09 | ppl    22.03 | bpc    4.461\n",
            "| epoch   5 |   100/  559 batches | lr 0.0001 | ms/batch 81.07 | loss  3.03 | ppl    20.66 | bpc    4.369\n",
            "| epoch   5 |   150/  559 batches | lr 0.0001 | ms/batch 81.10 | loss  3.03 | ppl    20.66 | bpc    4.369\n",
            "| epoch   5 |   200/  559 batches | lr 0.0001 | ms/batch 81.04 | loss  3.03 | ppl    20.73 | bpc    4.374\n",
            "| epoch   5 |   250/  559 batches | lr 0.0001 | ms/batch 81.44 | loss  3.03 | ppl    20.60 | bpc    4.365\n",
            "| epoch   5 |   300/  559 batches | lr 0.0001 | ms/batch 81.46 | loss  3.03 | ppl    20.62 | bpc    4.366\n",
            "| epoch   5 |   350/  559 batches | lr 0.0001 | ms/batch 81.41 | loss  3.03 | ppl    20.61 | bpc    4.365\n",
            "| epoch   5 |   400/  559 batches | lr 0.0001 | ms/batch 81.13 | loss  3.03 | ppl    20.59 | bpc    4.364\n",
            "| epoch   5 |   450/  559 batches | lr 0.0001 | ms/batch 81.14 | loss  3.02 | ppl    20.56 | bpc    4.362\n",
            "| epoch   5 |   500/  559 batches | lr 0.0001 | ms/batch 81.13 | loss  3.02 | ppl    20.57 | bpc    4.362\n",
            "| epoch   5 |   550/  559 batches | lr 0.0001 | ms/batch 81.12 | loss  3.02 | ppl    20.51 | bpc    4.359\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 46.71s | valid loss  3.01 | valid ppl    20.37 | bpc    4.349\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  559 batches | lr 0.0001 | ms/batch 83.23 | loss  3.08 | ppl    21.86 | bpc    4.450\n",
            "| epoch   6 |   100/  559 batches | lr 0.0001 | ms/batch 81.09 | loss  3.02 | ppl    20.51 | bpc    4.358\n",
            "| epoch   6 |   150/  559 batches | lr 0.0001 | ms/batch 81.09 | loss  3.02 | ppl    20.52 | bpc    4.359\n",
            "| epoch   6 |   200/  559 batches | lr 0.0001 | ms/batch 81.15 | loss  3.02 | ppl    20.59 | bpc    4.364\n",
            "| epoch   6 |   250/  559 batches | lr 0.0001 | ms/batch 81.20 | loss  3.02 | ppl    20.49 | bpc    4.357\n",
            "| epoch   6 |   300/  559 batches | lr 0.0001 | ms/batch 81.14 | loss  3.02 | ppl    20.50 | bpc    4.358\n",
            "| epoch   6 |   350/  559 batches | lr 0.0001 | ms/batch 81.21 | loss  3.02 | ppl    20.51 | bpc    4.358\n",
            "| epoch   6 |   400/  559 batches | lr 0.0001 | ms/batch 81.23 | loss  3.02 | ppl    20.50 | bpc    4.357\n",
            "| epoch   6 |   450/  559 batches | lr 0.0001 | ms/batch 81.20 | loss  3.02 | ppl    20.48 | bpc    4.356\n",
            "| epoch   6 |   500/  559 batches | lr 0.0001 | ms/batch 81.26 | loss  3.02 | ppl    20.49 | bpc    4.357\n",
            "| epoch   6 |   550/  559 batches | lr 0.0001 | ms/batch 81.24 | loss  3.02 | ppl    20.44 | bpc    4.353\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 46.73s | valid loss  3.01 | valid ppl    20.29 | bpc    4.343\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  559 batches | lr 0.0001 | ms/batch 83.17 | loss  3.08 | ppl    21.77 | bpc    4.444\n",
            "| epoch   7 |   100/  559 batches | lr 0.0001 | ms/batch 81.24 | loss  3.02 | ppl    20.44 | bpc    4.354\n",
            "| epoch   7 |   150/  559 batches | lr 0.0001 | ms/batch 81.10 | loss  3.02 | ppl    20.45 | bpc    4.354\n",
            "| epoch   7 |   200/  559 batches | lr 0.0001 | ms/batch 81.19 | loss  3.02 | ppl    20.53 | bpc    4.360\n",
            "| epoch   7 |   250/  559 batches | lr 0.0001 | ms/batch 81.06 | loss  3.02 | ppl    20.43 | bpc    4.353\n",
            "| epoch   7 |   300/  559 batches | lr 0.0001 | ms/batch 81.21 | loss  3.02 | ppl    20.44 | bpc    4.354\n",
            "| epoch   7 |   350/  559 batches | lr 0.0001 | ms/batch 81.11 | loss  3.02 | ppl    20.44 | bpc    4.353\n",
            "| epoch   7 |   400/  559 batches | lr 0.0001 | ms/batch 81.25 | loss  3.02 | ppl    20.45 | bpc    4.354\n",
            "| epoch   7 |   450/  559 batches | lr 0.0001 | ms/batch 81.11 | loss  3.02 | ppl    20.43 | bpc    4.353\n",
            "| epoch   7 |   500/  559 batches | lr 0.0001 | ms/batch 81.09 | loss  3.02 | ppl    20.43 | bpc    4.352\n",
            "| epoch   7 |   550/  559 batches | lr 0.0001 | ms/batch 81.15 | loss  3.02 | ppl    20.40 | bpc    4.350\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 46.70s | valid loss  3.01 | valid ppl    20.24 | bpc    4.339\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  559 batches | lr 0.0001 | ms/batch 82.84 | loss  3.08 | ppl    21.72 | bpc    4.441\n",
            "| epoch   8 |   100/  559 batches | lr 0.0001 | ms/batch 81.14 | loss  3.02 | ppl    20.40 | bpc    4.350\n",
            "| epoch   8 |   150/  559 batches | lr 0.0001 | ms/batch 81.24 | loss  3.02 | ppl    20.42 | bpc    4.352\n",
            "| epoch   8 |   200/  559 batches | lr 0.0001 | ms/batch 81.30 | loss  3.02 | ppl    20.48 | bpc    4.356\n",
            "| epoch   8 |   250/  559 batches | lr 0.0001 | ms/batch 81.33 | loss  3.02 | ppl    20.39 | bpc    4.350\n",
            "| epoch   8 |   300/  559 batches | lr 0.0001 | ms/batch 81.23 | loss  3.02 | ppl    20.40 | bpc    4.351\n",
            "| epoch   8 |   350/  559 batches | lr 0.0001 | ms/batch 81.17 | loss  3.02 | ppl    20.40 | bpc    4.351\n",
            "| epoch   8 |   400/  559 batches | lr 0.0001 | ms/batch 81.08 | loss  3.02 | ppl    20.42 | bpc    4.352\n",
            "| epoch   8 |   450/  559 batches | lr 0.0001 | ms/batch 81.12 | loss  3.02 | ppl    20.39 | bpc    4.350\n",
            "| epoch   8 |   500/  559 batches | lr 0.0001 | ms/batch 81.10 | loss  3.02 | ppl    20.39 | bpc    4.350\n",
            "| epoch   8 |   550/  559 batches | lr 0.0001 | ms/batch 81.08 | loss  3.01 | ppl    20.35 | bpc    4.347\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 46.70s | valid loss  3.01 | valid ppl    20.21 | bpc    4.337\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  559 batches | lr 0.0001 | ms/batch 81.86 | loss  3.08 | ppl    21.68 | bpc    4.439\n",
            "| epoch   9 |   100/  559 batches | lr 0.0001 | ms/batch 80.76 | loss  3.01 | ppl    20.36 | bpc    4.347\n",
            "| epoch   9 |   150/  559 batches | lr 0.0001 | ms/batch 81.27 | loss  3.01 | ppl    20.37 | bpc    4.349\n",
            "| epoch   9 |   200/  559 batches | lr 0.0001 | ms/batch 80.80 | loss  3.02 | ppl    20.44 | bpc    4.354\n",
            "| epoch   9 |   250/  559 batches | lr 0.0001 | ms/batch 81.24 | loss  3.01 | ppl    20.35 | bpc    4.347\n",
            "| epoch   9 |   300/  559 batches | lr 0.0001 | ms/batch 81.26 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   350/  559 batches | lr 0.0001 | ms/batch 81.24 | loss  3.01 | ppl    20.37 | bpc    4.349\n",
            "| epoch   9 |   400/  559 batches | lr 0.0001 | ms/batch 81.20 | loss  3.01 | ppl    20.38 | bpc    4.349\n",
            "| epoch   9 |   450/  559 batches | lr 0.0001 | ms/batch 80.94 | loss  3.01 | ppl    20.35 | bpc    4.347\n",
            "| epoch   9 |   500/  559 batches | lr 0.0001 | ms/batch 81.33 | loss  3.01 | ppl    20.36 | bpc    4.348\n",
            "| epoch   9 |   550/  559 batches | lr 0.0001 | ms/batch 81.27 | loss  3.01 | ppl    20.32 | bpc    4.345\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 46.62s | valid loss  3.00 | valid ppl    20.18 | bpc    4.335\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  559 batches | lr 0.0001 | ms/batch 82.15 | loss  3.07 | ppl    21.65 | bpc    4.436\n",
            "| epoch  10 |   100/  559 batches | lr 0.0001 | ms/batch 81.30 | loss  3.01 | ppl    20.33 | bpc    4.345\n",
            "| epoch  10 |   150/  559 batches | lr 0.0001 | ms/batch 81.16 | loss  3.01 | ppl    20.34 | bpc    4.346\n",
            "| epoch  10 |   200/  559 batches | lr 0.0001 | ms/batch 81.04 | loss  3.02 | ppl    20.42 | bpc    4.352\n",
            "| epoch  10 |   250/  559 batches | lr 0.0001 | ms/batch 81.13 | loss  3.01 | ppl    20.33 | bpc    4.345\n",
            "| epoch  10 |   300/  559 batches | lr 0.0001 | ms/batch 81.13 | loss  3.01 | ppl    20.34 | bpc    4.346\n",
            "| epoch  10 |   350/  559 batches | lr 0.0001 | ms/batch 81.10 | loss  3.01 | ppl    20.33 | bpc    4.346\n",
            "| epoch  10 |   400/  559 batches | lr 0.0001 | ms/batch 81.13 | loss  3.01 | ppl    20.34 | bpc    4.346\n",
            "| epoch  10 |   450/  559 batches | lr 0.0001 | ms/batch 81.09 | loss  3.01 | ppl    20.32 | bpc    4.345\n",
            "| epoch  10 |   500/  559 batches | lr 0.0001 | ms/batch 81.11 | loss  3.01 | ppl    20.33 | bpc    4.345\n",
            "| epoch  10 |   550/  559 batches | lr 0.0001 | ms/batch 81.10 | loss  3.01 | ppl    20.30 | bpc    4.343\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 46.64s | valid loss  3.00 | valid ppl    20.15 | bpc    4.333\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  3.01 | test ppl    20.29 | bpc    4.343\n",
            "=========================================================================================\n"
          ]
        }
      ]
    }
  ]
}